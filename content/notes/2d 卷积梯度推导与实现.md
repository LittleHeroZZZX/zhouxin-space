---
title: 2d 卷积梯度推导与实现
tags:
  - CUDA
  - 梯度推导
创建时间: 
修改时间: 
publish: true
dir: notes
slug: 2d convolution gradient derivation and implementation
---
# 符号说明
{{< math_block >}}
\begin{align*}
X &: 卷积输入，\text{shape} 为[b,h,w,c_{in}]\\
W &: 卷积核，\text{shape}为[a,a,c_{in},c_{out}]\\
s &: 步长\\
f &: 卷积结果，\text{shape}为[b,(h-k)/s+1,(w-k)/s+1,c_{out}]\\
loss &: 损失函数，loss = g(f)
\end{align*}
{{< /math_block >}}
约定，所有张量下标从0开始。
# 卷积运算
对于结果矩阵中f\[i,j,k,l\]，其卷积的范围（感受野）为：
{{< math_block >}}
X[i,js:js+a,ks:ks+a,:]
{{< /math_block >}}

那么卷积运算就可以表示为：
{{< math_block >}}
\begin{align*}
f[i,j,k,l] &= \sum_{m=0}^{a-1} \sum_{n=0}^{a-1} \sum_{p=0}^{c_{in}-1}(X[i,m+js,n+ks,p]\cdot w[m,n,p,l])\\
&=\vec{x_{vec}}^T  \vec{w_{vec}}
\end{align*}
{{< /math_block >}}
通过im2col技术，可以将卷积运算转换为向量内积。

## 损失函数对W的梯度
前式中，f\[i,j,k,l\]对于w\[m,n,p,l\]的梯度贡献只有一项x\[i,m+js,n+ks,p\]。我们需要确保x的索引有效，因此有如下约束条件：
{{< math_block >}}
\begin{cases}
0\leq i < b-1\\
0\leq m+js < h\\
0\leq n+ks < w \\
0\leq p <c_{in}
\end{cases}
{{< /math_block >}}

化简得到符合条件的ijkl的约束为：
{{< math_block >}}
\begin{cases}
0\leq i < b-1\\
j<(h-m)/s\\
k<(w-n)/s
\end{cases}
{{< /math_block >}}

根据链式法则，有：
{{< math_block >}}
\begin{align*}
\frac{\partial  loss}{\partial w[m,n,p,l]} 
&= \sum_{i=0}^{b-1}\sum_{j=0}^{\lfloor{(h-m)/s-1\rfloor}}\sum_{k=0}^{\lfloor{(w-n)/s-1\rfloor}} \frac{\partial loss}{\partial f[i,j,k,l]}\frac{\partial f[i,j,k,l]}{\partial w[m,n,p,l]}\\
&=\sum_{i=0}^{b-1}\sum_{j=0}^{\lfloor{(h-m)/s-1\rfloor}}\sum_{k=0}^{\lfloor{(w-n)/s-1\rfloor}} \frac{\partial loss}{\partial f[i,j,k,l]} X[i, m+js, n+ks, p]
\end{align*}
{{< /math_block >}}
其中$\partial{loss} /\partial f$在反向传播时已经得到了，且$\partial{loss} /(\partial {f[i,j,k,l]})$等于$(\partial{loss} /\partial {f})[i,j,k,l]$，将$\partial{loss} /\partial f$记为outgrad。

观察上式，其和我们之前推导的卷积表达式非常像：后两个求和项的索引为j,k与结果索引无关，说明其在这两个维度上进行了卷积操作，第一个索引l与结果索引有关，说明这是一个向量内积。具体来，这个表达式可以视为卷积操作，卷积核为loss对w的导数，被卷积对象为X，batch的维度在最后一个，做内积的维度在第一个。

对比二式，卷积核为autograd，卷积的单个感受野内部存在空洞，长宽方向上两个像素之间均隔了s-1个长度。这是一种空洞卷积，如下图所示，红色为卷积位置。
{{< math_block >}}
\left[ \begin{matrix}
	{\color[RGB]{240, 0, 0} 1}&		2&		{\color[RGB]{240, 0, 0} 3}&		4\\
	5&		6&		7&		8\\
	{\color[RGB]{240, 0, 0} 9}&		10&		{\color[RGB]{240, 0, 0} 11}&		12\\
	13&		14&		15&		16\\
\end{matrix} \right]
{{< /math_block >}}
怎么实现这个空洞卷积呢？我们可以扩张我们的卷积核outgrad，即在每一行没一列上都dilate填充s-1个元素，将2×2的的卷积核心扩展成4×4的卷积和，按照步长为1进行卷积：
{{< math_block >}}
\left[ \begin{matrix}
	w_1&		w_2\\
	w_3&		w_4\\
\end{matrix} \right] \,\,\Longrightarrow \left[ \begin{matrix}
	w_1&		0&		w_2&		0\\
	0&		0&		0&		0\\
	w_3&		0&		w_4&		0\\
	0&		0&		0&		0\\
\end{matrix} \right]
{{< /math_block >}}
到这里，我们的损失函数对权重的梯度表达式就可以写出来了：
```python
X # 输入 [b, h, w, c_in]
W # 卷积核 [a, a, w_in, w_out]
outgrad # loss对输出的梯度
stride # 卷积步长

outgrad_dilated = dilate(outgrad, axis=(1, 2), stride-1) # [b, *, *, c_out]
outgrad_dilated_permuted = permute(outgrad_dilated, (1, 2, 0, 3)) # [*, *, b, cout]
X_permuted = permute(X, (3, 1, 2, 0)) # [c_in, h, w, b]
W_grad_ = conv(X_permuted, outgrad_dilated_permuted) #[c_in, h, w, c_out]
W_grad = permute(W_grad_, (1, 2, 0, 3))
```

对于padding不为1的情况，我们直接从shape来考虑。在正向过程中，可以直接假定padding为0，输入为pad后新的输入。根据这一等价转换，`conv(X_permuted, outgrad_dilated_permuted)`这一步得到中X_permuted是根据真实的X得到，而outgrad是等价的X得到的，作为卷积核的outgrad其偏大了2padding，因此在卷积这一步中要指定padding=2padding：
```python
X # 输入 [b, h, w, c_in]
W # 卷积核 [a, a, w_in, w_out]
outgrad # loss对输出的梯度
stride # 卷积步长
padding # 

outgrad_dilated = dilate(outgrad, axis=(1, 2), stride-1) # [b, *, *, c_out]
outgrad_dilated_permuted = permute(outgrad_dilated, (1, 2, 0, 3)) # [*, *, b, cout]
X_permuted = permute(X, (3, 1, 2, 0)) # [c_in, h, w, b]
W_grad_ = conv(X_permuted, outgrad_dilated_permuted, padding=2*padding) #[c_in, h, w, c_out]
W_grad = permute(W_grad_, (1, 2, 0, 3))
```
## 损失函数对X的梯度
有了上面的基础，我们来讨论loss对X的梯度。首先来讨论一点，对于X\[i,j,k,l\]，如果其与w\[m,n,l,p\]相乘了，那么其应该在计算卷积f\[i,(j-m)/s,(k-n)/s,p\]的结果，即：
{{< math_block >}}
f[i,(j-m)/s,(k-n)/s,p] = \sum_{p=0}^{c_{out}-1}w[m,n,l,p]\cdot X[i,j,k,l]
{{< /math_block >}}

那么loss对于X\[i,j,k,l\]的梯度，只有f\[i,(j-m)/s,(k-n)/s,p\]对其有贡献，且贡献为w\[m,n,l,p\]。

接下来可以推导loss对于X\[i,j,k,l\]的表达式：
{{< math_block >}}
\begin{align*}
\frac{\partial loss}{\partial X\left[ i,j,k,l \right]}
&=\sum_{m=0}^{a-1}{\sum_{n=0}^{a-1}{\sum_{p=0}^{c_{out}}{\frac{\partial loss}{\partial f[i,(j-m)/s,(k-n)/s,p]}\cdot \frac{\partial f[i,(j-m)/s,(k-n)/s,p]}{\partial X\left[ i,j,k,l \right]}}}}
\\
&=\sum_{m=0}^{a-1}{\sum_{n=0}^{a-1}{\sum_{p=0}^{c_{out}}{\frac{\partial loss}{\partial f[i,(j-m)/s,(k-n)/s,p]}w\left[ m,n,l,p \right]}}}
\end{align*}
{{< /math_block >}}

又是似曾相识的一幕，有了上面的经验，这次分析就游刃有余得多：卷积核是W，被卷积对象是autograd，在autograd的最后一个维度上进行线性变换，将其从c_out映射到c_in上。batch的维度是W的第一个维度。在长宽两个维度上，感受野内部每次的步长是-1，也就是说卷积核第一个元素将与最后一个元素相乘。我们将卷积核flip一下即可。聪明的你肯定注意到了，感受野内部不是连续的，两个元素之间间隔了s-1个元素，因此也需要将outgrad使用dilate填充s-1个0元素。

可达鸭眉头一皱，事情没有这么简单。理论上，这个梯度的shape应当与X相等，但outgrad本来就比X小，经过卷积之后应该更小了。怎会如此？我们直接观察j=0、k=0的状态，代入上式，会发现我们对outgrad的索引为负值了。这时候就需要将outgrad周围填充a-1个元素。

到这里，我们的损失函数对输入的梯度表达式就可以写出来了：
```python
X # 输入 [b, h, w, c_in]
W # 卷积核 [a, a, w_in, w_out]
outgrad # loss对输出的梯度
stride # 卷积步长

W_flipped = flip(W, axis=(0,1)) # 在前两个维度上翻转stride
W_flipped_permuted = permute(W_flipped, axis=(0,1,3,2)) # [a, a, w_out, w_in]
outgrad_dilated = dilate(outgrad, axis=(1, 2), stride-1) # dilate填充stride-1个0
W_grad = conv(outgrad_dilated, W_flipped_permuted, padding=a-1) # [b, h, w, c_in]

```
对于padding不为1的情况，我们一样从shape来考虑。`conv(outgrad_dilated, W_flipped_permuted, padding=a-1)`这一句中outgrad偏大2padding，W无偏，因此padding数要少一倍的padding：

```python
X # 输入 [b, h, w, c_in]
W # 卷积核 [a, a, w_in, w_out]
outgrad # loss对输出的梯度
stride # 卷积步长

W_flipped = flip(W, axis=(0,1)) # 在前两个维度上翻转stride
W_flipped_permuted = permute(W_flipped, axis=(0,1,3,2)) # [a, a, w_out, w_in]
outgrad_dilated = dilate(outgrad, axis=(1, 2), stride-1) # dilate填充stride-1个0
W_grad = conv(outgrad_dilated, W_flipped_permuted, padding=a-1-padding) # [b, h, w, c_in]

```
# 参考文档
[Backpropagation through a Conv Layer](https://johnwlambert.github.io/conv-backprop/)
