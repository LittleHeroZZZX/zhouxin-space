---
title: MIT 6.5940 EfficientML Lab 1 å®éªŒç¬”è®°
tags:
  - EfficientML
date: 2024-11-27T14:53:00+08:00
lastmod: 2025-02-12T23:26:00+08:00
publish: true
dir: notes
slug: notes on mit efficientml Lab 1
---

# å®éªŒå‡†å¤‡

## Python ç¯å¢ƒ

éœ€è¦ç”¨åˆ°å¦‚ä¸‹ Python ç¯å¢ƒï¼š
- PyTorch GPU ç‰ˆæœ¬
- jupyter notebook
- tqdm
- matplotlib
- torchprofile

## æ•°æ®é›†å‡†å¤‡

Lab 1 ä¸­ç”¨åˆ°äº† CIFAR-10 æ•°æ®é›†ï¼Œå¯ä»¥ä½¿ç”¨ [https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz) ç›´æ¥ä¸‹è½½ï¼Œå¹¶å°†æ•´ä¸ª `cifar-10-batched-py` æ–‡ä»¶å¤¹è§£å‹åˆ° `data/cifar10` æ–‡ä»¶å¤¹å†…ã€‚

# Part 1: Fine-grained Pruning

## Question 1

![å„å±‚æƒé‡åˆ†å¸ƒç›´æ–¹å›¾](https://pics.zhouxin.space/20241127193237.png)

é™¤æœ€åä¸€å±‚åˆ†ç±»å¤´å¤–ï¼Œå…¶å®ƒå±‚å‡æœä»å‡å€¼ä¸º 0 çš„æ— åæ­£æ€åˆ†å¸ƒï¼Œè¿™æ„å‘³ç€å å¾ˆå¤§æ¯”ä¾‹çš„å‚æ•°æ˜¯å¯ä»¥è¢«ç§»é™¤çš„ï¼Œè¿™ä¸ºæ¨¡å‹å‹ç¼©ç•™ä¸‹äº†å¾ˆå¤§çš„ç©ºé—´ã€‚

## Question 2

ç¬¬äºŒä¸ªé—®é¢˜è¦æ±‚å®ç°ç»†ç²’åº¦å‰ªæï¼Œå³å¯ä»¥å¯¹æƒé‡çŸ©é˜µä¸­çš„å•ä¸ªå…ƒç´ è¿›è¡Œå‰ªæï¼Œå…³äºä¸åŒé¢—ç²’åº¦çš„å‰ªæä»‹ç»ï¼Œè§ [è¯¾ç¨‹ç¬¬ä¸‰è®²ç¬”è®°](https://www.zhouxin.space/notes/notes-on-mit-efficientml-3rd-lecture/#%E5%89%AA%E6%9E%9D%E7%BB%86%E7%B2%92%E7%A8%8B%E5%BA%A6)ã€‚

è¿™é‡Œä½¿ç”¨æ¯ä¸ªå‚æ•°çš„ç»å¯¹å€¼æ¥è¡¨ç¤ºå…¶é‡è¦æ€§ï¼Œå‰ªæ‰ä¸é‡è¦çš„å‚æ•°ï¼Œä¿ç•™é‡è¦çš„å‚æ•°ã€‚

æœ¬é—®æ¯”è¾ƒç®€å•ï¼Œæ ¹æ®ç¨€ç–åº¦è®¡ç®—å‡ºéœ€è¦å‰ªå»çš„å‚æ•°æ€»é‡ï¼Œç„¶åä½¿ç”¨æ‰¾åˆ°é˜ˆå€¼å¹¶æ ¹æ®é˜ˆå€¼å¾—åˆ° mask çŸ©é˜µã€‚å”¯ä¸€çš„ä¸€ä¸ªæ³¨æ„ç‚¹æ˜¯è®¡ç®— mask çŸ©é˜µæ˜¯ä½¿ç”¨å¤§äºè€Œä¸æ˜¯å¤§äºç­‰äºï¼Œè¿™æ˜¯ç”±äºè®¡ç®—å¾—åˆ°çš„é˜ˆå€¼ä¹Ÿéœ€è¦è¢«å‰ªæ‰ã€‚

```python
##################### YOUR CODE STARTS HERE #####################
# Step 1: calculate the #zeros (please use round())
num_zeros = round(sparsity * num_elements)
# Step 2: calculate the importance of weight
importance = tensor.abs()
# Step 3: calculate the pruning threshold
threshold = torch.kthvalue(importance.view(-1), num_zeros).values
# Step 4: get binary mask (1 for nonzeros, 0 for zeros)
mask = importance > threshold
##################### YOUR CODE ENDS HERE #######################
```

## Question 3

é—®é¢˜ä¸‰è¦æ±‚æˆ‘ä»¬åœ¨ä¸€ä¸ª 5 x 5 çš„çŸ©é˜µä¸­ä¿ç•™ 10 ä¸ªå…ƒç´ ï¼Œç›¸åº”çš„ç¨€ç–åº¦ä¸º $1-\frac{10}{25}$ï¼Œæ­¤é—®å°±ç®—ç»“æŸäº†ã€‚

## Question 4

æ­¤é—®å¯¹ VGG ç½‘ç»œæ¯ä¸€å±‚è¿›è¡Œäº†çµæ•åº¦åˆ†æï¼Œå»ºè®®å°†æ­¥é•¿ä¿®æ”¹ä¸º 0.2 æˆ–è€… 0.1ï¼Œä»¥è·å¾—æ›´åŠ å¹³æ»‘çš„çµæ•åº¦æ›²çº¿ã€‚

![VGG å„å±‚çµæ•åº¦åˆ†æç»“æœ](https://pics.zhouxin.space/20241127203439.png)

ä»å›¾ä¸­å¯ä»¥çœ‹åˆ°å¤§éƒ¨åˆ†å±‚ä¸­ï¼Œéšç€ç¨€ç–åº¦çš„å¢åŠ ï¼Œæ¨¡å‹ç²¾åº¦ç›¸åº”å˜ä½ï¼Œä¸åŒå±‚çš„æ•æ„Ÿç¨‹åº¦ä¸åŒï¼Œç¬¬ 0 ä¸ªå·ç§¯å±‚å¯¹ç¨€ç–åº¦æœ€æ•æ„Ÿã€‚

## Question 5

ç¬¬ 5 é—®ä¸­ï¼Œè¦æ±‚æ ¹æ®å‰é¢çµæ•åº¦åˆ†æç»“æœå’Œæ¨¡å‹å‚æ•°è®¡ç®—é‡ï¼Œè®¾ç½®æ¯ä¸€å±‚å‰ªææ—¶çš„ç¨€ç–åº¦ã€‚â—ï¸æ³¨æ„ï¼Œæœ€ç»ˆæ•´ä¸ªæ¨¡å‹çš„ç¨€ç–åº¦å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºå‚æ•°é‡æ¯”è¾ƒå¤§çš„å±‚çš„ç¨€ç–åº¦ï¼Œå¯¹äºå‚æ•°é‡æ¯”è¾ƒå¤§çš„å±‚ï¼Œå¯ä»¥è€ƒè™‘è®¾ç½®æ¯”è¾ƒé«˜çš„ç¨€ç–åº¦ã€‚

æˆ‘é€‰æ‹©çš„ç¨€ç–åº¦å‚æ•°ä¸ºï¼š

```python
sparsity_dict = {
##################### YOUR CODE STARTS HERE #####################
    # please modify the sparsity value of each layer
    # please DO NOT modify the key of sparsity_dict
    'backbone.conv0.weight': 0,
    'backbone.conv1.weight': 0.6,
    'backbone.conv2.weight': 0.5,
    'backbone.conv3.weight': 0.5,
    'backbone.conv4.weight': 0.5,
    'backbone.conv5.weight': 0.6,
    'backbone.conv6.weight': 0.6,
    'backbone.conv7.weight': 0.75,
    'classifier.weight': 0
##################### YOUR CODE ENDS HERE #######################
}
```

ç»è¿‡å‰ªæåï¼Œå¤§å°çº¦ä¸ºåŸå§‹ç¨ å¯†æ¨¡å‹çš„ 38.48%ï¼Œç²¾åº¦ä» 92.9% é™ä½åˆ°äº† 91.50%ï¼Œåœ¨ 5 è½®çš„å¾®è°ƒåï¼Œæ¨¡å‹ç²¾åº¦æ¢å¤ä¸º 92.95%ã€‚

# Part 2: Channel Pruning

## Question 6

ç¬¬ 6 é—®éœ€è¦å®ç° Channel Pruningï¼Œå‰ªææ ‡å‡†æ˜¯åªä¿ç•™å‰ k ä¸ªé€šé“ã€‚é—®é¢˜æœ¬èº«æ—¶ç®€å•çš„ï¼Œç”¨å¥½ Python ä¸­çš„åˆ‡ç‰‡å³å¯ï¼š

```python
def get_num_channels_to_keep(channels: int, prune_ratio: float) -> int:
    """A function to calculate the number of layers to PRESERVE after pruning
    Note that preserve_rate = 1. - prune_ratio
    """
    ##################### YOUR CODE STARTS HERE #####################
    return int(round(channels * (1 - prune_ratio)))
    ##################### YOUR CODE ENDS HERE #####################

@torch.no_grad()
def channel_prune(model: nn.Module,
                  prune_ratio: Union[List, float]) -> nn.Module:
    """Apply channel pruning to each of the conv layer in the backbone
    Note that for prune_ratio, we can either provide a floating-point number,
    indicating that we use a uniform pruning rate for all layers, or a list of
    numbers to indicate per-layer pruning rate.
    """
    # sanity check of provided prune_ratio
    assert isinstance(prune_ratio, (float, list))
    
    n_conv = len([m for m in model.backbone if isinstance(m, nn.Conv2d)])
    # note that for the ratios, it affects the previous conv output and next
    # conv input, i.e., conv0 - ratio0 - conv1 - ratio1-...
    if isinstance(prune_ratio, list):
        assert len(prune_ratio) == n_conv - 1
    else:  # convert float to list
        prune_ratio = [prune_ratio] * (n_conv - 1)
    # we prune the convs in the backbone with a uniform ratio
    model = copy.deepcopy(model)  # prevent overwrite
    # we only apply pruning to the backbone features
    all_convs = [m for m in model.backbone if isinstance(m, nn.Conv2d)]
    all_bns = [m for m in model.backbone if isinstance(m, nn.BatchNorm2d)]
    # apply pruning. we naively keep the first k channels
    assert len(all_convs) == len(all_bns)
    for i_ratio, p_ratio in enumerate(prune_ratio):
        prev_conv = all_convs[i_ratio]
        prev_bn = all_bns[i_ratio]
        next_conv = all_convs[i_ratio + 1]
        original_channels = prev_conv.out_channels  # same as next_conv.in_channels
        n_keep = get_num_channels_to_keep(original_channels, p_ratio)

        # prune the output of the previous conv and bn
        prev_conv.weight.set_(prev_conv.weight.detach()[:n_keep])
        prev_bn.weight.set_(prev_bn.weight.detach()[:n_keep])
        prev_bn.bias.set_(prev_bn.bias.detach()[:n_keep])
        prev_bn.running_mean.set_(prev_bn.running_mean.detach()[:n_keep])
        prev_bn.running_var.set_(prev_bn.running_var.detach()[:n_keep])

        # prune the input of the next conv (hint: just one line of code)
        ##################### YOUR CODE STARTS HERE #####################
        next_conv.weight.set_(next_conv.weight.detach()[:, :n_keep])
        ##################### YOUR CODE ENDS HERE #####################

    return model

```

è®°å¾—ä¸€æçš„æ˜¯æ¡†æ¶å·²ç»ç»™å‡ºçš„ä»£ç ï¼Œæ‰€è°“ Channel æ˜¯åœ¨å·ç§¯ä¸­æ‰ä¼šå‡ºç°çš„ï¼Œå‰ªæä¹Ÿæ˜¯å¯¹è¾“å‡ºé€šé“è¿›è¡Œå‰ªæã€‚ä¾‹å¦‚ï¼Œå½“å‰å·ç§¯æ ¸ä¸­æœ¬æ¥æœ‰ k ä¸ªé€šé“è¾“å‡ºï¼Œå‰ªæåå˜æˆ l ä¸ªè¾“å‡ºé€šé“ï¼Œé‚£ä¹ˆä¸‹ä¸€å±‚çš„å·ç§¯æ ¸çš„è¾“å…¥é€šé“ä¹Ÿè¦ç›¸å¯¹åº”åœ°ä» k å˜æˆ lã€‚æ­¤å¤–ä¸€èˆ¬ Conv åéƒ½ä¼šæœ‰ä¸€ä¸ª Batch Normï¼Œåº”è¯¥è¿™ä¸ª Conv çš„ weightã€biasã€running_mean å’Œ running_var ä¹Ÿè¦ä¸€èµ·è¿›è¡Œå‰ªæã€‚

## Question 7

æ”¹è¿› Channel Pruningï¼Œä½¿ç”¨ Frobenius èŒƒæ•°æ¥è¯„ä¼°æ¯ä¸€ä¸ªé€šé“çš„é‡è¦ç¨‹åº¦ã€‚æœ¬é—®çš„æ ¸å¿ƒå°±æ˜¯ Frobenius èŒƒæ•°çš„è®¡ç®—ï¼Œè¯´æ˜ä¸­æ¨èä½¿ç”¨ [torch.norm](https://pytorch.org/docs/main/generated/torch.norm.html#torch.norm) è¿›è¡Œå®ç°ï¼Œä½†æ˜¯å®˜ç½‘æ–‡æ¡£ä¸­æåˆ°è¿™ä¸ª API å·²ç»è¢«å¼ƒç”¨ï¼Œè¿™é‡Œæ”¹ç”¨Â [torch.linalg.vector_norm()](https://pytorch.org/docs/main/generated/torch.linalg.vector_norm.html#torch.linalg.vector_norm "torch.linalg.vector_norm")ã€‚æ ¹æ®æ–‡æ¡£ï¼Œ`dim` æŒ‡å®šä¸ºéœ€è¦å±•å¼€ä¸ºå‘é‡çš„ç»´åº¦ï¼Œå³ç¬¬ `[0, 2, 3]`ã€‚

```python
# function to sort the channels from important to non-important
def get_input_channel_importance(weight):
    in_channels = weight.shape[1]
    # importances = []
    # # compute the importance for each input channel
    # for i_c in range(weight.shape[1]):
    #     channel_weight = weight.detach()[:, i_c]
    #     ##################### YOUR CODE STARTS HERE #####################
    #     importance = torch.linalg.norm(channel_weight, ord="fro", dim
    #     ##################### YOUR CODE ENDS HERE #####################
    #     importances.append(importance.view(1))
    # return torch.cat(importances)
    return torch.linalg.vector_norm(weight, ord=2, dim=(0, 2, 3))

@torch.no_grad()
def apply_channel_sorting(model):
    model = copy.deepcopy(model)  # do not modify the original model
    # fetch all the conv and bn layers from the backbone
    all_convs = [m for m in model.backbone if isinstance(m, nn.Conv2d)]
    all_bns = [m for m in model.backbone if isinstance(m, nn.BatchNorm2d)]
    # iterate through conv layers
    for i_conv in range(len(all_convs) - 1):
        # each channel sorting index, we need to apply it to:
        # - the output dimension of the previous conv
        # - the previous BN layer
        # - the input dimension of the next conv (we compute importance here)
        prev_conv = all_convs[i_conv]
        prev_bn = all_bns[i_conv]
        next_conv = all_convs[i_conv + 1]
        # note that we always compute the importance according to input channels
        importance = get_input_channel_importance(next_conv.weight)
        # sorting from large to small
        sort_idx = torch.argsort(importance, descending=True)

        # apply to previous conv and its following bn
        prev_conv.weight.copy_(torch.index_select(
            prev_conv.weight.detach(), 0, sort_idx))
        for tensor_name in ['weight', 'bias', 'running_mean', 'running_var']:
            tensor_to_apply = getattr(prev_bn, tensor_name)
            tensor_to_apply.copy_(
                torch.index_select(tensor_to_apply.detach(), 0, sort_idx)
            )

        # apply to the next conv input (hint: one line of code)
        ##################### YOUR CODE STARTS HERE #####################
        next_conv.weight.copy_(torch.index_select(next_conv.weight.detach(), 1, sort_idx))
        ##################### YOUR CODE ENDS HERE #####################

    return model
```

ç›¸æ¯”æ²¡æœ‰è®¡ç®—é‡è¦æ€§çš„é€šé“å‰ªæï¼Œæ”¹è¿›å‰ªæåçš„æ¨¡å‹çš„å‡†ç¡®ç‡ä» 28.15% æå‡åˆ° 36.81ã€‚ç»è¿‡å¾®è°ƒåæ¢å¤ä¸º 92.41%ã€‚

## Question 8

1. ä¸ºä»€ä¹ˆå‰ªæ 30% ä½†æ˜¯è®¡ç®—é‡å‡å°‘äº†å¤§çº¦ 50%ã€‚  
VGG æ¨¡å‹ä¸»è¦ç”±å·ç§¯å±‚æ„æˆï¼Œå·ç§¯å±‚çš„è®¡ç®—é‡ FLOPs ä¸ºï¼š

{{< math_block >}}
FLOPs = K\times K\times C_{in}\times C_{out}\times H \times W
{{< /math_block >}}

å…¶ä¸­è¾“å…¥å’Œè¾“å‡ºé€šé“éƒ½å˜ä¸ºåŸæ¥çš„ 70%ï¼Œå› è€Œæ€»è®¡ç®—é‡å˜ä¸ºåŸæ¥çš„ 49%ã€‚

2. è§£é‡Šä¸€ä¸‹ä¸ºä»€ä¹ˆå»¶è¿Ÿï¼ˆlatencyï¼‰çš„å‡å°‘æ¯”ä¾‹ç•¥å°äºè®¡ç®—é‡çš„å‡å°‘æ¯”ä¾‹ã€‚  
å»¶è¿Ÿä¸ä»…ä»…æ¥æºäºè®¡ç®—ï¼Œè¿˜æ¥è‡ªäºæ•°æ®çš„æ¬è¿ï¼Œè¿™éƒ¨åˆ†æ—¶é—´åœ¨æ²¡åšç®—å­èåˆçš„æƒ…å†µä¸‹å‡å°‘å¹¶ä¸æ˜¾è‘—ã€‚

## Question 9

1. è®¨è®ºä¸€ä¸‹ fine-grained pruning å’Œ channel pruning çš„ä¼˜ç¼ºç‚¹ã€‚  
ç»†ç²’åº¦å‰ªæï¼šå‹ç¼©ç‡æ›´é«˜ã€å¯¹ç¡¬ä»¶ä¸å‹å¥½ã€å»¶è¿Ÿé«˜ï¼›  
é€šé“å‰ªæï¼šå‹ç¼©ç‡ä½ã€ç¡¬ä»¶å‹å¥½ã€å»¶è¿Ÿä½ã€æ˜“äºå¾®è°ƒã€‚

2. å¦‚æœæƒ³åœ¨æ™ºèƒ½æ‰‹æœºä¸ŠåŠ é€Ÿæ¨¡å‹ï¼Œä½¿ç”¨å“ªç§æ–¹æ¡ˆæ›´åˆé€‚ã€‚  
é€šé“å‰ªæã€‚æ™ºèƒ½æ‰‹æœºä¸Šä¸€èˆ¬ç¼ºä¹å¯¹äºç¨€ç–çŸ©é˜µçš„æ”¯æŒï¼Œé€‰å–å¯¹ç¡¬ä»¶æ›´å‹å¥½çš„æ–¹æ¡ˆã€‚

# å°ç»“

ç¬¬ä¸€ä¸ª Lab æœ¬èº«æ¯”è¾ƒç®€å•ï¼Œåšå®Œèƒ½å¤Ÿå»ºç«‹èµ·å¯¹äºå‰ªæçš„åˆæ­¥è®¤è¯†ï¼Œå¸Œæœ›åé¢çš„å®éªŒèƒ½å¤Ÿä¸Šç‚¹å¼ºåº¦ï¼Œä»£ç é‡ä¹Ÿå¤ªå°‘äº†ğŸ˜‚ã€‚