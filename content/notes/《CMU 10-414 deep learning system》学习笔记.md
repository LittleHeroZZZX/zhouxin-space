---
title: ã€ŠCMU 10-414 deep learning systemã€‹å­¦ä¹ ç¬”è®°
tags:
  - CUDA
  - æ·±åº¦å­¦ä¹ ç³»ç»Ÿ
date: 2024-05-28T12:24:00+08:00
lastmod: 2024-09-08T20:38:00+08:00
publish: true
dir: notes
slug: notes on cmu 10-414 deep learning system
images:
  - https://pics.zhouxin.space/202406041918872.png?x-oss-process=image/quality,q_90/format,webp
math: "true"
---

# å†™åœ¨æœ€å‰é¢

ä» 2024-04-28 åˆ° 2024-09-08ï¼Œå†å²å››ä¸ªå¤šæœˆï¼Œæ€»ç®—æŠŠ DLSys å­¦å®Œäº†ã€‚è¿™é—¨è¯¾çš„ä¸€äº›æ”¶è·ï¼š

- è‡ªåŠ¨å¾®åˆ†ç†è®ºçŸ¥è¯†å’Œåœ¨å®è·µè¿‡ç¨‹ä¸­è¡ç”Ÿçš„åŒ…æ‹¬è®¡ç®—å›¾ç­‰çŸ¥è¯†
- ç³»ç»Ÿå­¦ä¹ äº† ML ä¸­å‡ ä¸ªåŸºæœ¬æ¨¡å‹å’Œç»„ä»¶
- Tensor çš„ strides ç›¸å…³å†…å®¹
- åŸºç¡€ CUDA ç¼–ç¨‹

ä¸ªäººè®¤ä¸ºè¿™é—¨è¯¾ä¸€äº›æ²¡è¾¾åˆ°æˆ‘é¢„æœŸçš„åœ°æ–¹ï¼š

- CUDA ç¼–ç¨‹çš„å†…å®¹å¤ªæµ…
- åç»­è®² CNNã€RNNã€Transformer çš„éƒ¨åˆ†æ²¡å¿…è¦ï¼Œå¯ä»¥ç»§ç»­æ·±å…¥ CUDA æˆ–è€…å‹ç¼©è¯¾æ—¶

æœ¬é—¨è¯¾ç¨‹çš„æ ¸å¿ƒå†…å®¹åœ¨ Lecture 0~15ï¼Œå¯¹åº”çš„ homework æ˜¯ hw0~3ï¼Œåé¢çš„å†…å®¹æ²¡æœ‰æ—¶é—´å¯ä»¥è·³è¿‡ã€‚

# Lecture 1: Introduction and Logistics

## è¯¾ç¨‹çš„ç›®æ ‡

æœ¬è¯¾ç¨‹çš„ç›®æ ‡æ˜¯å­¦ä¹ ç°ä»£æ·±åº¦å­¦ä¹ ç³»ç»Ÿï¼Œäº†è§£åŒ…æ‹¬è‡ªåŠ¨å¾®åˆ†ã€ç¥ç»ç½‘ç»œæ¶æ„ã€ä¼˜åŒ–ä»¥åŠ GPU ä¸Šçš„é«˜æ•ˆæ“ä½œåœ¨å†…çš„æŠ€æœ¯çš„åº•å±‚åŸç†ã€‚ä½œä¸ºå®è·µï¼Œæœ¬è¯¾ç¨‹å°†å®ç°ä¸€ä¸ª needleï¼ˆdeep learning libraryï¼‰åº“ï¼Œç±»ä¼¼ PyTorchã€‚

## ä¸ºä»€ä¹ˆå­¦ä¹ æ·±åº¦å­¦ä¹ ç³»ç»Ÿï¼Ÿ

ä¸ºä»€ä¹ˆå­¦ä¹ ï¼Ÿæ·±åº¦å­¦ä¹ è¿™ä¸€æ¦‚å¿µå¾ˆæ—©å°±å­˜åœ¨äº†ï¼Œä½†ç›´åˆ° PyTorchã€TensorFlow æ­¤ç±»ç°ä»£æ·±åº¦å­¦ä¹ æ¡†æ¶å‘å¸ƒï¼Œæ·±åº¦å­¦ä¹ æ‰å¼€å§‹è¿…é€Ÿå‘å±•ã€‚ç®€å•æ˜“ç”¨çš„è‡ªåŠ¨å·®åˆ†åº“æ˜¯æ·±åº¦å­¦ä¹ å‘å±•çš„æœ€å¤§åŠ¨åŠ›ã€‚

é™¤äº†ä½¿ç”¨è¿™äº›åº“ï¼Œæˆ‘ä»¬ä¸ºä»€ä¹ˆè¿˜è¦å­¦ä¹ æ·±åº¦å­¦ä¹ ç³»ç»Ÿï¼Ÿ

- ä¸ºäº†æ„å»ºæ·±åº¦å­¦ä¹ ç³»ç»Ÿ  
å¦‚æœæƒ³è¦ä»äº‹æ·±åº¦å­¦ä¹ ç³»ç»Ÿçš„å¼€å‘ï¼Œé‚£æ¯«æ— ç–‘é—®å¾—å…ˆå­¦ä¹ å®ƒã€‚ç›®å‰æ·±åº¦å­¦ä¹ æ¡†æ¶å¹¶æ²¡å®Œå…¨æˆç†Ÿï¼Œè¿˜æœ‰å¾ˆå¤šå¼€å‘æ–°åŠŸèƒ½ï¼Œä¹ƒè‡³æ–°çš„æ¡†æ¶çš„æœºä¼šã€‚

- ä¸ºäº†èƒ½å¤Ÿæ›´é«˜æ•ˆåœ°ä½¿ç”¨ç°æœ‰ç³»ç»Ÿ  
äº†è§£ç°æœ‰ç³»ç»Ÿçš„å†…éƒ¨å®ç°ï¼Œå¯ä»¥å¸®åŠ©æˆ‘ä»¬å†™å‡ºæ›´åŠ é«˜æ•ˆçš„æ·±åº¦å­¦ä¹ ä»£ç ã€‚å¦‚æœæƒ³è¦æé«˜è‡ªå®šä¹‰ç®—å­çš„æ•ˆç‡ï¼Œé‚£å¿…é¡»å…ˆäº†è§£ç›¸å…³æ“ä½œæ˜¯å¦‚ä½•å®ç°çš„ã€‚

- æ·±åº¦å­¦ä¹ ç³»ç»Ÿæœ¬èº«å°±å¾ˆæœ‰è¶£  
å°½ç®¡è¿™ä¸ªç³»ç»Ÿçœ‹ä¸Šå»å¾ˆå¤æ‚ï¼Œä½†æ˜¯å…¶æ ¸å¿ƒç®—æ³•çš„åŸç†ç¡®å®ç›¸å½“ç®€å•çš„ã€‚ä¸¤åƒè¡Œå·¦å³çš„ä»£ç ï¼Œå°±å¯ä»¥å†™å‡ºä¸€ä¸ªæ·±åº¦å­¦ä¹ åº“ã€‚

## é¢„å¤‡çŸ¥è¯†

- systems programming
- çº¿æ€§ä»£æ•°
- å…¶ä»–æ•°å­¦çŸ¥è¯†ï¼šè®¡ç®—ã€æ¦‚ç‡ã€ç®€å•çš„è¯æ˜
- Python å’Œ C++ ç»éªŒ
- æœºå™¨å­¦ä¹ çš„ç›¸å…³ç»éªŒ

# Lecture 2: ML Refresher & Softmax Regression

## æœºå™¨å­¦ä¹ åŸºç¡€

æ·±åº¦å­¦ä¹ æ˜¯ç”±æ•°æ®é©±åŠ¨çš„ï¼Œæ‰€è°“æ•°æ®é©±åŠ¨ï¼Œè¿™æ„å‘³ç€å½“æˆ‘ä»¬æƒ³è¦å†™ä¸€ä¸ªç”¨äºè¯†åˆ«æ‰‹å†™æ•°å­—çš„æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬å…³æ³¨çš„ä¸æ˜¯æŸä¸ªæ•°å­—å½¢çŠ¶ä¸Šæœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Œå¦‚ä½•é€šè¿‡ç¼–ç¨‹è¯†åˆ«è¯¥ç‰¹ç‚¹ï¼Œè€Œæ˜¯ç›´æ¥å°†æ•°æ®é›†å–‚ç»™æ¨¡å‹ï¼Œæ¨¡å‹è‡ªåŠ¨è®­ç»ƒå¹¶è¯†åˆ«æ•°å­—ç±»åˆ«ã€‚

æ·±åº¦å­¦ä¹ æ¨¡å‹ç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼š

- å‡è¯´æ¨¡å‹ï¼šæ¨¡å‹çš„ç»“æ„ï¼ŒåŒ…æ‹¬ä¸€ç³»åˆ—å‚æ•°ï¼Œå…¶æè¿°äº†æ¨¡å‹ä»è¾“å…¥åˆ°è¾“å‡ºçš„æ˜ å°„å…³ç³»ï¼›
- æŸå¤±å‡½æ•°ï¼šæŒ‡å®šäº†å¯¹æ¨¡å‹çš„è¯„ä»·ï¼ŒæŸå¤±å‡½æ•°å€¼è¶Šå°ï¼Œè¯´æ˜è¯¥æ¨¡å‹åœ¨æŒ‡å®šä»»åŠ¡ä¸Šå®Œæˆå¾—æ›´å¥½ï¼›
- ä¼˜åŒ–æ–¹æ³•ï¼šç”¨äºå¯¹æ¨¡å‹ä¸­å‚æ•°è¿›è¡Œä¼˜åŒ–ï¼Œä½¿å¾—æŸå¤±å‡½æ•°æœ€å°çš„æ–¹æ³•ã€‚

## Softmax å›å½’

ä»¥ç»å…¸çš„ softmax å›å½’æ¨¡å‹ä¸ºä¾‹ï¼Œç®€å•å›é¡¾ä¸€ä¸‹ ML æ¨¡å‹ã€‚

è€ƒè™‘ä¸€ä¸ª k åˆ†ç±»ä»»åŠ¡ï¼Œå…¶ä¸­æ•°æ®é›†ä¸º $x^{(i)} \in R^n\ ,\  y^{(i)} \in \{ 1,...,k\}\   \ \  i = 1,...,m$ï¼Œ$n$ æ ‡è¯†è¾“å…¥æ•°æ®é›†çš„ç»´åº¦ï¼Œ$k$ æ ‡è¯†æ ‡ç­¾ç±»åˆ«æ•°ï¼Œ$m$ æ ‡è¯†æ•°æ®é›†æ ·æœ¬æ•°é‡ã€‚

ä¸€ä¸ªå‡è¯´æ¨¡å‹å°±æ˜¯å°†ä¸€ä¸ª $n$ ç»´çš„è¾“å…¥æ˜ å°„åˆ°ä¸€ä¸ª $k$ ç»´çš„è¾“å‡ºï¼Œå³ï¼š$h: R^n \rightarrow R^k$ã€‚æ³¨æ„ï¼Œæ¨¡å‹å¹¶ä¸ä¼šç›´æ¥è¾“å‡ºç±»åˆ«çš„åºå·ï¼Œè€Œæ˜¯é€šè¿‡è¾“å‡ºä¸€ä¸ª $k$ ç»´å‘é‡ $h(x)$ï¼Œå…¶ä¸­ç¬¬ $i$ ä¸ªå…ƒç´  $h_i(x)$ è¡¨ç¤ºæ˜¯ç¬¬ $i$ ä¸ªç±»åˆ«çš„æ¦‚ç‡ã€‚

å¯¹äºçº¿æ€§æ¨¡å‹æ¥è¯´ï¼Œä½¿ç”¨ $\theta \in R^{n\times k}$ è¿™ä¸ªæ¨¡å‹ä¸­çš„å‚æ•°ï¼Œé‚£ä¹ˆ $h_\theta(x) = \theta^T x$ã€‚

å¦‚æœä¸€æ¬¡è¾“å…¥å¤šä¸ªæ•°æ®ï¼Œé‚£ä¹ˆè¾“å…¥æ•°æ®å°±å¯ä»¥ç»„ç»‡æˆä¸€ä¸ªçŸ©é˜µï¼Œç›¸æ¯”èµ·å¤šä¸ªå‘é‡æ“ä½œï¼ŒçŸ©é˜µçš„æ“ä½œé€šå¸¸æ•ˆç‡æ›´é«˜ï¼Œæˆ‘ä»¬åœ¨ä»£ç å®ç°ä¸­ä¸€èˆ¬ä¹Ÿæ˜¯ç”¨çŸ©é˜µæ“ä½œã€‚æ•°æ®é›†å¯ä»¥è¡¨ç¤ºä¸ºï¼š

{{< math_block >}}
X\in R^{m\times n}=\left[ \begin{array}{c}
	x^{(1)T}\\
	\vdots\\
	x^{\left( m \right) T}\\
\end{array} \right] ,  y\in \left\{ 1,...,k \right\} ^m=\left[ \begin{array}{c}
	y^{\left( 1 \right)}\\
	\vdots\\
	y^{\left( m \right)}\\
\end{array} \right]
{{< /math_block >}}

æ•°æ®é›†çš„çŸ©é˜µæ˜¯ä¸€ä¸ªä¸ªæ ·æœ¬è½¬ç½®åå †å  stack èµ·æ¥çš„ã€‚é‚£ä¹ˆè¾“å‡ºå¯ä»¥è¡¨ç¤ºä¸ºï¼š

{{< math_block >}}
h_{\theta}\left( X \right) =\left[ \begin{array}{c}
	h_{\theta}\left( x^{\left( 1 \right)} \right) ^T\\
	\vdots\\
	h_{\theta}\left( x^{\left( m \right)} \right) ^T\\
\end{array} \right] =\left[ \begin{array}{c}
	x^{\left( 1 \right) T}\theta\\
	\vdots\\
	x^{\left( m \right) T}\theta\\
\end{array} \right] =X\theta
{{< /math_block >}}

å…³äºæŸå¤±å‡½æ•° $l_{err}$ï¼Œä¸€ç§æœ´ç´ çš„æƒ³æ³•æ˜¯å°†æ¨¡å‹é¢„æµ‹é”™è¯¯çš„æ¨¡å‹æ•°æ®é‡ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œå³å¦‚æœæ¨¡å‹é¢„æµ‹çš„æ­£ç¡®ç‡æœ€é«˜çš„é‚£ä¸ªç±»åˆ«ä¸çœŸå®ç±»åˆ«ä¸ç›¸åŒï¼Œåˆ™æŸå¤±å‡½æ•°ä¸º 1ï¼Œå¦åˆ™ä¸º 0ï¼š

{{< math_block >}}
l_{err}\left( h\left( x \right) , y \right) \,\,=\,\,\left\{ \begin{align*}
	0 \ &\mathrm{if} \ \mathrm{argmax} _i\,\,h_i\left( x \right) =y\\
	1 \ &\mathrm{otherwise}\\
\end{align*} \right.
{{< /math_block >}}

é—æ†¾çš„æ˜¯ï¼Œè¿™ä¸ªç¬¦åˆç›´è§‰å‡½æ•°æ˜¯ä¸å¯å¾®åˆ†çš„ï¼Œéš¾ä»¥å¯¹å‚æ•°è¿›è¡Œä¼˜åŒ–ã€‚æ›´åˆé€‚çš„åšæ³•æ˜¯ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°ã€‚

åœ¨æ­¤ä¹‹å‰ï¼Œæˆ‘ä»¬å°†å…ˆè®²è¾“å‡ºè¿‡ä¸€ä¸ª softmax å‡½æ•°ï¼Œä½¿ä¹‹çš„è¡Œä¸ºæ›´åƒä¸€ä¸ªæ¦‚ç‡â€”â€”å„ä¸ªç±»åˆ«çš„æ¦‚ç‡ä¹‹å’Œä¸º 1ï¼š

{{< math_block >}}
z_i=p\left( \mathrm{label}=i \right) =\frac{\exp \left( h_i\left( x \right) \right)}{\sum_{j=1}^k{\exp \left( h_j\left( x \right) \right)}}
{{< /math_block >}}

é‚£ä¹ˆäº¤å‰ç†µæŸå¤±å‡½æ•°å°±å¯ä»¥å®šä¹‰ä¸ºï¼š

{{< math_block >}}
l_{ce}\left( h\left( x \right) ,y \right) =-\log p\left( \mathrm{label}=y \right) =-h_y\left( x \right) +\log \sum_{j=1}^k{\exp \left( h_j\left( x \right) \right)}
{{< /math_block >}}

æ³¨æ„åœ¨è®¡ç®—äº¤å‰ç†µæ—¶ï¼Œé€šè¿‡è¿ç®—è¿›è¡Œäº†åŒ–ç®€ï¼Œè¿™ä½¿å¾—æˆ‘ä»¬å¯ä»¥çœå»è®¡ç®— softmax çš„è¿‡ç¨‹ï¼Œç›´æ¥è®¡ç®—æœ€ç»ˆçš„ç»“æœã€‚ä¸ä½†å¦‚æ­¤ï¼Œäº¤å‰ç†µçš„è®¡ç®—ä¸­ï¼Œå¦‚æœ $h_i(x)$ çš„å€¼å¾ˆå°ï¼Œé‚£ä¹ˆå–å¯¹æ•°ä¼šå‡ºç°å¾ˆå¤§çš„å€¼ï¼ŒåŒ–ç®€åçš„è®¡ç®—åˆ™é¿å…äº†è¿™ç§æƒ…å†µã€‚

æ‰€æœ‰çš„æ·±åº¦å­¦ä¹ é—®é¢˜ï¼Œéƒ½å¯ä»¥å½’ç»“ä¸ºä¸€ä¸‹è¿™ä¸ªæœ€ä¼˜åŒ–é—®é¢˜ï¼š{{< math_block >}}
\mathop {\mathrm{minimize}} \limits_{\theta}\ \ \frac{1}{m}\sum_{i=1}^m{l(h_{\theta}(x^{(i)}),y^{(i)}))}
{{< /math_block >}}
æˆ‘ä»¬ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•å¯¹è¯¥é—®é¢˜è¿›è¡Œä¼˜åŒ–ã€‚åœ¨æ­¤ä¹‹å‰ï¼Œé¦–å…ˆä»‹ç»ä¸€ä¸‹å…³äºæ¢¯åº¦ã€‚æˆ‘ä»¬çš„ä¼˜åŒ–ç›®æ ‡å¯ä»¥çœ‹ä½œä¸€ä¸ªå…³äº$\theta \in R^{n\times k}$çš„å‡½æ•°$f$ï¼Œé‚£ä¹ˆå…¶åœ¨$\theta_0$å¤„çš„æ¢¯åº¦å¯ä»¥è¡¨ç¤ºä¸ºï¼š
{{< math_block >}}
\nabla _{\theta}f\left( \theta _0 \right) \in R^{n\times k}=\left[ \begin{matrix}  
	\frac{\partial f\left( \theta _0 \right)}{\partial \theta _{11}}&		\cdots&		\frac{\partial f\left( \theta _0 \right)}{\partial \theta _{k1}}\\  
	\vdots&		\ddots&		\vdots\\  
	\frac{\partial f\left( \theta _0 \right)}{\partial \theta _{n1}}&		\cdots&		\frac{\partial f\left( \theta _0 \right)}{\partial \theta _{nk}}\\  
\end{matrix} \right]
{{< /math_block >}}
å…¶ä¸­ï¼Œç¬¬$i$è¡Œç¬¬$j$ä¸ªå…ƒç´ è¡¨ç¤ºé™¤$\theta_{ij}$ä¹‹å¤–çš„å‚æ•°éƒ½è¢«å½“ä½œå¸¸æ•°ï¼Œå¯¹$\theta_{ij}$æ±‚åå¯¼ã€‚

æ¢¯åº¦ä¸‹é™ï¼Œå°±æ˜¯æ²¿ç€æ¢¯åº¦æ–¹å‘ä¸æ–­è¿›è¡Œè¿­ä»£ï¼Œä»¥æ±‚æ‰¾åˆ°æœ€ä½³çš„$\theta$ä½¿å¾—ç›®æ ‡å‡½æ•°å€¼æœ€å°ã€‚
{{< math_block >}}
\theta :=\theta _0-\alpha \nabla f\left( \theta _0 \right)
{{< /math_block >}}
ä¸Šå¼ä¸­ï¼Œ$\alpha$è¢«ç§°ä¸ºå­¦ä¹ ç‡æˆ–è€…æ­¥é•¿ã€‚

äº‹å®ä¸Šï¼Œåœ¨ç°ä»£æ·±åº¦å­¦ä¹ ä¸­ï¼Œå¹¶ä¸æ˜¯ä½¿ç”¨çš„ä¼ ç»Ÿæ¢¯åº¦ä¸‹é™çš„æ–¹æ¡ˆï¼Œå› ä¸ºå…¶æ— æ³•å°†æ‰€æœ‰è®­ç»ƒé›†ä¸€æ¬¡æ€§è¯»å…¥å¹¶è®¡ç®—æ¢¯åº¦ã€‚ç°ä»£ä½¿ç”¨çš„æ˜¯éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆStochastic Gradient Descentï¼ŒSGDï¼‰

é¦–å…ˆå°†mä¸ªè®­ç»ƒé›†æ ·æœ¬åˆ’åˆ†ä¸€ä¸ªä¸ªå°batchï¼Œæ¯ä¸ªbatchéƒ½æœ‰Bæ¡æ•°æ®ã€‚é‚£æ¯ä¸€batchçš„æ•°æ®è¡¨ç¤ºä¸º$X\in R^{B\times n}$ï¼Œæ›´æ–°å‚æ•°$\theta$çš„å…¬å¼å˜ä¸ºï¼š
{{< math_block >}}
\theta :=\theta _0-\frac{\alpha}{B}\nabla f\left( \theta _0 \right)
{{< /math_block >}}
æˆ‘ä»¬çš„æ¢¯åº¦å˜æˆäº†æ¯ä¸ªå°batchå¯¹å…¨ä½“æ ·æœ¬æ¢¯åº¦çš„ä¼°è®¡ã€‚

é‚£å¦‚ä½•è®¡ç®—æ¢¯åº¦è¡¨è¾¾å¼å‘¢ï¼Ÿæ¢¯åº¦çŸ©é˜µä¸­æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªåå¯¼æ•°ï¼Œæˆ‘ä»¬å°±å…ˆä»è®¡ç®—åå¯¼æ•°å¼€å§‹ã€‚å‡è®¾$h$æ˜¯ä¸ªå‘é‡ï¼Œæˆ‘ä»¬æ¥è®¡ç®—åå¯¼æ•°$\frac{\partial l_{ce}\left( h,y \right)}{\partial h_i}$ï¼š
{{< math_block >}}
\begin{align*}  
\frac{\partial l_{ce}\left( h,y \right)}{\partial h_i}&=\frac{\partial}{\partial h_i}\left( -h_y+\log \sum_{j=1}^k{\exp h_j} \right)  
\\  
&=-1\left\{ i=y \right\} +\frac{\exp \left( h_j \right)}{\sum_{j=1}^k{\exp h_j}}  
\\  
&=-1\left\{ i=y \right\} +\mathrm{softmax} \left( h \right)  
\\  
&=z-e_y  
\end{align*}
{{< /math_block >}}

å¦‚æœ$h$æ˜¯ä¸ªå‘é‡ï¼Œé‚£ä¹ˆæ¢¯åº¦$\nabla_h l_{ce}(h,y)$å°±èƒ½å¤Ÿä»¥å‘é‡çš„å½¢å¼è¡¨ç¤ºä¸ºï¼š
{{< math_block >}}
\nabla_h l_{ce}(h,y) = z-e_y
{{< /math_block >}}
è¿™é‡Œæˆ‘ä»¬å°†å¯¹$h$è¿›è¡Œsoftmaxæ ‡å‡†åŒ–è®°ä¸º$z$ï¼Œ$e_y$è¡¨ç¤ºå¯¹åº”çš„å•ä½å‘é‡ã€‚

äº‹å®ä¸Šï¼Œæˆ‘ä»¬è¦è®¡ç®—çš„æ¢¯åº¦æ˜¯å…³äº$\theta$çš„ï¼Œå…·ä½“æ¥è¯´ï¼Œè¡¨è¾¾å¼ä¸º$\nabla_\theta l_{ce}(\theta^Tx,y)$ï¼Œå…¶ä¸­ï¼Œ$\theta$æ˜¯ä¸ªçŸ©é˜µã€‚æˆ–è®¸ï¼Œå¯ä»¥ä½¿ç”¨é“¾å¼æ³•åˆ™è¿›è¡Œæ±‚è§£ï¼Œä½†æ˜¯å¤ªéº»çƒ¦äº†ï¼Œè¿™é‡Œè¿˜æ¶‰åŠçŸ©é˜µå¯¹å‘é‡çš„æ±‚å¯¼ã€‚æˆ‘ä»¬éœ€è¦ä¸€ç§æ›´åŠ é€šç”¨çš„æ±‚å¯¼æ–¹æ¡ˆã€‚

æœ‰ä¸¤ä¸ªè§£å†³åŠæ³•ï¼š
- æ­£ç¡®ä¸”å®˜æ–¹çš„åšæ³•ï¼šä½¿ç”¨çŸ©é˜µå¾®åˆ†å­¦ã€é›…å¯æ¯”çŸ©é˜µã€å…‹ç½—å†…å…‹ç§¯å’Œå‘é‡åŒ–ç­‰çŸ¥è¯†è¿›è¡Œæ±‚è§£ã€‚
- ä¸€ä¸ªhackyã€ç™»ä¸ä¸Šå°é¢ã€ä½†å¤§å®¶éƒ½åœ¨ç”¨çš„æ–¹æ¡ˆï¼šå°†æ‰€æœ‰çš„çŸ©é˜µå’Œå‘é‡å½“ä½œæ ‡é‡ï¼Œä½¿ç”¨é“¾å¼æ³•åˆ™æ±‚è§£ï¼Œå¹¶è¿›è¡Œè½¬ç½®æ“ä½œä½¿å¾—ç»“æœçš„sizeç¬¦åˆé¢„æœŸï¼Œæœ€åæ£€æŸ¥æ•°å€¼ä¸Šç»“æœæ˜¯å¦æ­£ç¡®ã€‚

æŒ‰ç…§ç¬¬äºŒä¸ªæ–¹æ³•çš„é€»è¾‘ï¼Œè¿‡ç¨‹ä¸ºï¼š
{{< math_block >}}
\begin{align*}  
\frac{\partial}{\partial \theta}l_{ce}\left( \theta ^Tx,y \right) &=\frac{\partial l_{ce}\left( \theta ^Tx,y \right)}{\partial \theta ^Tx}\cdot \frac{\partial \theta ^Tx}{\partial \theta}  
\\  
&=\left[ z-e_y \right] _{k\times 1}\cdot x_{n\times 1}  
\\  
&=x\cdot \left[ z-e_y \right]  
\end{align*}
{{< /math_block >}}
å…¶ä¸­ï¼Œ$z=\text{softmax}(\theta^Tx)$ã€‚æ³¨æ„ï¼Œå€’æ•°ç¬¬äºŒæ­¥æ±‚å‡ºçš„ç»“æœæ˜¯ä¸¤ä¸ªåˆ—å‘é‡ç›¸ä¹˜ï¼Œä¸èƒ½è¿ç®—ã€‚åˆå·²çŸ¥ç»“æœåº”è¯¥æ˜¯$n\times k$çš„çŸ©é˜µï¼Œè°ƒæ•´å‘é‡ä¹‹é—´çš„é¡ºåºå³å¯ã€‚

ç…§çŒ«ç”»è™ï¼Œå¯ä»¥å†™å‡ºbatchçš„æƒ…å†µï¼Œ$X\in R^{B\times n}$ï¼š
{{< math_block >}}
\begin{align*}  
\frac{\partial}{\partial \theta}l_{ce}\left( \theta ^TX,y \right) &=\frac{\partial l_{ce}\left( \theta ^TX,y \right)}{\partial \theta ^TX}\cdot \frac{\partial \theta ^TX}{\partial \theta}  
\\  
&=\left[ Z-E_y \right] _{B\times k}\cdot X_{B\times n}  
\\  
&=X^T\cdot \left[ Z-E_y \right]  
\end{align*}
{{< /math_block >}}

# Lecture 3: Manual Neural Networks
è¿™èŠ‚è¯¾ï¼Œæˆ‘ä»¬å°†äººå·¥å®ç°å…¨è¿æ¥ç¥ç»ç½‘ç»œï¼Œä¹‹åçš„è¯¾ç¨‹ï¼Œå°†å¼•å…¥è‡ªåŠ¨å¾®åˆ†æŠ€æœ¯ã€‚
## ä»çº¿æ€§æ¨¡å‹è½¬å˜ä¸ºéçº¿æ€§æ¨¡å‹
![image.png](https://pics.zhouxin.space/202406071816708.png?x-oss-process=image/quality,q_90/format,webp)

å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œçº¿æ€§æ¨¡å‹æœ¬è´¨ä¸Šæ˜¯å°†æ ·æœ¬ç©ºé—´åˆ’åˆ†ä¸ºçº¿æ€§çš„å‡ ä¸ªéƒ¨åˆ†ï¼Œè¿™æ ·çš„æ¨¡å‹æ€§èƒ½ååˆ†æœ‰é™ï¼Œå› æ­¤å¾ˆå¤šä¸æ»¡è¶³è¿™æ ·åˆ†å¸ƒçš„å®é™…é—®é¢˜å°±ä¸èƒ½è¢«è§£å†³ã€‚

ä¸€ç§è§£å†³æ–¹æ¡ˆæ˜¯ï¼Œåœ¨å°†æ ·æœ¬è¾“å…¥åˆ°çº¿æ€§åˆ†ç±»å™¨å‰ï¼Œå…ˆäººå·¥æŒ‘é€‰å‡ºæŸäº›ç‰¹å¾ï¼Œå³å¯¹$X$åº”ç”¨ä¸€ä¸ªå‡½æ•°$\phi$ï¼Œå…¶å°†$X$æ˜ å°„åˆ°$\phi(X)$ä¸Šï¼Œæ˜ å°„åçš„ç©ºé—´å¯ä»¥è¢«çº¿æ€§åˆ’åˆ†ã€‚ä¸€æ–¹é¢ï¼Œå®ƒç¡®å®æ˜¯æ—©æœŸå®è·µä¸­è¡Œä¹‹æœ‰æ•ˆçš„æ–¹æ¡ˆï¼›å¦ä¸€æ–¹é¢ï¼Œäººå·¥æå–ç‰¹å¾çš„æ³›åŒ–æ€§èƒ½æœ‰é™ï¼Œå—é™äºå…·ä½“é—®é¢˜å’Œç ”ç©¶äººå‘˜çš„å¯¹é—®é¢˜çš„æ´å¯Ÿç¨‹åº¦ã€‚

å¦‚æœæˆ‘ä»¬ä½¿ç”¨çº¿æ€§ç½‘ç»œæå–ç‰¹å¾ï¼Œå¹¶ç›´æ¥æ¥ä¸Šä¸€ä¸ªçº¿æ€§åˆ†ç±»å¤´ï¼Œè¿™ä¸¤ä¸ªçº¿æ€§å±‚ç­‰æ•ˆä¸ºä¸€ä¸ªçº¿æ€§å±‚ï¼Œå¹¶ä¸èƒ½åšåˆ°éçº¿æ€§åŒ–çš„è¦æ±‚ï¼ˆåŸºç¡€çŸ¥è¯†ï¼Œæ­¤å¤„ä¸å†è§£é‡Šï¼‰ã€‚

å› æ­¤ï¼Œåœ¨ä½¿ç”¨çº¿æ€§ç½‘ç»œæå–ç‰¹å¾åï¼Œéœ€è¦å†æ¥ä¸Šä¸€ä¸ªéçº¿æ€§å‡½æ•°$\sigma$ï¼Œå³$\phi = \sigma (W^T X)$ã€‚
## ç¥ç»ç½‘ç»œ
ä¸Šæ–‡æåˆ°çš„ä½¿ç”¨éçº¿æ€§å‡½æ•°åçš„æ¨¡å‹ï¼Œå°±å¯ä»¥è§†ä½œä¸€ç§æœ€ç®€å•çš„ç¥ç»ç½‘ç»œã€‚æ‰€è°“ç¥ç»ç½‘ç»œï¼Œå€¼å¾—æ˜¯æœºå™¨å­¦ä¹ ä¸­æŸä¸€ç±»ç‰¹å®šçš„å‡è¯´æ¨¡å‹ï¼Œå…¶ç”±å¤šå±‚ç»„æˆï¼Œæ¯ä¸€å±‚éƒ½æœ‰å¤§é‡å¯ä»¥å¾®åˆ†çš„å‚æ•°ã€‚

ç¥ç»ç½‘ç»œæœ€åˆçš„ç¡®èµ·æºäºæ¨¡æ‹Ÿäººç±»ç¥ç»å…ƒè¿™ä¸€åŠ¨æœºï¼Œä½†éšç€å…¶å‘å±•ï¼Œè¶Šæ¥è¶Šå¤šçš„ç¥ç»ç½‘ç»œæ¨¡å‹å‡ºç°ï¼Œä¸äººç±»å¤§è„‘ç¥ç»ç½‘ç»œè¶Šæ¥è¶Šä¸ç›¸å…³ã€‚

ä»¥åŒå±‚ç¥ç»ç½‘ç»œä¸ºä¾‹ï¼Œå…¶å½¢å¼åŒ–è¡¨ç¤ºä¸º$h_\theta(x) = W_2^T \sigma(W_1^T x)$ï¼Œæ‰€æœ‰å¯å­¦ä¹ çš„å‚æ•°ä½¿ç”¨$\theta$è¡¨ç¤ºã€‚ä»¥batchçš„çŸ©é˜µå½¢å¼è¡¨ç¤ºä¸ºï¼š
{{< math_block >}}
h_\theta(X) = \sigma(XW_1)W_2
{{< /math_block >}}
æ¥ä¸‹æ¥ç»™å‡ºLå±‚å¤šå±‚æ„ŸçŸ¥æœºï¼ˆa.k.a. MLPã€å‰é¦ˆç¥ç»ç½‘ç»œã€å…¨è¿æ¥å±‚ï¼‰çš„å½¢å¼åŒ–è¡¨è¾¾ï¼š
{{< math_block >}}
\left\{\begin{array}{l}  
Z_{i+1} = \sigma_i(Z_iW_i), i=1,...,L  \\  
Z_1 = X\\  
h_\theta(X) =Z_{L+1}\\  
[Z_i\in R^{m\times n_i}, W_i \in R^{n_i\times n_{i+1}}]\\  
\sigma_i:R\rightarrow R

\end{array} \right.
{{< /math_block >}}
æ¯ä¸€å±‚çš„è¾“å…¥ä¸º$Z_i$ï¼Œè¾“å‡ºä¸º$Z_{i+1}$ã€‚

ä¸ºä»€ä¹ˆè¦æ˜¯ç”¨æ·±åº¦ç½‘ç»œè€Œä¸æ˜¯å®½åº¦ç½‘ç»œï¼Ÿæ²¡æœ‰å¾ˆå®Œç¾çš„è§£é‡Šï¼Œä½†æœ€å¥½å¹¶ä¸”æœ€ç°å®çš„è§£é‡Šæ˜¯ï¼šç»éªŒè¯æ˜ï¼Œå½“å‚æ•°é‡å›ºå®šæ—¶ï¼Œæ·±åº¦ç½‘ç»œæ€§èƒ½ä¼˜äºå®½åº¦ç½‘ç»œã€‚
## åå‘ä¼ æ’­ï¼ˆæ¢¯åº¦è®¡ç®—ï¼‰
ä¸Lecture 2ä¸€è‡´ï¼Œä½¿ç”¨äº¤å‰ç†µä½œä¸ºæŸå¤±å‡½æ•°ï¼Œä½¿ç”¨SGDä½œä¸ºä¼˜åŒ–ç®—æ³•ï¼Œå”¯ä¸€çš„åŒºåˆ«æ˜¯ï¼Œè¿™æ¬¡è¦å¯¹MLPç½‘ç»œæ±‚è§£æ¢¯åº¦ã€‚

å¯¹äºä¸¤å±‚ç¥ç»ç½‘ç»œ$h_\theta(X) = \sigma(XW_1)W_2$ï¼Œå¾…æ±‚çš„æ¢¯åº¦è¡¨è¾¾å¼ä¸ºï¼š
{{< math_block >}}
\nabla_{\{W_1, W_2\}}l_{ce}(\sigma(XW_1)W_2,y)
{{< /math_block >}}
å¯¹äº$W_2$çš„æ¢¯åº¦ï¼Œå…¶ä¸Lecture 2çš„è®¡ç®—ç±»ä¼¼ï¼š
{{< math_block >}}
\begin{align*}  
\frac{\partial l_{ce}(\sigma(XW_1)W_2,y)}{\partial W_2}&=\frac{\partial l_{ce}(\sigma(XW_1)W_2,y)}{\partial \sigma(XW_1)W_2} \cdot \frac{\partial\sigma(XW_1)W_2}{\partial W_2}\\  
&=(S-I_y)_{m\times k}\cdot \sigma(XW_1)_{m\times d}\\  
&=\sigma(XW_1)^T\cdot (S-I_y)\\  
&[S=\text{softmax}(\sigma(XW_1))]  
\end{align*}
{{< /math_block >}}

å¯¹äº$W_1$çš„æ¢¯åº¦ï¼Œå…¶éœ€è¦å¤šæ¬¡åº”ç”¨é“¾å¼æ³•åˆ™ï¼Œä½†å¹¶ä¸éš¾è®¡ç®—ï¼š
{{< math_block >}}
\begin{align*}  
\frac{\partial l_{ce}(\sigma(XW_1)W_2,y)}{\partial W_1}&=\frac{\partial l_{ce}(\sigma(XW_1)W_2,y)}{\partial \sigma(XW_1)W_2} \cdot \frac{\partial\sigma(XW_1)W_2}{\partial \sigma(XW_1)}\cdot \frac{\partial \sigma(XW_1)}{\partial XW_1}\cdot\frac{\partial XW_1}{\partial X_1}\\  
&=(S-I_y)_{m\times k}\cdot [W_2]_{d\times k}\cdot \sigma\prime(XW_1)_{m\times d}\cdot X_{m\times n}\\  
&=X^T\cdot [\sigma\prime(XW_1)\odot((S-I_y)\cdot W_2^T)]\\  
&[S=\text{softmax}(\sigma(XW_1))]  
\end{align*}
{{< /math_block >}}
ä»¥ä¸Šå…¬å¼ä¸­$\odot$è¡¨ç¤ºé€å…ƒç´ ä¹˜æ³•ã€‚è‡³äºä¸ºå•¥è¿™ä¹ˆç®—ï¼Œä¿ºä¹Ÿä¸çŸ¥é“ã€‚

æ¥ä¸‹æ¥å°†å…¶æ¨å¹¿åˆ°ä¸€èˆ¬æƒ…å†µï¼Œå³$L$å±‚çš„MLPä¸­å¯¹$W_i$æ±‚å¯¼ï¼š
{{< math_block >}}
\begin{align*}  
\frac{\partial l(Z_{l+1},y)}{\partial W_i} &=\frac{\partial l}{\partial Z_{l+1}}\cdot \frac{\partial Z_{l+1}}{\partial Z_{l}}\cdot...\cdot \frac{\partial Z_{i+2}}{\partial Z_{i+1}}\cdot\frac{\partial Z_{i+1}}{\partial W_{i}}\\  
&=G_{i+1}\cdot\frac{\partial Z_{i+1}}{\partial W_{i}}=\frac{\partial l}{\partial Z_{i+1}}\cdot \frac{\partial Z_{i+1}}{W_i}\\

\end{align*}
{{< /math_block >}}

ç”±ä¸Šè¿°å…¬å¼ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°ä¸€ä¸ªåå‘è¿­ä»£è®¡ç®—çš„$G_i$ï¼Œå³ï¼š
{{< math_block >}}
\begin{align*}  
G_i &= G_{i+1}\cdot \frac{Z_{i+1}}{Z_i} \\  
&=G_{i+1}\cdot \frac{\partial \sigma(Z_iW_i)}{\partial Z_iW_i}\cdot\frac{\partial Z_iW_i}{Z_i}\\  
&=G_{i+1}\cdot \sigma\prime(Z_iW_i)\cdot W_i\\  
\end{align*}
{{< /math_block >}}

ä¸Šé¢çš„è®¡ç®—éƒ½æ˜¯å°†çŸ©é˜µå½“ä½œæ ‡é‡è¿›è¡Œçš„ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬è€ƒè™‘å…¶ç»´åº¦ã€‚å·²çŸ¥ï¼Œ$Z_i \in R^{m\times n_i}$æ˜¯ç¬¬$i$å±‚çš„è¾“å…¥ï¼Œ$G_i = \frac{\partial l}{\partial Z_{i}}$ï¼Œå…¶ç»´åº¦å¦‚ä½•å‘¢ï¼Ÿ$G_i$æ¯ä¸ªå…ƒç´ è¡¨ç¤ºæŸå¤±å‡½æ•°$l$å¯¹ç¬¬$i$å±‚è¾“å…¥çš„æ¯ä¸€é¡¹æ±‚åå¯¼ï¼Œä¹Ÿå¯ä»¥è®°ä½œæ˜¯$l$å¯¹$Z_i$æ±‚æ¢¯åº¦ï¼Œå³$\nabla_{Z_i} l$ï¼Œå…¶ç»´åº¦æ˜¾ç„¶æ˜¯$m\times n_i$ï¼Œç»§ç»­è®¡ç®—å‰æ–‡$G_i$ï¼š
{{< math_block >}}
\begin{align*}  
G_i &=[G_{i+1}]_{m\times n_{i+1}}\cdot \sigma\prime(Z_iW_i)_{m\times n_{i+1}}\cdot [W_i]_{n_i\times n_{i+1}}\\  
&= [G_{i+1}\odot \sigma\prime(Z_iW_i)]W_i^T  
\end{align*}
{{< /math_block >}}

æœ‰äº†$G_i$ï¼Œå°±å¯ä»¥ç»§ç»­è®¡ç®—$l$å¯¹$W_i$çš„åå¯¼æ•°äº†ï¼š
{{< math_block >}}
\begin{align*}  
\frac{\partial l(Z_{l+1},y)}{\partial W_i} &=G_{i+1}\cdot\frac{\partial Z_{i+1}}{\partial W_{i}} \\  
&=G_{i+1}\cdot \frac{\partial\sigma(Z_iW_i)}{\partial Z_iW_i}\cdot\frac{\partial Z_iW_i}{\partial W_i}\\  
&=[G_{i+1}]_{m\times n_{i+1}}\cdot \sigma\prime(Z_iW_i)_{m\times n_{i+1}}\cdot [Z_i]_{m\times n_i}\\  
&=Z_i^T\cdot[G_{i+1}\odot\sigma\prime(Z_iW_i)]  
\end{align*}
{{< /math_block >}}

è‡³æ­¤ï¼Œæ¯ä¸ªå°ç»„ä»¶éƒ½å·²åˆ¶é€ å®Œæ¯•ï¼Œè®©æˆ‘ä»¬æ¥æŠŠå®ƒè£…èµ·æ¥å§ï¼
- å‰å‘ä¼ æ’­
	- åˆå§‹åŒ–ï¼š$Z_1 = X$
	- è¿­ä»£ï¼š$Z_{i+1} = \sigma(Z_iW_i)$ ç›´è‡³$i=L$ï¼ˆæ³¨æ„ï¼Œæœ€åä¸€å±‚æ²¡æœ‰éçº¿æ€§éƒ¨åˆ†ï¼Œæ­¤å¤„æ²¡æœ‰å±•ç¤ºå‡ºæ¥ï¼‰
- åå‘ä¼ æ’­
	- åˆå§‹åŒ–ï¼š$G_{L+1} = S-I_y$
	- è¿­ä»£ï¼š$G_i=[G_{i+1}\odot \sigma\prime(Z_iW_i)]W_i^T$ ç›´è‡³$i=1$
å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨åå‘ä¼ æ’­ä¸­ï¼Œéœ€è¦ç”¨åˆ°å‰å‘ä¼ æ’­çš„ä¸­é—´ç»“æœ$Z_i$ã€‚ä¸ºäº†æ›´é«˜æ•ˆåœ°è®¡ç®—æ¢¯åº¦ï¼Œä¸å¾—ä¸ä»¥ç‰ºç‰²å†…å­˜ç©ºé—´ä¸ºä»£ä»·ï¼Œå³ç©ºé—´æ¢æ—¶é—´ã€‚

> è®¸å¤šè¯¾ç¨‹ï¼Œè®²åˆ°è¿™é‡Œå°±ç»“æŸäº†ï¼Œä½†å¯¹æˆ‘ä»¬è¿™é—¨è¯¾æ¥è¯´ï¼Œæ‰åˆšåˆšå¼€å§‹...

# Lecture 4: Automatic Differentiation
## åŸºæœ¬å·¥å…·
- è®¡ç®—å›¾
è®¡ç®—å›¾æ˜¯è‡ªåŠ¨å¾®åˆ†ä¸­å¸¸ç”¨çš„ä¸€ç§å·¥å…·ã€‚è®¡ç®—å›¾æ˜¯ä¸€å¼ æœ‰å‘æ— ç¯å›¾ï¼Œæ¯ä¸ªèŠ‚ç‚¹è¡¨ç¤ºï¼ˆä¸­é—´ç»“æœï¼‰å€¼ï¼Œæ¯æ¡è¾¹è¡¨ç¤ºè¾“å…¥è¾“å‡ºå˜é‡ã€‚ä¾‹å¦‚ï¼Œ$y=f(x_1, x_2) = \ln(x_1)+x_1x_2-\sin x_2$å¯¹åº”çš„è®¡ç®—å›¾ä¸ºï¼š
![](https://pics.zhouxin.space/202406071612073.webp?x-oss-process=image/quality,q_90/format,webp)
æŒ‰ç…§æ‹“æ‰‘åºåˆ—éå†è¿™å¼ å›¾ï¼Œå°±å¯ä»¥å¾—åˆ°å¯¹åº”è¡¨è¾¾å¼çš„å€¼ã€‚
## å¯¹è‡ªåŠ¨å¾®åˆ†æ–¹æ³•çš„ç®€å•ä»‹ç»
æ·±åº¦å­¦ä¹ ä¸­ï¼Œä¸€ä¸ªæ ¸å¿ƒå†…å®¹å°±æ˜¯è®¡ç®—æ¢¯åº¦ã€‚è¿™é‡Œä»‹ç»é›†ä¸­è®¡ç®—æ¢¯åº¦çš„æ–¹æ¡ˆï¼š
- åå¯¼æ•°å®šä¹‰
- 
æ¢¯åº¦æ˜¯ç”±ä¸€ä¸ªä¸ªåå¯¼æ•°ç»„æˆçš„ï¼Œå¯ä»¥ç›´æ¥æ ¹æ®åå¯¼æ•°çš„å®šä¹‰æ¥è®¡ç®—æ¢¯åº¦ï¼š
{{< math_block >}}
\frac{\partial f(\theta)}{\partial \theta_i} = \lim_{\epsilon \to 0}\frac{f(\theta + \epsilon e_i) - f(\theta)}{\epsilon}
{{< /math_block >}}
å…¶ä¸­ï¼Œ$e_i$æ˜¯è¡¨ç¤ºç¬¬$i$ä¸ªæ–¹å‘ä¸Šçš„å•ä½å‘é‡ã€‚

- æ•°å€¼æ±‚è§£
æ ¹æ®ä¸Šè¿°å®šä¹‰ï¼Œæˆ‘ä»¬å¯ä»¥é€‰å–ä¸€ä¸ªå¾ˆå°çš„é‡ä»£å…¥$\epsilon$ï¼Œå¾—åˆ°æ•°å€¼è®¡ç®—åå¯¼çš„æ–¹æ³•ï¼š
{{< math_block >}}
\frac{\partial f(\theta)}{\partial \theta_i} = \frac{f(\theta + \epsilon e_i) - f(\theta - \epsilon e_i)}{2\epsilon} + o(\epsilon^2)
{{< /math_block >}}
è¿™é‡Œå¹¶ä¸æ˜¯ç›´æ¥ä½¿ç”¨ç¬¬ä¸€é¡¹çš„å…¬å¼ï¼Œå³åˆ†å­ä¸æ˜¯$f(\theta + \epsilon e_i) - f(\theta)$ï¼Œå¹¶ä¸”è¯¯å·®é¡¹æ˜¯$\epsilon^2$ï¼Œè¿™æ˜¯ç”±äºæ³°å‹’å±•å¼€ï¼š
{{< math_block >}}
\begin{align*}  
f(\theta+\delta) = f(\theta)+f^\prime (\theta)\delta+\frac{1}{2}f^{\prime \prime}(\theta)\delta^2+o(\delta^3)\\  
f(\theta-\delta) = f(\theta)+f^\prime (\theta)\delta-\frac{1}{2}f^{\prime \prime}(\theta)\delta^2+o(\delta^3)  
\end{align*}
{{< /math_block >}}
ä¸Šè¿°ä¸¤å¼ä½œå·®ï¼Œå³å¯å¾—åˆ°æ•°å€¼è®¡ç®—$f^\prime(\theta)$çš„æ–¹æ³•ã€‚

è¿™ä¸ªæ–¹æ³•çš„é—®é¢˜åœ¨äºå­˜åœ¨è¯¯å·®ï¼Œå¹¶ä¸”æ•ˆç‡ä½ä¸‹ï¼ˆè¿™é‡Œè¦è®¡ç®—ä¸¤æ¬¡fï¼‰ï¼Œè¯¥æ–¹æ³•å¸¸ç”¨äºéªŒè¯å…¶å®ƒæ–¹æ³•çš„å…·ä½“å®ç°æ˜¯å¦å‡ºé”™ã€‚å…·ä½“æ¥è¯´ï¼ŒéªŒè¯å¦‚ä¸‹ç­‰å¼æ˜¯å¦æˆç«‹ï¼š
{{< math_block >}}
\delta^T \nabla_\theta f(\theta) = \frac{f(\theta + \epsilon \delta) - f(\theta - \epsilon \delta)}{2 \epsilon} + o(\epsilon^2)
{{< /math_block >}}
å…¶ä¸­$\delta$æ˜¯å•ä½çƒä¸Šçš„æŸä¸ªå‘é‡ï¼Œ$\nabla_\theta f(\theta)$æ˜¯ä½¿ç”¨å…¶å®ƒæ–¹æ³•è®¡ç®—å¾—åˆ°çš„æ¢¯åº¦ã€‚ç­‰å¼å·¦è¾¹æ˜¯å…¶å®ƒæ–¹æ³•è®¡ç®—çš„æ¢¯åº¦åœ¨$\delta$ä¸Šçš„æŠ•å½±ï¼Œå³ä¾§æ˜¯ä½¿ç”¨æ•°å€¼æ±‚è§£å¾—åˆ°çš„æ¢¯åº¦å€¼ï¼ŒéªŒè¯è¯¥ç­‰å¼æ˜¯å¦æˆç«‹å°±å¯ä»¥åˆ¤æ–­å·¦ä¾§æ¢¯åº¦æ˜¯å¦è®¡ç®—é”™è¯¯ã€‚

- ç¬¦å·å¾®åˆ†
ç¬¦å·å¾®åˆ†ï¼Œå°±æ˜¯æ ¹æ®å¾®åˆ†çš„è®¡ç®—è§„åˆ™ä½¿ç”¨ç¬¦å·æ‰‹åŠ¨è®¡ç®—å¾®åˆ†ã€‚éƒ¨åˆ†è§„åˆ™ä¸ºï¼š
{{< math_block >}}
\begin{align*}  
&\frac{\partial (f(\theta) + g(\theta))}{\partial \theta} = \frac{\partial f(\theta)}{\partial \theta} + \frac{\partial g(\theta)}{\partial \theta}\\  
&\frac{\partial (f(\theta) g(\theta))}{\partial \theta} = g(\theta) \frac{\partial f(\theta)}{\partial \theta} + f(\theta) \frac{\partial g(\theta)}{\partial \theta}\\  
&\frac{\partial f(g(\theta))}{\partial\theta}=\frac{\partial f(g(\theta))}{\partial g(\theta)}\frac{\partial g(\theta)}{\partial\theta}  
\end{align*}
{{< /math_block >}}
æ ¹æ®è¯¥å…¬å¼ï¼Œå¯ä»¥è®¡ç®—å¾—åˆ°$f(\theta) = \prod_{i=1}^{n} \theta_i$çš„æ¢¯åº¦è¡¨è¾¾å¼ä¸ºï¼š$\frac{\partial f(\theta)}{\partial \theta_k} = \prod_{j \neq k}^{n} \theta_j$ã€‚å¦‚æœæˆ‘ä»¬æ ¹æ®è¯¥å…¬å¼æ¥è®¡ç®—æ¢¯åº¦ï¼Œä¼šå‘ç°éœ€è¦è®¡ç®—$n(n-2)$æ¬¡ä¹˜æ³•æ‰èƒ½å¾—åˆ°ç»“æœã€‚è¿™æ˜¯å› ä¸ºåœ¨ç¬¦å·è¿ç®—çš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¿½ç•¥äº†å¯ä»¥åå¤åˆ©ç”¨çš„ä¸­é—´ç»“æœã€‚

- æ­£å‘æ¨¡å¼è‡ªåŠ¨å¾®åˆ† forward mode automatic differentiation
æ²¿ç€è®¡ç®—å›¾çš„æ‹“æ‰‘åºåˆ—ï¼ŒåŒæ ·å¯ä»¥è®¡ç®—å‡ºè¾“å‡ºå…³äºè¾“å…¥çš„å¯¼æ•°ï¼Œè¿˜æ˜¯ä»¥$y=f(x_1, x_2) = \ln(x_1)+x_1x_2-\sin x_2$ä¸ºä¾‹ï¼Œå…¶è®¡ç®—å›¾ä¸ºï¼š
![image.png](https://pics.zhouxin.space/202406071612328.png?x-oss-process=image/quality,q_90/format,webp)


æ•´ä¸ªæ¢¯åº¦è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹ï¼Œåœ¨æ­¤è¿‡ç¨‹ä¸­åº”ç”¨åˆ°äº†å…·ä½“å‡½æ•°çš„æ±‚å¯¼å…¬å¼ï¼š
{{< math_block >}}
\begin{align*}  
&x_1 = 2\\  
&x_2 = 5\\  
&\dot v_{1} =1 \\  
&\dot v_{2} =0 \\  
&\dot{v}_{3} =\dot v_{1}/v_{1}=0.5 \\  
&\dot{v}_{4} =\dot{v}_{1}v_{2}+\dot v_{2}v_{1}=1\times5+0\times2=5 \\  
&\dot v{5} =\dot{v_{2}}\cos v_{2}=0\times\cos5=0 \\  
&\dot{v}_{6} =\dot v_{3}+\dot v_{4}=0.5+5=5.5 \\  
&\dot{v}_{7} =\dot{v_{6}}-\dot{v_{5}}=5.5-0=5.5  
\end{align*}
{{< /math_block >}}

å¯¹äº$f:\mathbb{R}^n \to \mathbb{R}^k$ï¼Œå‰å‘ä¼ æ’­éœ€è¦$n$æ¬¡å‰å‘è®¡ç®—æ‰èƒ½å¾—åˆ°å…³äºæ¯ä¸ªè¾“å…¥çš„æ¢¯åº¦ï¼Œè¿™å°±æ„å‘³å‰å‘ä¼ æ’­é€‚åˆ$n$æ¯”è¾ƒå°ã€$k$æ¯”è¾ƒå¤§çš„æƒ…å†µã€‚ä½†æ˜¯åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œé€šå¸¸$n$æ¯”è¾ƒå¤§ã€$k$æ¯”è¾ƒå°ã€‚

- åå‘æ¨¡å¼è‡ªåŠ¨å¾®åˆ†
å®šä¹‰$\text{adjoint}:\overline{v_i}=\frac{\partial y}{\partial v_i}$,å…¶è¡¨ç¤ºæŸå¤±å‡½æ•°å¯¹äºå‚æ•°$v_i$çš„åå¯¼ã€‚
æ•´ä¸ªè®¡ç®—è¿‡ç¨‹å¦‚ä¸‹æ‰€ç¤ºï¼Œéœ€è¦æ³¨æ„çš„æ˜¯$\overline{v_2}$çš„è®¡ç®—è¿‡ç¨‹ï¼Œå…¶åœ¨è®¡ç®—å›¾ä¸Šå»¶ä¼¸å‡ºäº†ä¸¤ä¸ªèŠ‚ç‚¹ï¼Œå› æ­¤æ¢¯åº¦ä¹Ÿç”±ä¸¤éƒ¨åˆ†ç›¸åŠ ï¼š
{{< math_block >}}
\begin{align*}  
&\overline{v_{7}}=\frac{\partial y}{\partial v_{7}}=1\\  
&\overline{v_{6}}=\overline{v_{7}}\frac{\partial v_{7}}{\partial v_{6}}=\overline{v_{7}}\times1=1\\  
&\overline{v_{5}}=\overline{v_{7}}\frac{\partial v_{7}}{\partial v_{5}}=\overline{v_{7}}\times(-1)=-1\\  
&\overline{v_{4}}=\overline{v_{6}}\frac{\partial v_{6}}{\partial v_{4}}=\overline{v_{6}}\times1=1\\  
&\overline{v_{3}}=\overline{v_{6}}\frac{\partial v_{6}}{\partial v_{3}}=\overline{v_{6}}\times1=1\\  
&\overline{v_{2}}=\overline{v_{5}}\frac{\partial v_{5}}{\partial v_{2}}+\overline{v_{4}}\frac{\partial v_{4}}{\partial v_{2}}=\overline{v_{5}}\times\cos v_{2}+\overline{v_{4}}\times v_{1}\\  
&\overline{v_{1}}=\overline{v_{4}} \frac{\partial v_{4}}{\partial v_{1}}+\overline{v_{3}} \frac{\partial v_{3}}{\partial v_{1}}=\overline{v_{4}}\times v_{2}+ \overline{v_{3}} \frac{1}{v_{1}}=5+\frac{1}{2}=5.5

\end{align*}
{{< /math_block >}}

æ¥ä¸‹æ¥æˆ‘ä»¬è®¨è®ºä¸€ä¸‹ä¸ºä»€ä¹ˆå‰æ–‡ä¸­$\overline{v_2}$ç”±ä¸¤éƒ¨åˆ†ç»„æˆã€‚è€ƒè™‘å¦‚ä¸‹ä¸€ä¸ªè®¡ç®—å›¾ï¼š
![image.png](https://pics.zhouxin.space/202406071612078.png?x-oss-process=image/quality,q_90/format,webp)

$y$å¯ä»¥è¢«è§†ä½œå…³äº$v_2$å’Œ$v_3$çš„å‡½æ•°ï¼Œå³$y = f(v_2, v_3)$ï¼Œé‚£ä¹ˆï¼š
{{< math_block >}}
\overline{v_{1}}=\frac{\partial y}{\partial v_{1}}=\frac{\partial f(v_{2},v_{3})}{\partial v_{2}}\frac{\partial v_{2}}{\partial v_{1}}+\frac{\partial f(v_{2},v_{3})}{\partial v_{3}} \frac{\partial v_{3}}{\partial v_{1}}=\overline{v_{2}} \frac{\partial v_{2}}{\partial v_{1}}+\overline{v_{3}} \frac{\partial v_{3}}{\partial v_{1}}
{{< /math_block >}}
å› æ­¤ï¼Œå®šä¹‰partial adjoint $\overline{v_{i\to j}} = \overline{v_j} \frac{\partial v_j}{\partial v_i}$ï¼Œé‚£ä¹ˆ$\overline{v_i}$å¯ä»¥è¡¨ç¤ºä¸ºï¼š
{{< math_block >}}
\overline{\nu_i}=\sum_{j\in next(i)}\overline{\nu_{i\rightarrow j}}
{{< /math_block >}}

## åå‘æ¨¡å¼å¾®åˆ†ç®—æ³•çš„å®ç°
åŸºäºä»¥ä¸Šåˆ†æï¼Œå¯ä»¥å†™å‡ºå¦‚ä¸‹çš„å®ç°åå‘æ¨¡å¼å¾®åˆ†ç®—æ³•çš„ä¼ªä»£ç ï¼š
![image.png](https://pics.zhouxin.space/202406071612188.png?x-oss-process=image/quality,q_90/format,webp)

å…¶ä¸­`node_to_grad`æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œä¿å­˜ç€æ¯ä¸ªèŠ‚ç‚¹çš„partial adjointå€¼ã€‚ç”±äºæ˜¯æŒ‰ç…§é€†æ‹“æ‰‘åºåˆ—éå†çš„èŠ‚ç‚¹ï¼Œå› æ­¤å¯ä»¥ä¿è¯å½“éå†åˆ°$i$æ—¶ï¼Œæ‰€æœ‰ä»¥$i$ä¸ºè¾“å…¥çš„èŠ‚ç‚¹ï¼ˆkèŠ‚ç‚¹æ‰€åœ¨çš„é›†åˆï¼‰éƒ½å·²è¢«éå†å®Œæ¯•ï¼Œå³$\overline{v_k}$å·²ç»è®¡ç®—å‡ºæ¥ã€‚

é‚£ä¹ˆpartial adjointå€¼ä½¿ç”¨ä»€ä¹ˆæ•°æ®ç»“æ„ä¿å­˜å‘¢ï¼Ÿä¸€ä¸ªå¸¸è§çš„æ€è·¯æ˜¯ä½¿ç”¨é‚»æ¥çŸ©é˜µï¼Œä½†æ˜¯è¿™ä¸ªçŸ©é˜µä¸­æœ‰å¤§é‡å…ƒç´ æ˜¯ä¸å­˜åœ¨äº†ï¼Œç©ºé—´æµªè´¹å¾ˆå¤§ã€‚æˆ‘ä»¬å¯ä»¥åœ¨åŸæœ‰è®¡ç®—å›¾çš„åŸºç¡€ä¸Šè¿›è¡Œæ‹“å±•æ¥ä¿å­˜partial adjointå’Œadjonitzhiä¹‹é—´çš„è®¡ç®—å…³ç³»ã€‚

å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œé»‘è‰²éƒ¨åˆ†æ˜¯åŸè¡¨è¾¾å¼çš„è®¡ç®—å›¾ï¼Œçº¢è‰²éƒ¨åˆ†æ˜¯å°†adjointå’Œpartial adjountçš„è®¡ç®—å›¾ï¼š
![image.png](https://pics.zhouxin.space/202406071611419.png?x-oss-process=image/quality,q_90/format,webp)




ä½¿ç”¨è®¡ç®—å›¾ï¼Œé™¤äº†èƒ½å¤ŸèŠ‚çœå†…å­˜å¤–ï¼Œè¿˜èƒ½æ¸…æ¥šçš„çœ‹åˆ°æ­£å‘è®¡ç®—çš„ä¸­é—´ç»“æœå’Œåå‘è®¡ç®—ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œè¿›è€Œä¼˜åŒ–è®¡ç®—ã€‚

## åå‘æ¨¡å¼adå’Œåå‘ä¼ æ’­çš„åŒºåˆ«
![image.png](https://pics.zhouxin.space/202406071817738.png?x-oss-process=image/quality,q_90/format,webp)

åå‘ä¼ æ’­ï¼š
- åœ¨åå‘è®¡ç®—è¿‡ç¨‹ä¸­ä½¿ç”¨ä¸å‰å‘ä¼ æ’­å®Œå…¨ç›¸åŒçš„è®¡ç®—å›¾
- åº”ç”¨äºç¬¬ä¸€ä»£æ·±åº¦å­¦ä¹ æ¡†æ¶

åå‘ADï¼š
- ä¸ºadjointåœ¨è®¡ç®—å›¾ä¸­åˆ›å»ºç‹¬ç«‹çš„èŠ‚ç‚¹
- è¢«åº”ç”¨äºç°ä»£æ·±åº¦å­¦ä¹ æ¡†æ¶

ç°ä»£æ™®éåº”ç”¨åå‘ADçš„åŸå› ï¼š
- æŸäº›æŸå¤±å‡½æ•°æ˜¯å…³äºæ¢¯åº¦çš„å‡½æ•°ï¼Œè¿™ç§æƒ…å†µä¸‹éœ€è¦è®¡ç®—æ¢¯åº¦çš„æ¢¯åº¦ï¼Œä½†åå‘ä¼ æ’­å°±ä¸èƒ½è®¡ç®—æ­¤ç±»æƒ…å†µï¼Œè€Œåœ¨åå‘ADä¸­åªè¦å¢åŠ ä¸€ä¸ªèŠ‚ç‚¹ååœ¨æ­¤è®¡ç®—æ¢¯åº¦å³å¯ï¼›
- åå‘ADä¼˜åŒ–ç©ºé—´æ›´å¤§ã€‚

## è€ƒè™‘Tensorçš„åå‘æ¨¡å¼AD
å‰é¢éƒ½æ˜¯åœ¨å‡è®¾ä¸­é—´å˜é‡æ˜¯æ ‡é‡çš„åŸºç¡€ä¸Šè®¨è®ºçš„ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°†å…¶æ¨å¹¿åˆ°Tensorä¸Šã€‚

é¦–å…ˆæ¨å¹¿adjointï¼Œå®šä¹‰å¯¹äºä¸€ä¸ªTensor$Z$ï¼Œå…¶adjoint$\overline{Z}$ä¸ºï¼š
{{< math_block >}}
=\begin{bmatrix}\frac{\partial y}{\partial Z_{1,1}}&...&\frac{\partial y}{\partial Z_{1,n}}\\...&...&...\\\frac{\partial y}{\partial Z_{m,1}}&...&\frac{\partial y}{\partial Z_{m,n}}\end{bmatrix}
{{< /math_block >}}
é‰´äº
{{< math_block >}}
\begin{align*}Z_{ij}&=\sum_kX_{ik}W_{kj}\\v&=f(Z)\end{align*}
{{< /math_block >}}
é‚£ä¹ˆåœ¨è®¡ç®—$\overline{X_{i,k}}$æ—¶ï¼Œéœ€è¦å°†æ‰€æœ‰è®¡ç®—å›¾ä¸Šä»¥$X_{i,k}$ä¸ºè¾“å…¥çš„èŠ‚ç‚¹éƒ½æ‰¾å‡ºæ¥ï¼Œå³$Z$çš„ç¬¬$i$è¡Œçš„æ¯ä¸ªå…ƒç´ ã€‚å› æ­¤$\overline{X_{i,k}}$çš„è®¡ç®—å…¬å¼ä¸ºï¼š
{{< math_block >}}
\overline{X_{i,k}}=\sum_{j}\frac{\partial Z_{i,j}}{\partial X_{i,k}}\bar{Z}_{i,j}=\sum_{j}W_{k,j}\bar{Z}_{i,j}
{{< /math_block >}}
ä¸Šè¿°å…¬å¼è®°ä¸ºçŸ©é˜µå½¢å¼ä¸ºï¼š
{{< math_block >}}
\overline X = \overline Z W^T
{{< /math_block >}}

# Lecture 5: Automatic Differentiation Implementation
è¿™è®²ä¸»è¦ä»‹ç»æˆ‘ä»¬hwä¸­è¦å®ç°çš„needleçš„æ€»ä½“æ¡†æ¶ï¼Œé¡¹ç›®ä¸­å·²ç»™å‡ºäº†çº¦1000è¡Œä»£ç ã€‚

## autograd.py
autogradä¿å­˜ä¸å®ç°è‡ªåŠ¨å¾®åˆ†ç›¸å…³çš„ä»£ç ã€‚

`Value`ç±»å¯¹åº”è®¡ç®—å›¾ä¸Šçš„èŠ‚ç‚¹ï¼Œå…¶æ•°æ®æˆå‘˜åŒ…æ‹¬ï¼š
```python
class Value:
    """A value in the computational graph."""

    # trace of computational graph
    op: Optional[Op]
    inputs: List["Value"]
    # The following fields are cached fields for
    # dynamic computation
    cached_data: NDArray
    requires_grad: bool
```
`op`ç”¨äºä¿å­˜è¯¥èŠ‚ç‚¹çš„è¿ç®—ç¬¦ï¼Œ`inputs`ä¿å­˜è¯¥è¿ç®—ç¬¦çš„æ“ä½œæ•°ï¼Œ`cached_data`ä¿å­˜è¯¥èŠ‚ç‚¹çš„æ•°å€¼ï¼Œå…¶æ•°æ®ç»“æ„å› å¹³å°ä¸åŒè€ŒåŒºåˆ«ã€‚

## ops
æœ¬èŠ‚ä¸»è¦ä»‹ç»needleåº“çš„ä»£ç ç»“æ„ï¼Œç¬”è®°ç›¸å½“è‰ç‡ï¼Œå»ºè®®çœ‹åŸè§†é¢‘ã€‚

opsæ–‡ä»¶å¤¹ï¼ˆ2023ç‰ˆæœ¬ï¼‰æˆ–è€…op.pyï¼ˆ2022ï¼‰ç‰ˆæœ¬ä¿å­˜å„ç§ç®—å­çš„å®ç°ã€‚
`Op`ç±»è§„å®šäº†ä¸¤ä¸ªå¿…é¡»è¦å®ç°çš„æ¥å£ï¼š
```python
class Op:
    """Operator definition."""

    def compute(self, *args: Tuple[NDArray]):
        """Calculate forward pass of operator.

        Parameters
        ----------
        input: np.ndarray
            A list of input arrays to the function

        Returns
        -------
        output: nd.array
            Array output of the operation

        """
        raise NotImplementedError()

    def gradient(
        self, out_grad: "Value", node: "Value"
    ) -> Union["Value", Tuple["Value"]]:
        """Compute partial adjoint for each input value for a given output adjoint.

        Parameters
        ----------
        out_grad: Value
            The adjoint wrt to the output value.

        node: Value
            The value node of forward evaluation.

        Returns
        -------
        input_grads: Value or Tuple[Value]
            A list containing partial gradient adjoints to be propagated to
            each of the input node.
        """
        raise NotImplementedError()
```
`compute`æ¥å£ç”¨äºæè¿°è¯¥è¿ç®—ç¬¦å®æ–½çš„è¿ç®—ï¼Œ`gradient`æè¿°è¯¥è¿ç®—ç¬¦å¯¹åº”çš„æ¢¯åº¦è®¡ç®—æ–¹å¼ã€‚

# Lecture 6: Fully connected network, optimization, initialization
## å…¨è¿æ¥ç½‘ç»œ
ä¹‹å‰æˆ‘ä»¬è®¨è®ºçš„å…¨è¿æ¥ç½‘ç»œéƒ½æ˜¯ä¸å«åæ‰§é¡¹çš„ï¼ˆä¸ºäº†æ–¹ä¾¿è¿›è¡Œæ‰‹åŠ¨å¾®åˆ†ï¼‰ï¼Œæœ¬ç« å°†ä»‹ç»çœŸæ­£çš„MLPã€‚å…¶é€šè¿‡è¿­ä»£çš„è¿‡ç¨‹è¿›è¡Œå®šä¹‰ï¼š
{{< math_block >}}
\begin{align*}  
&z_{i+1} = \sigma_i(W_i^Tz_i+b_i), \ \ \ i=1,...,L\\  
&h_\theta(x) = z_{L+1}\\  
&z_1 = x  
\end{align*}
{{< /math_block >}}
ä¸Šè¿°æ¨¡å‹ä¸­ï¼Œå¯ä¼˜åŒ–çš„å‚æ•°é›†åˆä¸º$\theta = \{W_{1:L}, b_{1:L} \}$ã€‚$\sigma_i(x)$æ˜¯éçº¿æ€§çš„æ¿€æ´»å‡½æ•°ï¼Œç‰¹åˆ«çš„ï¼Œæœ€åä¸€å±‚æ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼Œå³$\sigma_L (x)= x$ã€‚

è¿­ä»£çš„è¡¨è¾¾å¼å†™æˆçŸ©é˜µå½¢å¼ä¸ºï¼š
{{< math_block >}}
\begin{align*}  
Z_{i+1} = \sigma_i(Z_iW_i+1b_i^T)  
\end{align*}
{{< /math_block >}}
å…¶ä¸­ï¼Œ$1$è¡¨ç¤ºä¸€ä¸ªè¡¨ç¤ºä¸€ä¸ªå…¨1çš„åˆ—å‘é‡ï¼Œç”¨äºå°†åˆ—å‘é‡$b_i^T$å¹¿æ’­åˆ°ä¸çŸ©é˜µ$Z_iW_i$ç›¸åŒ¹é…çš„å½¢çŠ¶ã€‚

åœ¨å®é™…å®ç°è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¸ç”¨æµªè´¹ç©ºé—´å»æ„é€ è¿™æ ·ä¸€ä¸ªå…¨1åˆ—å‘é‡ï¼Œè€Œæ˜¯ç›´æ¥ä½¿ç”¨å¹¿æ’­ç®—å­ã€‚åœ¨NumPyæœ‰è®¸å¤šè‡ªåŠ¨çš„å¹¿æ’­æ“ä½œï¼Œä½†æ˜¯åœ¨æˆ‘ä»¬å®ç°çš„needleåº“ä¸­ï¼Œè¿™ä¸€æ“ä½œæ›´åŠ æ˜¾å¼ï¼Œä¾‹å¦‚å¯¹äº$(n\times 1) \to (m \times n)$ï¼Œè¦æ‰§è¡Œçš„æ“ä½œä¸º`A.reshape((1, n)).broadcast_to((m, n))`ã€‚

## ä¼˜åŒ–
å¯¹äºæœ‰ç›‘ç£çš„æ·±åº¦å­¦ä¹ ä»»åŠ¡ï¼Œä¸€èˆ¬çš„ä¼˜åŒ–ç›®æ ‡ä¸ºï¼š
{{< math_block >}}
\mathop{\text{minimize}}_{\theta} \ \ f(\theta) = \frac{1}{m}\sum_{i=1}^m{l(h_\theta(x^{(i)},y^{(i)}))}
{{< /math_block >}}
æ¥ä¸‹æ¥å°†ä»‹ç»å‡ å¸¸ç”¨çš„ä¼˜åŒ–ç®—æ³•ã€‚

- æ¢¯åº¦ä¸‹é™ gradient desecent
æ¢¯åº¦ä¸‹é™æ³•ä¹‹å‰å‡ èŠ‚è¯¾è®²è¿‡äº†ï¼Œè¿™é‡Œç›´æ¥ç»™å‡ºå…¶æ•°å­¦è¡¨è¾¾å¼ï¼š
{{< math_block >}}
\theta_{t+1} = \theta_t - \alpha \nabla_\theta f(\theta_t)
{{< /math_block >}}
å…¶ä¸­ï¼Œ$t$è¡¨ç¤ºè¿­ä»£æ¬¡æ•°ã€‚

å­¦ä¹ ç‡è¿™ä¸€å‚æ•°å¯¹äºè¯¥æ–¹æ³•æ ¼å¤–é‡è¦ï¼Œä¸åŒçš„å­¦ä¹ ç‡çš„è¡¨ç°ç›¸å·®å¾ˆå¤§å¾ˆå¤§ï¼š
![image.png](https://pics.zhouxin.space/202406161006752.png?x-oss-process=image/quality,q_90/format,webp)

ä¸Šå›¾å±•ç¤ºäº†å¤§å­¦ä¹ ç‡å’Œå°å­¦ä¹ ç‡çš„è¿­ä»£è¿‡ç¨‹ï¼Œå¦‚æœç›®æ ‡å‡½æ•°å†å¤æ‚ä¸€ç‚¹ï¼Œé‚£ä¹ˆç¡®å®šåˆé€‚çš„å­¦ä¹ ç‡å°±ä¼šå˜å¾—æ›´åŠ å¤æ‚ã€‚æ¥ä¸‹æ¥å°†ä»‹ç»ä¸€äº›ä¸åŒçš„æ–¹æ³•ï¼Œå®ƒä»¬å„æœ‰å…¶æ”¶æ•›è¡Œä¸ºã€‚

å¯¹äºæ¢¯åº¦ä¸‹é™æ³•çš„æ”¹è¿›ï¼Œæœ‰ä¸¤ç§æ–¹æ¡ˆï¼šæ¢¯åº¦è®¡ç®—çš„å˜ç§å’Œéšæœºçš„å˜ç§ã€‚é¦–å…ˆä»‹ç»ç¬¬ä¸€ç±»ã€‚

- ç‰›é¡¿æ³• Newton's Method
ç‰›é¡¿å‘ä½¿ç”¨äºŒæ¬¡æ›²é¢å¯¹ä¸€ä¸ªé«˜ç»´å‡½æ•°åšè¿‘ä¼¼ï¼Œå› æ­¤å…¶æ”¶æ•›é€Ÿåº¦æ˜¾è‘—å¿«äºä¸€é˜¶é€¼è¿‘çš„æ¢¯åº¦ä¸‹é™æ³•ã€‚å…¶è¿­ä»£å…¬å¼ä¸ºï¼š
{{< math_block >}}
\theta_{t+1} = \theta_t - \alpha(\nabla_\theta^2f(\theta_t))^{-1}\nabla_\theta f(\theta_t)
{{< /math_block >}}
å…¶ä¸­ï¼Œ$(\nabla_\theta^2f(\theta_t))^{-1}$æ˜¯*Hessian*çŸ©é˜µçš„é€†çŸ©é˜µã€‚*Hessian*çŸ©é˜µæ¯ä¸ªå…ƒç´ éƒ½æ˜¯äºŒé˜¶å¯¼æ•°ï¼Œå…¶å…·ä½“å®šä¹‰ä¸ºï¼š
{{< math_block >}}
\nabla_\theta^2f(\theta_t) = H=\begin{bmatrix}\frac{\partial^2f}{\partial x_1^2}&\frac{\partial^2f}{\partial x_1\partial x_2}&\cdots&\frac{\partial^2f}{\partial x_1\partial x_n}\\\frac{\partial^2f}{\partial x_2\partial x_1}&\frac{\partial^2f}{\partial x_2^2}&\cdots&\frac{\partial^2f}{\partial x_2\partial x_n}\\\vdots&\vdots&\ddots&\vdots\\\frac{\partial^2f}{\partial x_n\partial x_1}&\frac{\partial^2f}{\partial x_n\partial x_2}&\cdots&\frac{\partial^2f}{\partial x_n^2}\end{bmatrix}
{{< /math_block >}}
å¯¹äºäºŒæ¬¡å‡½æ•°ï¼Œç‰›é¡¿æ³•å¯ä»¥ä¸€æ¬¡ç»™å‡ºæŒ‡å‘æœ€ä¼˜ç‚¹çš„æ–¹å‘

è¿™ä¸€æ–¹æ³•å¹¿æ³›ç”¨äºä¼ ç»Ÿå‡¸ä¼˜åŒ–é¢†åŸŸï¼Œä½†æ˜¯å¾ˆå°‘ç”¨äºæ·±åº¦å­¦ä¹ ä¼˜åŒ–ã€‚æœ‰ä¸¤ä¸ªä¸»è¦åŸå› ï¼š1) HessiançŸ©é˜µæ˜¯$n\times n$çš„ï¼Œå› æ­¤å‚æ•°é‡ç¨å¾®å¤§ä¸€ç‚¹å…¶è®¡ç®—ä»£ç éƒ½éå¸¸éå¸¸ææ€–ï¼›2) å¯¹äºéå‡¸ä¼˜åŒ–ï¼ŒäºŒé˜¶æ–¹æ³•æ˜¯å¦æ›´æœ‰æ•ˆè¿˜æœ‰å¾…å•†æ¦·ã€‚

- åŠ¨é‡æ¢¯åº¦ä¸‹é™æ³• Momentum
åœ¨æ™®é€šæ¢¯åº¦ä¸‹é™æ³•ä¸­ï¼Œå¦‚æœå­¦ä¹ ç‡å¤ªå¤§ï¼Œå°±ä¼šå‡ºç°æ¥å›æ¨ªè·³çš„æƒ…å†µï¼Œå¦‚æœå¯¹å‰å‡ æ¬¡æ¢¯åº¦å–å¹³å‡ï¼Œåˆ™å¯èƒ½æ”¹å–„è¿™ä¸€æƒ…å†µã€‚

åŠ¨é‡æ³•æ­£æ˜¯å¯¹æ¢¯åº¦å–æŒ‡æ•°ç§»åŠ¨å¹³å‡[^1]çš„æ–¹æ¡ˆï¼Œå…·ä½“æ¥è¯´æœ‰ï¼š
{{< math_block >}}
\begin{align*}  
&u_{t+1} = \beta u_t +(1-\beta)\nabla_\theta f(\theta_t)\\  
&\theta_{t+1} = \theta_t - \alpha u_{t+1}  
\end{align*}
{{< /math_block >}}
è¯¥æ–¹æ³•å¯è§†åŒ–è¿‡ç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œåœ¨è¾ƒå¤§å­¦ä¹ ç‡çš„æƒ…å†µä¸‹ï¼Œå…¶ç›¸æ¯”æ¢¯åº¦ä¸‹é™æ³•ä¼˜åŒ–æ›²çº¿æ›´ä¸ºå¹³æ»‘ã€‚
![image.png](https://pics.zhouxin.space/202406161114979.png?x-oss-process=image/quality,q_90/format,webp)

- æ— ååŠ¨é‡æ³• Unbiasing momentum
å‰ä¸€ç« èŠ‚å®é™…ä¸Šæœ‰ä¸€ä¸ªå°ç‘•ç–µã€‚å¦‚æœ$u_0$åˆå§‹åŒ–ä¸º0ï¼Œé‚£ä¹ˆç¬¬ä¸€æ¬¡è¿›è¡Œæ›´æ–°æ˜¯çš„æ¢¯åº¦å€¼æ˜¯æ­£å¸¸æ›´æ–°çš„$(1-\beta)$å€ï¼Œå› æ­¤å…¶å‰æœŸçš„æ”¶æ•›è¿‡ç¨‹ä¼šç¨æ…¢ï¼Œéšç€è¿­ä»£çš„è¿›è¡Œï¼Œå…¶æ•ˆåº”ä¼šé€æ¸å‡å¼±ã€‚

ä¸ºäº†ä¿®æ­£å…¶å½±å“ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å‚æ•°æ›´æ–°è¿‡ç¨‹ä¸­å¯¹åŠ¨é‡è¿›è¡Œç¼©æ”¾ï¼Œå…·ä½“æ¥è¯´ï¼š
{{< math_block >}}
\theta_{t+1} = \theta_{t} - \frac{\alpha u_{t+1}}{1-\beta^{t+1}}
{{< /math_block >}}
å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œä¿®æ­£ä»¥åå…¶å‰æœŸçš„æ›´æ–°é€Ÿåº¦è¦å¿«äº†ä¸å°‘ã€‚
![image.png](https://pics.zhouxin.space/202406161128045.png?x-oss-process=image/quality,q_90/format,webp)

- Nesterov momentum
Nesterovæ˜¯æ¢¯åº¦ä¸‹é™ä¸­ä¸€ä¸ªéå¸¸æœ‰æ•ˆçš„â€œtrickâ€ï¼Œå…¶åœ¨ä¼ ç»Ÿmomentumçš„åŸºç¡€ä¸Šï¼Œå°†è®¡ç®—å½“å‰ä½ç½®çš„æ¢¯åº¦æ”¹ä¸ºè®¡ç®—ä¸‹ä¸€æ­¥ä½ç½®çš„æ¢¯åº¦ã€‚å³ï¼š
{{< math_block >}}
u_{t+1} = \beta u_t +(1-\beta)\nabla_\theta f(\theta_t - \alpha u_t)
{{< /math_block >}}
å…³äºå…¶ä¸ºå•¥æœ‰æ•ˆï¼Œçœ‹åˆ°äº†ä¸¤ç¯‡æ–‡ç« ã€‚ç¬¬ä¸€ç¯‡[^2]é€šè¿‡æ¨å¯¼è®¤ä¸ºè¯¥æ–¹æ¡ˆå¯¹äºŒé˜¶å¯¼æ•°è¿›è¡Œäº†è¿‘ä¼¼ï¼Œå› æ­¤å…¶æ”¶æ•›é€Ÿåº¦æ›´å¿«ï¼›ç¬¬äºŒç¯‡[^3]è®¤ä¸ºå…¶èƒ½å¤Ÿæ›´å¥½åœ°æ„ŸçŸ¥æœªæ¥ä½ç½®çš„æ¢¯åº¦ï¼Œåœ¨æœªæ¥æ¢¯åº¦å¾ˆå¤§æ—¶æ”¾æ…¢æ­¥å­ã€‚

ä¸çœ‹å¹¿å‘Šçœ‹ç–—æ•ˆï¼Œå¯¹æ¯”æ™®é€šMomentumï¼Œè¯¥æ–¹æ³•çš„æ”¶æ•›é€Ÿåº¦è¦å¿«å¾—å¤šã€‚æ®è¯´å…¶ä¹Ÿæ›´é€‚åˆä¸€ä¸ªæ·±åº¦ç½‘ç»œã€‚
![image.png](https://pics.zhouxin.space/202406161306619.png?x-oss-process=image/quality,q_90/format,webp)

- Adam
Adamæ˜¯ä¸€ç§è‡ªé€‚åº”çš„æ¢¯åº¦ä¸‹é™ç®—æ³•ã€‚ä¸åŒå‚æ•°å…¶å¯¹åº”çš„æ¢¯åº¦ä¹‹é—´çš„å¤§å°å·®å¼‚å¯èƒ½å¾ˆå¤§ï¼ŒAdamå¯¹æ­¤çš„è§£å†³æ–¹æ¡ˆæ˜¯æä¾›ä¸€ä¸ªç¼©æ”¾å› å­ï¼Œæ¢¯åº¦å€¼å°åˆ™å°†å…¶ç¼©æ”¾å¾—å¤§ä¸€ç‚¹ï¼Œå³ï¼š
{{< math_block >}}
\begin{align*}  
&u_{t+1} = \beta_1 u_t + (1-\beta_1)\nabla_\theta f(\theta_t)\\  
&v_{t+1} = \beta_2 v_t + (1-\beta_2)(\nabla_\theta f(\theta_t))^2  &\text{å¹³æ–¹ä¸ºé€å…ƒç´ è¿ç®—}\\  
&\theta_{t+1} = \theta_t - \frac{\alpha u_{t+1}}{\sqrt{v_{t+1}}+\epsilon} & \text{æ‰€æœ‰å…ƒç´ å‡ä¸ºé€å…ƒç´ è¿ç®—}\\  
\end{align*}
{{< /math_block >}}
Adamåœ¨å®è·µä¸­å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œåœ¨ç‰¹å®šä»»åŠ¡ä¸Šï¼Œå…¶å¯èƒ½ä¸æ˜¯æœ€ä½³çš„ä¼˜åŒ–å™¨ï¼ˆå¦‚ä¸‹å›¾ï¼‰ï¼Œä½†åœ¨å¤§éƒ¨åˆ†ä»»åŠ¡ä¸Šï¼Œå…¶éƒ½èƒ½æœ‰ä¸é”™çš„å¯ä»¥ä½œä¸ºåŸºçº¿çš„è¡¨ç°ã€‚
![image.png](https://pics.zhouxin.space/202406161602224.png?x-oss-process=image/quality,q_90/format,webp)

æ¥ä¸‹æ¥å°†ä»‹ç»éšæœºå˜ç§ã€‚éšæœºå˜ç§æ˜¯åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­åŠ å…¥äº†éšæœºå˜é‡ï¼ˆå™ªå£°ï¼‰ï¼Œä¾‹å¦‚æ¯æ¬¡ä½¿ç”¨æ•°æ®é›†çš„ä¸€ä¸ªå­é›†å¯¹å‚æ•°è¿›è¡Œæ›´æ–°ã€‚
- éšæœºæ¢¯åº¦ä¸‹é™ Stochastic gradient descent
éšæœºæ¢¯åº¦ä¸‹é™æ­£æ˜¯æ¯æ¬¡ä½¿ç”¨æ•°æ®é›†çš„ä¸€ä¸ªå­é›†å¯¹å‚æ•°è¿›è¡Œæ›´æ–°ï¼Œå³ï¼š
{{< math_block >}}
\theta_{t+1} = \theta_t - \frac{\alpha}{|B|}\sum_{i\in B}\nabla_\theta l(h_\theta(x^{(i)},y^{i}))
{{< /math_block >}}

çœ‹ä¸Šå»SGDçš„è¿­ä»£æ¬¡æ•°æ¯”æ¢¯åº¦ä¸‹é™è¦å¤šå¾—å¤šï¼Œä½†æ˜¯å…¶æ¯è½®è¿­ä»£çš„è®¡ç®—ä»£ä»·éƒ½è¦å°çš„å¤šï¼ŒåŒæ—¶
![image.png](https://pics.zhouxin.space/202406161624584.png?x-oss-process=image/quality,q_90/format,webp)

å°½ç®¡åœ¨å‡¸ä¼˜åŒ–ä¸Šå¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹ç»™äº†å¾ˆç›´è§‚çš„æ„Ÿå—ï¼Œä½†éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ·±åº¦å­¦ä¹ å¹¶ä¸æ˜¯å‡¸ä¼˜åŒ–æˆ–è€…äºŒæ¬¡å‡½æ•°ï¼Œè¿™äº›ä¼˜åŒ–æ–¹æ³•åœ¨æ·±åº¦å­¦ä¹ ä¸Šçš„åº”ç”¨ä¸åœ¨å‡¸ä¼˜åŒ–ä¸Šçš„æ•ˆæœå¯èƒ½å®Œå…¨ä¸åŒã€‚

## åˆå§‹åŒ–
å‚æ•°çš„åˆå§‹å€¼å¦‚ä½•ç¡®å®šï¼Ÿè¿™æ˜¯ä¸ªå¥½é—®é¢˜ã€‚

åœ¨å‡¸ä¼˜åŒ–ä¸­ï¼Œå°å°å°†æ‰€æœ‰å‚æ•°åˆå§‹åŒ–ä¸º0ï¼Œå¦‚æœåœ¨ç¥ç»ç½‘ç»œä¸­ä¹Ÿè¿™ä¹ˆåšï¼Œé‚£ä¹ˆæ¯ä¸€å±‚çš„è¾“å‡ºéƒ½æ˜¯0ï¼Œæ±‚å¾—çš„æ¢¯åº¦ä¹Ÿéƒ½æ˜¯0ğŸ™ã€‚å…¨0æ˜¯è¿™ä¸ªæ¨¡å‹çš„ä¸€ä¸ªä¸åŠ¨ç‚¹ï¼Œæ¨¡å‹å°†æ°¸è¿œå¾—ä¸åˆ°æ›´æ–°ã€‚

- åˆå§‹åŒ–å‚æ•°å¯¹æ¢¯åº¦çš„å½±å“å¾ˆå¤§
ä¸€ç§è‡ªç„¶çš„æƒ³æ³•æ˜¯å¯¹å‚æ•°è¿›è¡Œéšæœºåˆå§‹åŒ–ï¼Œä¾‹å¦‚æŒ‰ç…§å¤šå…ƒæ­£æ€åˆ†å¸ƒè¿›è¡Œåˆå§‹åŒ–ã€‚ä½†æ˜¯ï¼Œåˆ†å¸ƒä¸­å‚æ•°çš„é€‰æ‹©å¯¹äºæ¢¯åº¦çš„å½±å“å¯èƒ½ä¼šç›¸å½“å¤§ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
![image.png](https://pics.zhouxin.space/202406161659652.png?x-oss-process=image/quality,q_90/format,webp)
éšç€å±‚æ•°çš„å¢åŠ ï¼Œå¦‚æœæ¿€æ´»å€¼èŒƒæ•°å˜åŒ–çš„å¤ªå‰§çƒˆï¼Œä¼šå¯¼è‡´æ¢¯åº¦çˆ†ç‚¸æˆ–è€…æ¶ˆå¤±é—®é¢˜ï¼Œå¦‚æœæ¢¯åº¦å€¼è¿‡å¤§æˆ–è€…è¿‡å°ï¼Œä¹Ÿä¼šå¯¼è‡´è¿™äº›é—®é¢˜ã€‚

- æƒé‡çš„åœ¨è®­ç»ƒè¿‡ç¨‹çš„å˜åŒ–å¯èƒ½å¾ˆå°
å¯èƒ½å­˜åœ¨è¿™æ ·ä¸€ä¸ªè¯¯åŒºï¼šæ— è®ºåˆå§‹å€¼å¦‚ä½•é€‰æ‹©ï¼Œè¿™äº›å‚æ•°æœ€ç»ˆéƒ½ä¼šæ”¶æ•›åˆ°æŸä¸ªåŒºåŸŸé™„è¿‘ã€‚äº‹å®å¹¶éå¦‚æ­¤ï¼Œæ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­æƒé‡çš„å˜åŒ–å¹¶éå¦‚æ­¤å‰§çƒˆã€‚

- ä¸ºä»€ä¹ˆ2/nåœ¨å‰é¢æ˜¯ä¸ªåˆé€‚çš„åˆå§‹åŒ–å‚æ•°
è¿™é‡Œç›´æ¥ä½¿ç”¨gptå¯¹è¿™é¡µpptçš„è§£é‡Š
> è€ƒè™‘ç‹¬ç«‹çš„éšæœºå˜é‡ ğ‘¥âˆ¼ğ‘(0,1)xâˆ¼N(0,1) å’Œ ğ‘¤âˆ¼ğ‘(0,1ğ‘›)wâˆ¼N(0,n1â€‹)ï¼Œå…¶ä¸­ ğ‘¥x æ˜¯è¾“å…¥ï¼Œğ‘¤w æ˜¯æƒé‡ã€‚
> 
> #### æœŸæœ›å’Œæ–¹å·®
> 
> - ğ¸[ğ‘¥â‹…ğ‘¤ğ‘–]=0E[xâ‹…wiâ€‹]=0
> - Var[ğ‘¥â‹…ğ‘¤ğ‘–]=1ğ‘›Var[xâ‹…wiâ€‹]=n1â€‹
> 
> å› æ­¤ï¼Œå¯¹äº ğ‘¤ğ‘‡ğ‘¥wTxï¼š
> 
> - ğ¸[ğ‘¤ğ‘‡ğ‘¥]=0E[wTx]=0
> - Var[ğ‘¤ğ‘‡ğ‘¥]=1Var[wTx]=1ï¼ˆæ ¹æ®ä¸­å¿ƒæé™å®šç†ï¼Œğ‘¤ğ‘‡ğ‘¥wTxÂ æœä»Â ğ‘(0,1)N(0,1)ï¼‰
> 
> ### æ¿€æ´»å€¼çš„æ–¹å·®
> 
> å¦‚æœä½¿ç”¨çº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œå¹¶ä¸” ğ‘§ğ‘–âˆ¼ğ‘(0,ğ¼)ziâ€‹âˆ¼N(0,I)ï¼Œåˆ™ ğ‘Šğ‘–âˆ¼ğ‘(0,1ğ‘›ğ¼)Wiâ€‹âˆ¼N(0,n1â€‹I)ï¼Œé‚£ä¹ˆï¼š
> 
> ğ‘§ğ‘–+1=ğ‘Šğ‘–ğ‘§ğ‘–zi+1â€‹=Wiâ€‹ziâ€‹
> 
> ### ReLU éçº¿æ€§
> 
> å¦‚æœä½¿ç”¨ ReLU éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œç”±äº ReLU ä¼šå°†ä¸€åŠçš„ ğ‘§ğ‘–ziâ€‹ åˆ†é‡è®¾ä¸ºé›¶ï¼Œå› æ­¤ä¸ºäº†è¾¾åˆ°ç›¸åŒçš„æœ€ç»ˆæ–¹å·®ï¼Œéœ€è¦å°† ğ‘Šğ‘–Wiâ€‹ çš„æ–¹å·®å¢åŠ ä¸€å€ã€‚å› æ­¤ï¼š
> 
> ğ‘Šğ‘–âˆ¼ğ‘(0,2ğ‘›ğ¼)Wiâ€‹âˆ¼N(0,n2â€‹I)
> 
> è¿™å°±æ˜¯æ‰€è°“çš„ Kaiming æ­£æ€åˆå§‹åŒ–ï¼ˆHe åˆå§‹åŒ–ï¼‰ï¼Œå®ƒç‰¹åˆ«é€‚ç”¨äº ReLU æ¿€æ´»å‡½æ•°ã€‚

# Lecture 7: Neural Network Library Abstractions
è¿™èŠ‚è¯¾ä¸»è¦ä»‹ç»å¦‚ä½•ä½¿ç”¨æˆ‘ä»¬çš„needleåº“æ¥å®ç°ä¸€äº›ç®€å•çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œæ„é€ ä¸€äº›å°ç»„ä»¶ã€‚
## ç¨‹åºæŠ½è±¡
ç°ä»£æˆç†Ÿçš„æ·±åº¦å­¦ä¹ åº“æä¾›äº†ä¸€äº›APIï¼Œç«™åœ¨ä»Šå¤©çš„è§†è§’ï¼Œè¿™äº›APIéƒ½æ˜¯éƒ½æ˜¯æ°åˆ°å¥½å¤„çš„ã€‚é€šè¿‡æ€è€ƒä¸ºä»€ä¹ˆè¦è¿™æ ·è®¾è®¡æ¥å£ï¼Œå¯ä»¥è®©æˆ‘ä»¬æ›´å¥½åœ°ç†è§£æ·±åº¦å­¦ä¹ åº“åœ¨è¿›è¡Œç¨‹åºæŠ½è±¡æ—¶çš„å†…éƒ¨é€»è¾‘ã€‚

é¦–å…ˆå‡ ä¸ªç»å…¸çš„æ·±åº¦å­¦ä¹ æ¡†æ¶è¿›è¡Œåˆ†æï¼ŒåŒ…æ‹¬Caffeã€TensorFlowå’ŒPyTorchã€‚
- Caffe 1.0 ï¼ˆ2014ï¼‰
åœ¨Caffeä¸­ï¼Œä½¿ç”¨Layerè¿™ä¸€æ¦‚å¿µæ¥è¡¨ç¤ºç¥ç»ç½‘ç»œä¸­çš„ä¸€ä¸ªä¸ªå°æ¨¡å—ï¼Œé€šè¿‡æ‹¼æ¥å’Œæ›¿æ¢Layerï¼Œå¯ä»¥å®ç°å¿«é€Ÿæ„é€ å’Œä¿®æ”¹ç¥ç»ç½‘ç»œï¼Œå¹¶ä½¿ç”¨åŒä¸€å¥—ä»£ç è¿›è¡Œè®­ç»ƒã€‚

Layerç±»æä¾›äº†`forward`å’Œ`backward`ä¸¤ä¸ªæ¥å£ï¼š
```python
class Layer:
	def forward(bottom, top):
		pass

	def backward(top, propagate_down, bottom):
		pass
```

`forward`è´Ÿè´£å°†æ¥è‡ªbottomçš„æ•°æ®è¿›è¡Œå‰å‘ä¼ æ’­ï¼Œç„¶åå°†æ•°æ®ä¿å­˜åˆ°topä¸­ã€‚åœ¨`backward`æ¥å£ä¸­ï¼Œtopä¿å­˜æ¥è‡ªè¾“å‡ºçš„æ¢¯åº¦ï¼Œ`propagate_down`ç”¨ä»¥æŒ‡ç¤ºæ˜¯å¦è¦å¯¹å…¶æ±‚æ¢¯åº¦ï¼Œbottomç”¨äºå­˜æ”¾æ¢¯åº¦ã€‚

åœ¨Caffeä¸­ï¼Œè®¡ç®—æ¢¯åº¦æ˜¯â€œå°±åœ°â€å®Œæˆçš„ï¼Œè€Œéåœ¨è®¡ç®—å›¾ä¸Šæ–°å¢é¢å¤–çš„èŠ‚ç‚¹ã€‚ä½œä¸ºç¬¬ä¸€ä»£æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œç›´æ¥è®¡ç®—æ¢¯åº¦çš„æ€æƒ³æ˜¯æœ´ç´ ä½†æ˜¯ç¬¦åˆç›´è§‰çš„ã€‚

- TensorFlow 1.0 ï¼ˆ2015ï¼‰
ä½œä¸ºç¬¬äºŒä»£æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œå…¶åœ¨å¼•å…¥äº†è®¡ç®—å›¾çš„æ¦‚å¿µã€‚åœ¨è®¡ç®—å›¾ä¸­ï¼Œåªè¦å®šä¹‰å‰å‘è®¡ç®—çš„è®¡ç®—æ–¹å¼ï¼Œå½“éœ€è¦è®¡ç®—æ¢¯åº¦æ—¶ï¼Œç›´æ¥å¯¹è®¡ç®—å›¾è¿›è¡Œæ‹“å±•å³å¯ã€‚ä¸€ä¸ªç®€çŸ­å®ä¾‹ä¸ºï¼š
```python
import tensorflow as tf

v1 = tf.Variable()
v2 = tf.exp(v1)
v3 = v2 + 1
v4 = v2 * v3

sess = tf.Session()
value4 = sess.run(v4, feed_dict = {v1: numpy.array([1])})
```

ä»¥ä¸Šä»£ç `v1~4`ä»…ä»…æ˜¯å ä½ç¬¦ï¼Œç”¨äºæ„å»ºè®¡ç®—å›¾ï¼Œåœ¨æ²¡æœ‰è¾“å…¥ä¼ å…¥å‰å¹¶æ²¡æœ‰å€¼ã€‚é€šè¿‡ä¼šè¯æ¥è·å–æŸä¸ªè¾“å…¥çš„æƒ…å†µä¸‹è¾“å‡ºçš„å€¼ã€‚

ä¸Šè¿°è¿‡ç¨‹è¢«ç§°ä¸ºå£°æ˜å¼ç¼–ç¨‹ã€‚å³è®¡ç®—å›¾åœ¨å®šä¹‰æ—¶å¹¶ä¸ä¼šç«‹å³æ‰§è¡Œï¼Œè€Œæ˜¯ç­‰åˆ°ä¼šè¯ï¼ˆsessionï¼‰è¿è¡Œæ—¶æ‰æ‰§è¡Œã€‚è¿™ç§æ–¹å¼çš„ä¼˜ç‚¹æœ‰ï¼šä»£ç åˆ†åŒºï¼Œå¯è¯»æ€§é«˜ï¼›è¿è¡Œå‰è®¡ç®—å›¾å·²çŸ¥ï¼Œå¯ä»¥é’ˆå¯¹æ€§ä¼˜åŒ–ï¼›é€šè¿‡ä¼šè¯ä¾¿äºå®ç°åˆ†å¸ƒå¼è®¡ç®—

- PyTorch (needle)
PyTorchä½¿ç”¨çš„æ˜¯å‘½ä»¤å¼ç¼–ç¨‹ï¼Œç›¸æ¯”å£°æ˜å¼ç¼–ç¨‹ï¼Œå‘½ä»¤å¼ç¼–ç¨‹åœ¨æ„å»ºè®¡ç®—å›¾æ—¶å°±å·²ç»æŒ‡å®šå…¶å€¼ã€‚
```python
import needle as ndl

v1 = ndl.Tensor([1])
v2 = ndl.exp(v1)
v3 = v2 + 1
v4 = v2 * v3

```
å‘½ä»¤å¼ç¼–ç¨‹å¯ä»¥å¾ˆæ–¹ä¾¿åœ°ä¸PythonåŸç”Ÿæ§åˆ¶æµè¯­å¥ç»“åˆåœ¨ä¸€èµ·ï¼Œä¾‹å¦‚ï¼š
```python
if v4.numpy() > 0.5:
	v5 = v4 * 2
else:
	v5 = v4
```

tf1.0çš„æ•ˆç‡æ›´é«˜ï¼Œé€‚åˆæ¨ç†å’Œéƒ¨ç½²ã€‚PyTorch1.0åˆ™æ›´é€‚åˆå¼€å‘å’Œdebugã€‚

## é«˜çº§æ¨¡å—åŒ–åº“ç»„ä»¶
å¦‚ä½•ä½¿ç”¨æ·±åº¦å­¦ä¹ åº“æ¥å®ç°æ·±åº¦å­¦ä¹ å‘¢ï¼Ÿåœ¨hw1ä¸­æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªä¸ªåº•å±‚ç®—å­æ¥æ­å»ºæ¨¡å‹å’Œå®ç°è®­ç»ƒè¿‡ç¨‹ï¼Œä½†è¿™æ ·å¼€å‘å¤ªä½æ•ˆäº†ã€‚æ·±åº¦å­¦ä¹ æœ¬èº«æ˜¯å¾ˆæ¨¡å—åŒ–çš„ï¼šç”±æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–æ–¹æ³•ä¸‰éƒ¨åˆ†ç»„æˆã€‚ä¸ä½†å¦‚æ­¤ï¼Œæ¨¡å‹æœ¬èº«ä¹Ÿæ˜¯é«˜åº¦æ¨¡å—åŒ–çš„ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨å®ç°æ·±åº¦å­¦ä¹ åº“æ—¶ï¼Œå¿…é¡»ç²¾å¿ƒè®¾è®¡å¥½æ¥å£ï¼Œä»¥ä¾¿æ”¯æŒè¯¥æ¨¡å—åŒ–çš„ç‰¹æ€§ã€‚

åœ¨PyTorchä¸­ï¼Œæœ‰ä¸€ç±»å«åš`nn.Module`ï¼Œå¯¹åº”çš„å°±æ˜¯æ¨¡å‹ä¸­ä¸€ä¸ªä¸ªå°çš„å­æ¨¡å—ï¼Œå…¶ç‰¹ç‚¹æ˜¯ä»¥TensoråŒæ—¶ä½œä¸ºè¾“å…¥å’Œè¾“å‡ºã€‚æŸå¤±å‡½æ•°ä¹Ÿæ»¡è¶³è¿™ä¸€ç‰¹æ€§ï¼Œå…¶å¯ä»¥è¢«è§†ä¸ºä¸€ä¸ªæ¨¡å—ã€‚

å¯¹äºä¼˜åŒ–å™¨ï¼Œå…¶ä½œç”¨æ˜¯è¾“å…¥ä¸€ä¸ªæ¨¡å‹ï¼Œå¯¹è¯¥æ¨¡å‹ä¸­çš„å‚æ•°æŒ‰ç…§æŸä¸€è§„åˆ™è¿›è¡Œæ›´æ–°ã€‚

ä¸ºäº†é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæœ‰äº›æ¨¡å‹è¿˜å…·æœ‰æ­£åˆ™é¡¹ï¼Œå…¶æœ‰ä¸¤ç§å®ç°æ–¹å¼ï¼š
- ä½œä¸ºæŸå¤±å‡½æ•°çš„ä¸€éƒ¨åˆ†è¿›è¡Œå®ç°
- ç›´æ¥æ•´åˆè¿›ä¼˜åŒ–å™¨ä¸­

å‚æ•°åˆå§‹åŒ–åŒæ ·å¾ˆé‡è¦ï¼Œå…¶ä¸€èˆ¬åœ¨æ„å»º`nn.Module`ä¸­æŒ‡å®šã€‚

æ•°æ®åŠ è½½ä¹Ÿæ˜¯ä¸€ä¸ªå¾ˆé‡è¦çš„æ¨¡å—ã€‚æ•°æ®åŠ è½½ä¸­è¿˜ç»å¸¸å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†å’Œå¢å¼ºã€‚

å„ç»„ä»¶ä¹‹é—´æ•°æ®æµå›¾å¦‚ä¸‹æ‰€ç¤ºï¼š
![image.png](https://pics.zhouxin.space/202406200916559.png?x-oss-process=image/quality,q_90/format,webp)

# Lecture 8: Neural Network Implementation
## ä¿®æ”¹Tensorçš„dataåŸŸ
åœ¨å®ç°SGDæ—¶ï¼Œç”±äºå­˜åœ¨å¤šä¸ªbatchï¼Œå¯èƒ½ä¼šåœ¨ä¸€ä¸ªå¾ªç¯é‡Œå¯¹å¾…å­¦ä¹ å‚æ•°è¿›è¡Œæ›´æ–°ï¼Œå³ï¼š
```python
for _ in range(iterations):
	w -= lr * grad
```

æ­£å¦‚åœ¨[CMU 10-414 Assignments å®éªŒç¬”è®° > SGD for a two-layer neural network]({{< relref "CMU%2010-414%20Assignments%20%E5%AE%9E%E9%AA%8C%E7%AC%94%E8%AE%B0.md" >}}#sgd-for-a-two-layer-neural-network)è¸©è¿‡çš„å‘é‚£æ ·ï¼Œç›´æ¥ä½¿ç”¨Tensorä¹‹é—´çš„ç®—å­è¿›è¡Œå‚æ•°æ›´æ–°ä¼šå¯¼è‡´æ¯æ¬¡æ›´æ–°éƒ½ä¼šåœ¨è®¡ç®—å›¾ä¸Šå¢åŠ ä¸€ä¸ªæ–°çš„èŠ‚ç‚¹wï¼Œè¿™ä¸ªèŠ‚ç‚¹å…·æœ‰Opå’Œinputsï¼Œä¸¥é‡æ‹–ç´¯åå‘ä¼ æ’­é€Ÿåº¦ã€‚

ä¸ºäº†é¿å…æ¯æ¬¡æ›´æ–°å‚æ•°æ—¶éƒ½åœ¨è®¡ç®—å›¾ä¸Šç•™ä¸‹ä¸€ä¸ªéœ€è¦æ±‚æ¢¯åº¦çš„èŠ‚ç‚¹ï¼Œneedleåº“æä¾›äº†`Tensor.data()`æ–¹æ³•ï¼Œç”¨äºåˆ›å»ºä¸€ä¸ªä¸`Tensor`å…±äº«åŒä¸€ä¸ªåº•å±‚dataçš„èŠ‚ç‚¹ï¼Œä½†å…¶ä¸å­˜åœ¨Opå’Œinputsï¼Œä¹Ÿä¸ç”¨å¯¹å…¶è¿›è¡Œæ±‚å¯¼ã€‚

å› æ­¤ï¼Œå¯ä»¥ä½¿ç”¨`Tensor.data`æ–¹æ³•ï¼Œåœ¨ä¸å¹²æ‰°è®¡ç®—å›¾åå‘ä¼ æ’­çš„å‰æä¸‹å¯¹å‚æ•°è¿›è¡Œæ­£å¸¸çš„æ›´æ–°ï¼Œå³ï¼š
```python
w.data -= lr * grad.data
```

## æ•°å€¼ç¨³å®šæ€§
æ¯ä¸ªæ•°å€¼åœ¨å†…å­˜ä¸­çš„å­˜å‚¨ç©ºé—´éƒ½æ˜¯æœ‰é™çš„ï¼Œå› æ­¤ä¿å­˜çš„æ•°å€¼çš„èŒƒå›´å’Œç²¾åº¦éƒ½æ˜¯æœ‰é™çš„ï¼Œè®¡ç®—è¿‡ç¨‹ä¸­éš¾å…å‡ºç°æº¢å‡ºæˆ–è€…ç²¾åº¦ä¸¢å¤±çš„æƒ…å†µï¼Œåœ¨å®ç°ç®—å­æ—¶ï¼Œå¿…é¡»è€ƒè™‘åˆ°æ•°å€¼ç¨³å®šæ€§çš„é—®é¢˜ã€‚

ä¾‹å¦‚ï¼Œåœ¨softmaxå…¬å¼ä¸­ï¼Œç”±äºæŒ‡æ•°è¿ç®—çš„å­˜åœ¨ï¼Œæ•°å€¼å¾ˆæœ‰å¯èƒ½å°±ä¸Šæº¢äº†ï¼Œä¸€ä¸ªä¿®æ­£æ–¹å¼æ˜¯åœ¨è¿›è¡Œsoftmaxè¿ç®—å‰ï¼Œæ¯ä¸ªå…ƒç´ éƒ½å‡å»è¾“å…¥çš„æœ€å¤§å€¼ï¼Œä»¥é˜²æ­¢ä¸Šæº¢ã€‚å³ï¼š
{{< math_block >}}
z_i = \text{softmax}(x_i) = \frac{\exp(x_i -c)}{\sum_k {\exp(x_k-c)}}
{{< /math_block >}}
å…¶ä¸­ï¼Œ$c = \max(x)$ã€‚

ç±»ä¼¼çš„ï¼Œå…¶å®ƒç®—å­ä¹Ÿè¦è€ƒè™‘ç›¸åº”çš„ç¨³å®šæ€§é—®é¢˜ã€‚

## Parameter ç±»
`Parameter`ç±»ç”¨äºè¡¨ç¤ºå¯å­¦ä¹ çš„å‚æ•°ï¼Œå…¶æ˜¯`Tensor`çš„å­ç±»ã€‚ç›¸æ¯”`Tensor`ç±»ï¼Œè¿™ä¸ªç±»ä¸å¿…å†å¼•å…¥æ–°çš„è¡Œä¸ºæˆ–è€…æ¥å£ï¼Œå› æ­¤å…¶å®ç°å¾ˆç®€å•ï¼š
```python
class Parameter(ndl.Tensor):
Â  Â  """parameter"""
```

## Module ç±»
`Module`ç±»ç”¨äºè¡¨ç¤ºç¥ç»ç½‘ç»œä¸­ä¸€ä¸ªä¸ªå­æ¨¡å—ã€‚å…¶å…·æœ‰å¦‚ä¸‹æ¥å£ï¼š
- `parameters`ï¼šè·å–æ¨¡å—ä¸­æ‰€æœ‰å¯å­¦ä¹ å‚æ•°
- `__call__`ï¼šè¿›è¡Œå‰å‘ä¼ æ’­
åœ¨å®ç°æ—¶ï¼Œå®šä¹‰äº†ä¸€ä¸ªè¾…åŠ©å‡½æ•°`_get_params`ç”¨äºæå–ä¸€ä¸ªæ¨¡å—ä¸­çš„æ‰€æœ‰å¯å­¦ä¹ å‚æ•°ã€‚
```python
def _get_params(value):
    if isinstance(value, Parameter):
        return [value]
    if isinstance(value, dict):
        params = []
        for k, v in value.items():
            params += _get_params(v)
        return params
    if isinstance(value, Module):
        return value.parameters()
    return []

class Module:
    def parameters(self):
        return _get_params(self.__dict__)

    def __call__(self, *args, **kwargs):
        return self.forward(*args, **kwargs)
```

### Optimizer ç±»
`Optimizer`ç±»ç”¨äºä¼˜åŒ–æ¨¡å‹ä¸­å¯å­¦ä¹ å‚æ•°ï¼Œå…¶æœ‰ä¸¤ä¸ªå…³é”®æ¥å£ï¼š
- `reset_grad`ï¼šé‡ç½®æ¨¡å‹ä¸­å¯å­¦ä¹ å‚æ•°çš„gradå­—æ®µ
- `step`ï¼šæ›´æ–°å‚æ•°å€¼
`reset_grad`å®ç°æ¯”è¾ƒç®€å•ï¼Œ`step`æ–¹æ³•åˆ™ä¾èµ–äºä¼˜åŒ–ç®—æ³•çš„å…·ä½“å®ç°ï¼š
```python
class Optimizer:
    def __init__(self, params):
        self.params = params

    def reset_grad(self):
        for p in self.params:
            p.grad = None

    def step(self):
        raise NotImplemented()
```

# Lecture 9: Normalization and Regularization
## Normalization
åœ¨å‰é¢å‡ è®²æåˆ°è¿‡ï¼Œå‚æ•°åˆå§‹å€¼çš„é€‰æ‹©å¯¹äºæ¨¡å‹çš„è®­ç»ƒå¾ˆé‡è¦ï¼Œä¸æ°å½“çš„åˆå§‹å€¼å‚æ•°ä¼šå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±æˆ–è€…çˆ†ç‚¸ğŸ’¥ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œå½“è®­ç»ƒå®Œæˆåï¼Œè¿™äº›æ¢¯åº¦å’Œå‚æ•°å€¼å¤§å°ä»æœ‰åˆå§‹å€¼å·®ä¸å¤šï¼Œè¿™æ›´å¼ºè°ƒäº†åˆå§‹å€¼çš„é‡è¦æ€§ã€‚
![image.png](https://pics.zhouxin.space/202407051309612.png?x-oss-process=image/quality,q_90/format,webp)

ä¸ºäº†ä¿®å¤è¿™ä¸€é—®é¢˜ï¼Œå¼•å…¥äº†layer normalizationã€‚å…¶æ€æƒ³å°±æ˜¯å¯¹æ¿€æ´»å±‚çš„è¾“å‡ºè¿›è¡Œæ ‡å‡†åŒ–ï¼Œå³å°†è¾“å‡ºå‡å»æœŸæœ›åé™¤ä»¥æ ‡å‡†å·®ï¼š
{{< math_block >}}
\begin{align*}  
\hat{z}_{i+1} &= \sigma_i (W_i^Tz_i+b_i)\\  
z_{i+1} &=\frac{\hat{z}_{i+1} - E(\hat{z}_{i+1})}{Var(\hat{z}_{i+1})+\epsilon}  
\end{align*}
{{< /math_block >}}
ä¸Šè¿°æŠ€å·§ç›®å‰å·²ç»å¾—åˆ°å¹¿æ³›åº”ç”¨ï¼Œä½†åœ¨å®è·µä¸­ï¼Œåº”ç”¨layer normä¼šå¯¼è‡´æ¨¡å‹éš¾ä»¥æ”¶æ•›åˆ°ä¸€ä¸ªå¾ˆå°çš„losså€¼ã€‚

å¦å¤–ä¸€ç§æŠ€å·§æ˜¯batch normã€‚layer normæ˜¯å¯¹æ¯ä¸€ä¸ªsampleï¼ˆzçš„æ¯ä¸€è¡Œï¼‰åšå½’ä¸€åŒ–ï¼Œè€Œbatch normå¯¹æ¯ä¸€åˆ—å½’ä¸€åŒ–ã€‚è¿™ä¸€æ–¹æ³•ä½¿å¾—æ¯ä¸ªbatchçš„æ‰€æœ‰æ ·æœ¬éƒ½ä¼šå¯¹è¯¥batchä¸­æŸä¸ªæ ·æœ¬çš„æ¨ç†ç»“æœæœ‰å½±å“ï¼Œå› æ­¤åœ¨è¿›è¡Œæ¨ç†æ—¶ï¼Œbatch normä¸­çš„å½’ä¸€åŒ–çš„å‚æ•°åº”è¯¥ä½¿ç”¨æ•´ä¸ªè®­ç»ƒé›†ä¸Šçš„å‚æ•°ï¼Œè€Œéæ¨ç†æ—¶è¾“å…¥æ ·æœ¬çš„batchå‚æ•°ã€‚

## Regularization
æ­£åˆ™åŒ–ç”¨äºå¯¹æŠ—è¿‡æ‹Ÿåˆï¼Œæ‰€è°“è¿‡æ‹Ÿåˆæ˜¯æŒ‡æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šæ€§èƒ½éå¸¸å¥½ï¼Œä½†åœ¨æµ‹è¯•æœºä¸Šæ³›åŒ–æ€§èƒ½å¾ˆå·®ã€‚æ­£åˆ™åŒ–å°±æ˜¯é™åˆ¶å‚æ•°å¤æ‚åº¦çš„è¿‡ç¨‹ï¼Œå¯ä»¥åˆ†ä¸ºæ˜¾å¼æ­£åˆ™å’Œéšå¼æ­£åˆ™ã€‚

éšå¼æ­£åˆ™åŒ–æ˜¯æŒ‡ç°æœ‰ç®—æ³•æˆ–æ¶æ„åœ¨ä¸æ˜¾å¼æ·»åŠ æ­£åˆ™åŒ–é¡¹çš„æƒ…å†µä¸‹ï¼Œè‡ªç„¶åœ°å¯¹å‡½æ•°ç±»è¿›è¡Œé™åˆ¶ã€‚å…·ä½“æ¥è¯´ï¼Œéšå¼æ­£åˆ™åŒ–é€šè¿‡ä»¥ä¸‹æ–¹å¼å®ç°ï¼š
- **ç®—æ³•çš„å›ºæœ‰ç‰¹æ€§**ï¼šä¾‹å¦‚ï¼Œéšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ç­‰ä¼˜åŒ–ç®—æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªå¸¦æŸäº›æ­£åˆ™åŒ–æ•ˆæœã€‚è™½ç„¶æˆ‘ä»¬å¹¶æ²¡æœ‰æ˜¾å¼åœ°ä¼˜åŒ–æ‰€æœ‰å¯èƒ½çš„ç¥ç»ç½‘ç»œï¼Œè€Œæ˜¯é€šè¿‡SGDä¼˜åŒ–é‚£äº›åœ¨ç‰¹å®šæƒé‡åˆå§‹åŒ–ä¸‹çš„ç¥ç»ç½‘ç»œã€‚è¿™ç§ä¼˜åŒ–è¿‡ç¨‹æœ¬èº«å¯¹æ¨¡å‹çš„å¤æ‚åº¦è¿›è¡Œäº†é™åˆ¶ã€‚
- **æ¶æ„çš„è®¾è®¡**ï¼šæŸäº›ç½‘ç»œæ¶æ„è®¾è®¡æœ¬èº«å°±å…·æœ‰æ­£åˆ™åŒ–æ•ˆæœã€‚ä¾‹å¦‚ï¼Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„å…±äº«æƒé‡æœºåˆ¶å’Œå±€éƒ¨è¿æ¥ç‰¹æ€§ï¼Œè‡ªç„¶åœ°å‡å°‘äº†æ¨¡å‹å‚æ•°çš„æ•°é‡ï¼Œä»è€Œé™ä½äº†æ¨¡å‹å¤æ‚åº¦ã€‚

æ˜¾å¼æ­£åˆ™åŒ–æŒ‡çš„æ˜¯é€šè¿‡æ˜¾å¼å¾—ä¿®æ”¹æ¨¡å‹ä½¿å…¶èƒ½å¤Ÿé¿å…å¯¹è®­ç»ƒé›†è¿‡æ‹Ÿåˆã€‚

ä¸€ç§æœ€å¸¸è§çš„åº”ç”¨äºå‚æ•°çš„æ­£åˆ™åŒ–æ–¹æ¡ˆæ˜¯l2æ­£åˆ™åŒ–ï¼Œå³l2 regularization a.k.a weight decayã€‚ä¼ ç»Ÿè®¤ä¸ºï¼Œæ¨¡å‹å‚æ•°å€¼çš„å¤§å°å¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸ŠæŒ‡ç¤ºå‡ºæ¨¡å‹çš„å¤æ‚åº¦ï¼Œå› æ­¤é€šè¿‡åœ¨ä¼˜åŒ–ç›®æ ‡ä¸­å¼•å…¥l2æ­£åˆ™é¡¹æ¥æ§åˆ¶æ¨¡å‹çš„å¤§å°ã€‚ä¸€èˆ¬åœ°ï¼Œå¼•å…¥l2 regularizationçš„æœºå™¨å­¦ä¹ ä¼˜åŒ–é—®é¢˜å¯ä»¥è¡¨ç¤ºä¸ºï¼š

{{< math_block >}}
\mathrm{minimize} \quad \frac{1}{m}\sum_i^m{l(h_{w_{1:L}}(x^{(i)}, y^{(i)}))}+\frac{\lambda}{2}\sum_{i=1}^L{||w_i||_F^2}
{{< /math_block >}}
å…¶ä¸­ï¼Œ$||w_i||_F$æ˜¯FrobeniusèŒƒæ•°ï¼Œå…¶è¡¨ç¤ºçŸ©é˜µæ¯ä¸ªå…ƒç´ çš„å¹³æ–¹å’Œçš„å¹³æ–¹æ ¹ã€‚

å¾—ç›Šäºè¿™é‡Œçš„ç³»æ•°æ˜¯$1/2$ï¼Œåœ¨å¯¹$w_i$æ±‚å¯¼æ—¶æ­£åˆ™é¡¹æ°å¥½ä¸º$\lambda w_i$ã€‚æ¢¯åº¦æ›´æ–°çš„å…¬å¼ç›¸åº”å˜ä¸ºï¼š
{{< math_block >}}
W_i :=(1-\alpha \lambda)W_i-\alpha \nabla \frac{1}{m}l
{{< /math_block >}}

æ³¨æ„ï¼Œå¼•å…¥l2æ­£åˆ™åŒ–åï¼Œæ¯è½®è¿­ä»£éƒ½ä¼šå°†å‚æ•°ç¼©å°è‡³åŸæ¥çš„$1-\alpha \lambda$ã€‚å¾ˆå¤šåœ°æ–¹ä¸å°†l2æ­£åˆ™åŒ–ä½œä¸ºæŸå¤±å‡½æ•°çš„ä¸€éƒ¨åˆ†ï¼Œè€Œæ˜¯å°†å…¶ä½œä¸ºä¼˜åŒ–å™¨çš„ä¸€éƒ¨åˆ†ï¼Œå³ç›´æ¥å°†å‚æ•°è¿›è¡Œç¼©å°ï¼Œè¿™ç§æ–¹æ³•è¢«ç§°ä¸ºweight decayï¼Œæ˜¾ç„¶äºŒè€…æ˜¯ç­‰ä»·çš„ã€‚

å¦å¤–ä¸€ç§æ­£åˆ™åŒ–æ–¹æ³•æ˜¯dropoutï¼Œå…¶æ€æƒ³æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éšæœºåœ°å°†ä¸€äº›æ¿€æ´»å±‚çš„è¾“å‡ºç½®ä¸º0ï¼Œå¹¶å¯¹å…¶å®ƒè¾“å‡ºæ”¾å¤§ï¼Œä»¥ç¡®ä¿æ•´å±‚è¾“å‡ºçš„æ•°å­¦æœŸæœ›ä¸å˜ï¼Œå½¢å¼åŒ–è¡¨ç¤ºä¸ºï¼š
{{< math_block >}}
\begin{align*}  
\hat{z}_{i+1} &= \sigma_i(W^T_i z_i)+b_i\\  
(z_{i+1})_j &=  
\begin{cases}  
((\hat{z}_{i+1} )_j)/(1-p) \quad &\text{ä»¥æ¦‚ç‡}1-p\\  
0 &\text{ä»¥æ¦‚ç‡}p  
\end{cases}  
\end{align*}
{{< /math_block >}}
åœ¨æ¨ç†æ—¶ï¼Œåˆ™ä¸éœ€è¦è¿›è¡Œdropoutã€‚

ç›´è§‚åœ°è¯´ï¼Œdropoutèƒ½å¤Ÿæå‡æ¨¡å‹åœ¨æ¿€æ´»å±‚éƒ¨åˆ†ç¼ºå¤±æ—¶è¿›è¡Œæ¨ç†çš„èƒ½åŠ›ï¼Œä½†æ˜¾ç„¶è¿™ä¸€èƒ½åŠ›æ²¡ä»€ä¹ˆåµç”¨ã€‚å¦ä¸€ç§è§£é‡Šæ˜¯dropoutæå‡äº†æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„éšæœºæ€§ï¼Œç±»ä¼¼SGDã€‚


# Lecture 10: Convolutional Networks
## Convolutional operators in deep networks
åœ¨hw2ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡flattenæ“ä½œå°†å›¾ç‰‡è§†ä½œä¸€ä¸ªåºåˆ—è¿›è¡Œè®¡ç®—ï¼Œè¿™å¯¹äºå°å°ºå¯¸çš„å›¾ç‰‡æ˜¯å¯è¡Œçš„ï¼Œä½†å¯¹äºå¤§å°ºå¯¸çš„å›¾ç‰‡ï¼Œä¾‹å¦‚256Ã—256çš„å›¾ç‰‡ï¼Œå°†ä¼šå¯¼è‡´è¾“å…¥å¼‚å¸¸åºå¤§ï¼Œç½‘ç»œä¹Ÿéšä¹‹å˜å¤§ã€‚è¿™ç§ç®€å•ç²—æš´çš„å¤„ç†æ–¹å¼ä¸åˆ©äºæå–å›¾ç‰‡çš„å†…åœ¨ç‰¹å¾ï¼Œä¾‹å¦‚ï¼Œå¦‚æœå¯¹å›¾ç‰‡è¿›è¡Œå¹³ç§»ï¼Œå…¶è¾“å…¥åºåˆ—çš„å˜åŒ–ç›¸å½“å¤§ã€‚

å·ç§¯ç½‘ç»œå‡ºäºä»¥ä¸‹ä¸¤ä¸ªåŠ¨æœºï¼š
- å±‚ä¹‹é—´çš„æ¿€æ´»ä»¥å±€éƒ¨çš„æ–¹å¼å‘ç”Ÿï¼Œå¹¶ä¸”éšè—å±‚çš„è¾“å‡ºä¹Ÿè¢«è§†ä¸ºå›¾åƒ
- åœ¨æ‰€æœ‰çš„ç©ºé—´ä½ç½®å…±äº«æƒé‡

å·ç§¯ç½‘ç»œæœ‰ä»¥ä¸‹ä¸¤ä¸ªä¼˜ç‚¹ï¼š
- ä½¿ç”¨çš„å‚æ•°å¾ˆå°‘ã€‚å‚æ•°é‡ç”±å·ç§¯ç½‘ç»œçš„å¤§å°å†³å®šï¼Œè€Œå’Œè¾“å…¥çš„shapeæ— å…³ï¼›
- èƒ½å¤Ÿå¾ˆå¥½åœ°æ•è·å›¾ç‰‡çš„å†…åœ¨ä¸å˜å½¢ã€‚

å·ç§¯çš„è®¡ç®—ç¤ºæ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå·ç§¯æ ¸åœ¨åŸå›¾ä¸Šæ»‘åŠ¨ï¼Œä»è€Œäº§ç”Ÿä¸€å¼ æ–°çš„å›¾ç‰‡ã€‚
![image.png](https://pics.zhouxin.space/202407250959153.png?x-oss-process=image/quality,q_90/format,webp)

åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œè¾“å…¥å’Œéšè—å±‚éƒ½å¾ˆå°‘æ˜¯ä¸€ä¸ª1Dçš„çŸ©é˜µï¼Œä¸€èˆ¬è€Œè¨€ï¼Œå…¶æ˜¯ç”±å¤šä¸ªé€šé“çš„ã€‚ä¾‹å¦‚ï¼Œä¸€å¼ å½©è‰²å›¾ç‰‡ç”±RGBä¸‰é€šé“ç»„æˆï¼Œè€Œä¸­é—´çš„éšè—å±‚ï¼Œé€šå¸¸ä¼šæœ‰æ¯”è¾ƒå¤§çš„é€šé“æ•°ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
![image.png](https://pics.zhouxin.space/202407251015471.png?x-oss-process=image/quality,q_90/format,webp)
è®°å·ç§¯å±‚çš„è¾“å…¥$x\in \mathbb{R}^{h\times w \times c_{in}}$ï¼Œè¾“å‡º$z\in \mathbb{R}^{h\times w \times c_{out}}$ã€‚ä»ä¸Šå›¾å¯ä»¥å‘ç°ï¼Œå·ç§¯è¾“å‡ºçš„æŸä¸ªé€šé“ï¼Œéƒ½æ˜¯ç”±è¾“å…¥åœ¨åŒä¸€ä¸ªå±€éƒ¨çš„æ‰€æœ‰é€šé“å…±åŒå†³å®šçš„ï¼Œå› æ­¤ï¼Œå·ç§¯æ ¸$W\in \mathbb{R}^{c_{in}\times c_{out}\times k \times k}$ï¼Œå·ç§¯è¿‡ç¨‹å¯ä»¥å½¢å¼åŒ–è¡¨ç¤ºä¸ºï¼š
{{< math_block >}}
z[:,:,s] = \sum_{r=1}^{c_{in}}x[:,:,r] \cdot W[r,s,:,:]
{{< /math_block >}}
å…³äºå¤šé€šé“å·ç§¯ï¼Œå¦å¤–ä¸€ç§æ›´ç¬¦åˆç›´è§‰çš„ç†è§£æ˜¯å°†ç›¸åŒä½ç½®çš„å„é€šé“çš„ç»„åˆçœ‹ä½œæ˜¯ä¸€ä¸ªå‘é‡ï¼Œå³ä¸‹å›¾ä¸­ï¼Œ$x$æ¯ä¸€æ ¼éƒ½æ˜¯ä¸€ä¸ªå‘é‡ï¼Œ$W$æ¯ä¸€æ ¼éƒ½æ˜¯$c_{out} \times c_{in}$çš„çŸ©é˜µï¼Œå·ç§¯çš„è¾“å‡ºç”±å¯¹åº”ä½ç½®çš„$z$å’Œ$W$æŒ‰çŸ©é˜µä¹˜æ³•å¹¶æ±‚å’Œå¾—åˆ°ã€‚
![image.png](https://pics.zhouxin.space/202407251027480.png?x-oss-process=image/quality,q_90/format,webp)

## Elements of practical convolutions
åœ¨å®é™…çš„å·ç§¯æ“ä½œä¸­ï¼Œé€šå¸¸è¿˜ä¼šåº”ç”¨ä¸€äº›åˆ«çš„æŠ€æœ¯ã€‚
- Padding
åŸå§‹çš„å·ç§¯æ“ä½œï¼Œä¼šå°†è¾“å‡ºçš„é•¿å®½å˜å°$k-1$ä¸ªé•¿åº¦ï¼Œé€šè¿‡åœ¨å‘¨å›´å¡«å……$(k-1)/2$ä¸ª0å…ƒå¯ä»¥ä¿è¯è¾“å‡ºçš„shapeä¸è¾“å…¥ä¸€è‡´ã€‚ä¸ºäº†é¿å…ä¸¤ä¾§å¡«å……ä¸ä¸€è‡´è¿™ä¸ªåˆ«æ‰­çš„æƒ…å†µï¼Œæˆ‘ä»¬ä¸€èˆ¬é€‰å–å·ç§¯æ ¸å¤§å°ä¸ºå¥‡æ•°ã€‚

- Strided Convolutions / Pooling
ç»è¿‡paddingä¹‹åçš„å·ç§¯æ“ä½œï¼Œä¸æ”¹å˜å›¾ç‰‡çš„shapeï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ï¼Œé€šå¸¸ä¼šå¯¹å›¾ç‰‡è¿›è¡Œä¸‹é‡‡æ ·ã€‚ç”¨ä¸¤ç§è§£å†³æ–¹æ¡ˆï¼š
1. ä½¿ç”¨æœ€å¤§/å¹³å‡æ± åŒ–æ¥èšåˆä¿¡æ¯ï¼Œä¾‹å¦‚ï¼Œä½¿ç”¨ä¸€ä¸ª2Ã—2çš„æ ¸è¿›è¡Œæ± åŒ–æ“ä½œï¼Œæ¯æ¬¡ç§»åŠ¨çš„æ­¥é•¿ä¸º2ï¼Œå°±å¯ä»¥å°†æ•´å¼ å›¾ç‰‡é•¿å®½å„æ”¾ç¼©è‡³åŸæ¥ä¸€åŠï¼›
2. å·ç§¯æ“ä½œæ—¶ï¼Œå·ç§¯æ ¸ç§»åŠ¨çš„æ­¥é•¿å¤§äº1ã€‚

- Grouped Convolutions
å½“è¾“å…¥å’Œè¾“å‡ºçš„é€šé“æ•°å¾ˆå¤§æ—¶ï¼Œå·ç§¯æ ¸çš„å‚æ•°é‡ä»å¯èƒ½éå¸¸éå¸¸å¤§ã€‚ä¸€ç§è§£å†³æ–¹æ¡ˆæ˜¯ï¼Œä½¿ç”¨åˆ†ç»„å·ç§¯ï¼Œå³å°†è¾“å…¥é€šé“åˆ†ä¸ºå¤šä¸ªç»„ï¼Œæ¯ä¸ªç»„ç‹¬ç«‹è¿›è¡Œå·ç§¯æ“ä½œï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚å¦‚æœåˆ†ä¸ºGç»„ï¼Œåˆ™å‚æ•°é‡å¯å‡å°‘ä¸ºåŸæ¥çš„1/Gã€‚
![image.png](https://pics.zhouxin.space/202407251311275.png?x-oss-process=image/quality,q_90/format,webp)

- Dilations
ä¼ ç»Ÿå·ç§¯çš„æ„Ÿå—é‡å’Œå·ç§¯æ ¸ä¸€æ ·å¤§ï¼Œæ‰©å¼ å·ç§¯çš„æ€è·¯æ˜¯åœ¨å·ç§¯åŒºåŸŸä¸­æ’å…¥é—´éš”ï¼Œèƒ½å¤Ÿæ‰©å¤§å·ç§¯æ ¸çš„æ„Ÿå—é‡ã€‚ä¸‹å›¾è¡¨ç¤ºçš„å¾ˆå½¢è±¡ã€‚
![image.png](https://pics.zhouxin.space/202407251316286.png?x-oss-process=image/quality,q_90/format,webp)

## Differentiating convolutions
æ­£å¦‚å‰æ–‡æ‰€æåˆ°çš„ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸€ç³»åˆ—çŸ©é˜µå‘é‡ä¹˜æ³•å’Œæ±‚å’Œè¿ç®—æ¥å®ç°å·ç§¯æ“ä½œï¼Œä½†è¿™ä¹ˆåšæ•ˆç‡å¤ªä½äº†ï¼Œæˆ‘ä»¬çš„è®¡ç®—å›¾ä¸Šæœ‰å¾ˆå¤šä¸­é—´èŠ‚ç‚¹ï¼Œè¿™äº›ä¸­é—´å˜é‡å°†æ¶ˆè€—å¤§é‡çš„å†…å­˜ç©ºé—´ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¸åº”è¯¥ä½¿ç”¨å¾®åˆ†åº“ä¸­çš„ç®—å­æ¥è®¡ç®—å·å­ï¼Œè€Œæ˜¯å°†å…¶ä½œä¸ºä¸€ä¸ªç®—å­æ¥å®ç°ï¼Œå¹¶æ‰‹åŠ¨è®¡ç®—å…¶å¾®åˆ†ã€‚

é¦–å…ˆå®šä¹‰å·ç§¯æ“ä½œï¼š
{{< math_block >}}
z = \operatorname{conv}(x,W)
{{< /math_block >}}
$z$çš„æ¢¯åº¦æ€ä¹ˆä¸adjointsä¹˜å‘¢ï¼Ÿè¿™æ˜¯ä¸ªé—®é¢˜ã€‚$z$çš„æ¢¯åº¦æœ‰ä»¥ä¸‹äºŒè€…ï¼š$\frac{\partial z}{\partial x}$å’Œ$\frac{\partial z}{\partial W}$ï¼Œä»å½¢å¼ä¸Šçœ‹ï¼Œä»–ä»¬æ˜¯3é˜¶å¼ é‡åˆä»¥å››é˜¶å¼ é‡ï¼Œç›¸å½“å¤æ‚ã€‚

é¦–å…ˆè€ƒè™‘æœ€ç®€å•çš„çŸ©é˜µå’Œå‘é‡ç›¸ä¹˜çš„æƒ…å†µï¼Œå³ï¼š
{{< math_block >}}
z = Wx
{{< /math_block >}}
é‚£ä¹ˆ$z$å¯¹$x$çš„å¯¼æ•°å°±æ˜¯$W$ï¼Œå³å…¶ä¸adjointçš„ä¹˜æ³•è®¡ç®—å…¬å¼ä¸ºï¼š
{{< math_block >}}
W^T\bar{v}
{{< /math_block >}}
ä¹Ÿå°±æ˜¯è¯´å¦‚æœåœ¨å‰å‘ä¼ æ’­ä¸­æˆ‘ä»¬è®¡ç®—ä¸€ä¸ªçŸ©é˜µå’Œå‘é‡çš„ä¹˜ç§¯ï¼Œé‚£ä¹ˆåœ¨åå‘ä¼ æ’­ä¸­ï¼Œæˆ‘ä»¬è¦è®¡ç®—è¿™ä¸ªçŸ©é˜µçš„è½¬ç½®å’Œadjointçš„ä¹˜ç§¯ã€‚é‚£å¯¹äºå·ç§¯æ¥è¯´ï¼Œå®ƒçš„â€œè½¬ç½®â€æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿ

- å°†å·ç§¯è§†ä¸ºçŸ©é˜µè¿ç®—I
ä»¥1då·ç§¯ä¸ºä¾‹ï¼Œæˆ‘ä»¬è€ƒè™‘å¦‚ä¸‹çš„ä¸€ä¸ªå·ç§¯è¿ç®—ï¼Œå…¶ä¸­æ¯ä¸ªæ ¼å­éƒ½æ˜¯ä¸€ä¸ªå‘é‡æˆ–è€…çŸ©é˜µã€‚
![image.png](https://pics.zhouxin.space/202407251428228.png?x-oss-process=image/quality,q_90/format,webp)
å°†ä¸Šé¢è¿™ä¸ªçŸ©é˜µè¿ç®—å±•å¼€ï¼Œå¯ä»¥å¾—åˆ°ï¼š
{{< math_block >}}
\begin{bmatrix}z_1\\z_2\\z_3\\z_4\\z_5\end{bmatrix}=x*w=\begin{bmatrix}w_2&w_3&0&0&0\\w_1&w_2&w_3&0&0\\0&w_1&w_2&w_3&0\\0&0&w_1&w_2&w_3\\0&0&0&w_1&w_2\end{bmatrix}\begin{bmatrix}x_1\\x_2\\x_3\\x_4\\x_5\end{bmatrix}
{{< /math_block >}}
æœ‰äº†$\hat{W}$ï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“åœ°å†™å‡º$\hat{W}^T$,å³ï¼š
{{< math_block >}}
\hat W^T=\begin{bmatrix}w_2&w_1&0&0&0\\w_3&w_2&w_1&0&0\\0&w_3&w_2&w_1&0\\0&0&w_3&w_2&w_1\\0&0&0&w_3&w_2\end{bmatrix}
{{< /math_block >}}
ä¸éš¾å‘ç°ï¼Œè¿™ä¸ªç®—å­å®é™…ä¸Šæ˜¯$[w_3, w_2, w_1]$è¿™ä¸ªå·ç§¯æ ¸ï¼Œå³åŸå§‹å·ç§¯æ ¸ç¿»è½¬åçš„å·ç§¯æ ¸ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæ¢¯åº¦å’Œadjointçš„ä¹˜ç§¯å¯ä»¥è¡¨ç¤ºä¸ºï¼š
{{< math_block >}}
\hat{v}\frac{\partial \operatorname{conv}(x,w)}{\partial x} = \operatorname{conv}(\hat{v},\operatorname{flip}(w))
{{< /math_block >}}
- å°†å·ç§¯è§†ä¸ºçŸ©é˜µè¿ç®—II
æ¥ä¸‹æ¥æˆ‘ä»¬è€ƒè™‘å·ç§¯å¯¹äºå‚æ•°$w$çš„å¯¼æ•°ã€‚åŒæ ·ï¼Œæˆ‘ä»¬å°†çŸ©é˜µè¿ç®—å±•å¼€ï¼Œå¯ä»¥å¾—åˆ°ï¼š
{{< math_block >}}
\begin{bmatrix}z_1\\z_2\\z_3\\z_4\\z_5\end{bmatrix}=x*w=\begin{bmatrix}0&x_1&x_2\\x_1&x_2&x_3\\x_2&x_3&x_4\\x_3&x_4&x_5\\x_4&x_5&0\end{bmatrix}\begin{bmatrix}w_1\\w_2\\w_3\end{bmatrix}
{{< /math_block >}}
ç›¸æ¯”çŸ©é˜µè¿ç®—Iï¼Œæˆ‘ä»¬æ„é€ å‡ºçš„$\hat{X}$çŸ©é˜µæ˜¯ä¸€ä¸ªå¯†é›†çŸ©é˜µï¼Œåœ¨å®ç°å·ç§¯ç®—å­æ—¶ï¼Œæˆ‘ä»¬å¸¸å¸¸é‡‡ç”¨è¿™ä¸ªæ–¹æ¡ˆæ¥è¿ç®—ã€‚è¿™ä¸ª$\hat{X}$çŸ©é˜µè¢«ç§°ä¸ºâ€œim2colâ€çŸ©é˜µï¼ˆimage to columnï¼‰ã€‚

# Lecture 11: Hardware acceleration
## General acceleration techniques
ç°ä»£æœºå™¨å­¦ä¹ æ¡†æ¶å¯ä»¥è§†ä¸ºä¸¤å±‚ï¼šä¸Šå±‚æ˜¯è®¡ç®—å›¾ï¼Œç”¨äºå‰å‘æ¨ç†ã€è‡ªåŠ¨å¾®åˆ†å’Œåå‘ä¼ æ’­ï¼›ä¸‹å±‚æ˜¯å¼ é‡çº¿æ€§ä»£æ•°åº“ï¼Œå…¶è´Ÿè´£åº•å±‚çš„å¼ é‡è®¡ç®—ã€‚åœ¨needleä¸­ï¼Œæˆ‘ä»¬ç›®å‰ä½¿ç”¨numpyä½œä¸ºçº¿æ€§ä»£æ•°åº“ã€‚æœ¬èŠ‚æˆ‘ä»¬å°†ä»‹ç»ä¸€äº›å¸¸è§çš„åŠ é€ŸæŠ€æœ¯ã€‚
- Vectorization å‘é‡åŒ–
å¦‚æœæˆ‘ä»¬è¦å°†ä¸¤ä¸ª256é•¿åº¦çš„arrayç›¸åŠ ï¼Œä¸€ç§æ ‡é‡çš„å¤„ç†æ–¹å¼æ˜¯256ä¸ªå…ƒç´ é€ä¸ªç›¸åŠ ï¼Œä½†æ˜¯å¾ˆå¤šç¡¬ä»¶éƒ½æä¾›äº†æ‰¹é‡ä»å†…å­˜è¯»å–ã€å‘é‡è¿ç®—æŒ‡ä»¤ï¼Œå³ä¼˜åŒ–ä¸ºå¦‚ä¸‹ä»£ç ï¼š
```C
void vecadd(float* A, float* B, float* C){
	for(int i=0; i<64; i++){
		float4 a = load_float4(A + i*4);
		float4 b = load_float4(B + i*4);
		float4 c = add_float4(a, b);
		store_float4(C + i*4, c);
	}
}
```

è¿™é‡Œè¦æ±‚ABCæ‰€åœ¨çš„å†…å­˜å—è¦æ˜¯æŒ‰ç…§128 bitå¯¹é½çš„ã€‚

- Data layout & strides æ•°æ®å¸ƒå±€&æ­¥å¹…
åœ¨å†…å­˜ä¸­ï¼Œæ•°æ®æ˜¯çº¿æ€§æ’åˆ—çš„ï¼Œå› æ­¤ä¸€ä¸ªçŸ©é˜µåœ¨å†…å­˜ä¸­æœ‰ä¸¤ç§å¸ƒå±€æ–¹å¼ï¼šè¡Œä¼˜å…ˆå’Œåˆ—ä¼˜å…ˆã€‚ä¸€äº›å¤è€çš„è¯­è¨€ä½¿ç”¨åˆ—ä¼˜å…ˆï¼Œç°ä»£çš„è¯­è¨€åå‘ä½¿ç”¨è¡Œä¼˜å…ˆã€‚

åœ¨è®¸å¤šåº“ä¸­ï¼Œè¿˜å¼•å…¥äº†ä¸€ç§strideæ ¼å¼å¸ƒå±€ï¼Œå³åœ¨ä¿å­˜å¼ é‡æ—¶ï¼Œé¢å¤–ä¿å­˜ä¸€ä¸ªæ•°æ®ï¼Œç”¨äºæ ‡è¯†æ¯ä¸ªç»´åº¦ä¸Šéœ€è¦ç§»åŠ¨çš„æ­¥é•¿ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ`a[i, j] = a_data[i * strides[0] + j * strides[1]]`

è¿™ä¸ªæ–¹æ¡ˆå¯ä»¥åœ¨ä¸ç”¨å¤åˆ¶æ•°æ®çš„æƒ…å†µä¸‹å®ç°å¾ˆå¤šæ“ä½œï¼šé€šè¿‡æ”¹å˜offsetå’Œshapeæ¥å®ç°åˆ‡ç‰‡ï¼›é€šè¿‡äº¤æ¢stridesæ¥å®ç°è½¬ç½®ï¼›é€šè¿‡æ’å…¥ç­‰äº0çš„strideæ¥å®ç°å¹¿æ’­ã€‚

å…¶ç¼ºç‚¹æ˜¯è®¿å­˜æ“ä½œå¯èƒ½ä¸å†è¿ç»­ï¼Œå› æ­¤å‘é‡åŒ–æŠ€æœ¯ä¸å¯ç”¨ï¼Œå¾ˆå¤šåº“ä¹Ÿéœ€è¦å…ˆæŠŠä»–ä»¬æ‹¼æ¥ä¹‹åå†ä½¿ç”¨ã€‚

- Parallelization å¹¶è¡ŒåŒ–
ä½¿ç”¨openmpå¯ä»¥å°†è®¡ç®—åˆ†é…ç»™å¤šä¸ªæ ¸å¹¶è¡Œå¤„ç†ï¼š
```C
void vecadd(float* A, float* B, float* C){
	#pragma omp parallel for
	for(int i=0; i<64; i++){
		float4 a = load_float4(A + i*4);
		float4 b = load_float4(B + i*4);
		float4 c = add_float4(a, b);
		store_float4(C + i*4, c);
	}
}
```

## Case study: matrix multiplication
æœ¬èŠ‚æˆ‘ä»¬å°†è®¨è®ºå¦‚ä½•ä¼˜åŒ–çŸ©é˜µä¹˜æ³•ã€‚
- Vanilla matrix multiplication æœ´ç´ çŸ©é˜µä¹˜æ³•
æœ€æœ´ç´ çš„æƒ³æ³•æ˜¯ä½¿ç”¨ä¸‰é‡å¾ªç¯å®Œæˆï¼Œå…¶å¤æ‚åº¦æ˜¯$O(n^3)$ï¼Œå³å¦‚ä¸‹ä»£ç ï¼š
```c
float A[n][n], B[n][n], C[n][n];

for(int i=0; i<n; i++){
	for(int j=0; j<n; j++){
		c[i][j] = 0;
		for(int k=0; k<n; k++){
		c[i][j] += A[i][k] * B[k][j];
		}
	}
}
```

åœ¨ç°ä»£å­˜å‚¨å™¨ä¸­ï¼ŒL1 cacheçš„é€Ÿåº¦æ¯”DRAMå¿«200å€ï¼Œé€šè¿‡ä¼˜åŒ–æ•°æ®çš„è¯»å–å°±å¯ä»¥æ˜¾è‘—æå‡è®¡ç®—é€Ÿåº¦ï¼Œè€ƒè™‘åˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥å°†ä¸­é—´å˜é‡ä¿å­˜åˆ°å¯„å­˜å™¨ä¸­ï¼Œå³ï¼š
```c
dram float A[n][n], B[n][n], C[n][n];

for(int i=0; i<n; i++){
	for(int j=0; j<n; j++){
		register float c = 0;
		for(int k=0; k<n; k++){
		register float a = A[i][k];
		register float b = B[k][j];
		c += a*b;
		}
		C[i][j] = c;
	}
}
```
ä¸Šè¿°ä»£ç ä¸­ï¼Œä»è¯»å–Aã€Båˆ°å¯„å­˜å™¨çš„æ“ä½œåˆ†åˆ«è¿›è¡Œäº†$n^3$æ¬¡ï¼Œéœ€è¦3ä¸ªå¯„å­˜å™¨æ¥å®Œæˆè¯¥æ“ä½œã€‚

- Register tiled matrix multiplication å¯„å­˜å™¨åˆ†å—çŸ©é˜µä¹˜æ³•
è¯¥æ–¹æ¡ˆçš„æ€è·¯æ˜¯å°†ç»“æœè¿›è¡Œåˆ†å—ï¼Œæ¯æ¬¡è®¡ç®—å…¶ä¸­çš„ä¸€å—ï¼Œå³ï¼š
```c
dram float A[n/v1][n/v3][v1][v3];
dram float B[n/v2][n/v3][v2][v3];
dram float C[n/v1][n/v2][v1][v2];

for (int i = 0; i < n/v1; ++i) {
    for (int j = 0; j < n/v2; ++j) {
        register float c[v1][v2] = 0;
        for (int k = 0; k < n/v3; ++k) {
            register float a[v1][v3] = A[i][k];
            register float b[v2][v3] = B[j][k];
            c += dot(a, b.T);
        }
        C[i][j] = c;
    }
}
```
ä¸Šè¿°ä»£ç ä¸­ï¼Œè¦è®¡ç®—çš„çŸ©é˜µCè¢«åˆ†ä¸º$v_1\times v_2$çš„å°çŸ©é˜µï¼Œä¸ºäº†è®¡ç®—å‡ºæ¯ä¸€å—ï¼Œæ¯æ¬¡å¿…é¡»ä»Aä¸­é€‰å‡º$v_1$è¡Œï¼Œä»Bä¸­é€‰å‡º$v_2$åˆ—ï¼Œè¿™ä¸¤ç»„å­çŸ©é˜µå¯ä»¥æŒ‰ç…§é•¿åº¦$v_3$å†æ¬¡åˆ’åˆ†ã€‚åœ¨è®¡ç®—ä¸­ï¼Œå‰ä¸¤ä¸ªå¾ªç¯ä¾æ¬¡éå†Cä¸­çš„ä¸€å°å—ï¼Œç„¶ååˆå§‹åŒ–$v_1 \times v_2$ä¸ªå¯„å­˜å™¨ç”¨äºä¿å­˜è¯¥å—å†…å®¹ï¼Œç„¶åå†æ ¹æ®$v_3$çš„å¤§å°äºŒæ¬¡åˆ’åˆ†ï¼Œè¿›è¡ŒçŸ©é˜µè¿ç®—ï¼Œå°†è¿™äº›ç»“æœåŠ åˆ°å¯¹åº”çš„å¯„å­˜å™¨ä¸Šï¼Œç¬¬ä¸‰ä¸ªå¾ªç¯ç»“æŸåå°±è®¡ç®—å‡ºCçš„ä¸€ä¸ªå­å—ã€‚

Açš„æ•°æ®åŠ è½½å¼€é”€æ˜¯$n^3/v_2$ï¼ŒBçš„æ•°æ®åŠ è½½å¼€é”€æ˜¯$n^3/v_1$ï¼ŒAçš„å¯„å­˜å™¨å¼€é”€æ˜¯$v_1 \times v_3$ï¼ŒBçš„å¯„å­˜å™¨å¼€é”€æ˜¯$v_2\times v_3$ï¼ŒCçš„å¯„å­˜å™¨å¼€é”€æ˜¯$v_1\times v_2$ã€‚æ³¨æ„åˆ°$v_3$ä¸å½±å“æ•°æ®åŠ è½½çš„å¼€é”€ï¼Œå› æ­¤å¯ä»¥å–$v_3$ä¸º1ï¼Œç„¶ååœ¨æ»¡è¶³å¯„å­˜å™¨æ€»æ•°çº¦æŸçš„æƒ…å†µä¸‹ï¼Œæœ€å¤§åŒ–$v_1$å’Œ$v_2$ã€‚

ä¹‹æ‰€ä»¥èƒ½å¤Ÿå‡å°å¼€é”€æ˜¯å› ä¸ºåœ¨çŸ©é˜µè®¡ç®—ä¸­ï¼Œå…ƒç´ è¢«é‡å¤ä½¿ç”¨ï¼Œé€šè¿‡æ¯æ¬¡è®¡ç®—ä¸€ä¸ªåˆ†å—çš„æ–¹å¼ï¼Œå¯ä»¥ä¿è¯è¿™ä¸ªåˆ†å—å†…ç”¨åˆ°çš„é‡å¤æ•°æ®åªè¦åŠ è½½ä¸€æ¬¡ã€‚

- Cache line aware tiling ç¼“å­˜è¡Œæ„ŸçŸ¥åˆ†å—
å‰é¢æˆ‘ä»¬ä½¿ç”¨å¯„å­˜å™¨æ¥è¿›è¡ŒåŠ é€Ÿï¼Œæœ¬èŠ‚æˆ‘ä»¬è€ƒè™‘ä½¿ç”¨cacheæ¥åŠ é€Ÿã€‚æˆ‘ä»¬çš„å®ç°ä»£ç ä¸ºï¼š
```c
dram float A[n/b1][b1][n];
dram float B[n/b2][b2][n];
dram float C[n/b1][n/b2][b1][b2];

for (int i = 0; i < n/b1; ++i) {
    l1cache float a[b1][n] = A[i];
    for (int j = 0; j < n/b2; ++j) {
        l1cache float b[b2][n] = B[j];
        
        C[i][j] = dot(a, b.T);
    }
}
```
ä¸Šè¿°ä»£ç ä¸­ï¼Œç»“æœçŸ©é˜µCè¢«åˆ†å—ä¸º$b_1 \times b_2$ï¼ŒAå’ŒBåˆ†åˆ«æŒ‰è¡Œå’ŒæŒ‰åˆ—åˆ†å—ï¼Œé€šè¿‡ä¸¤å±‚å¾ªç¯éå†è®¡ç®—Cä¸­çš„æ¯ä¸ªå­å—ï¼Œè®¡ç®—å­å—çš„è¿‡ç¨‹å¯ä»¥ä½¿ç”¨å¯„å­˜å™¨åˆ†å—è¿›è¡ŒåŠ é€Ÿã€‚

ä¸Šè¿°ä»£ç ä¸­ï¼ŒAçš„åŠ è½½å¼€é”€æ˜¯$n^2$ï¼ŒBçš„åŠ è½½å¼€é”€æ˜¯$n^3/b1$ã€‚æœ‰ä¸¤ä¸ªçº¦æŸï¼Œä¸€ä¸ªæ˜¯$b_1n+b_2n < \text{l1 chche size}$ï¼Œå¦ä¸€ä¸ªæ˜¯$b_1 \% v_1=b_2 \% v_2 = 0$ã€‚

- Put it together
å°†ç¼“å­˜ç‰ˆæœ¬çš„`dot`è¿ç®—ä½¿ç”¨å¯„å­˜å™¨ç‰ˆæœ¬å±•å¼€ï¼Œå¯ä»¥å¾—åˆ°æœ€ç»ˆçš„åˆ†å—ä¹˜æ³•å®ç°ï¼š
```c
dram float A[n/b1][b1/v1][n][v1];
dram float B[n/b2][b2/v2][n][v2];

for (int i = 0; i < n/b1; ++i) {
    l1cache float a[b1/v1][n][v1] = A[i];
    for (int j = 0; j < n/b2; ++j) {
        l1cache b[b2/v2][n][v2] = B[j];
        for (int x = 0; x < b1/v1; ++x)
            for (int y = 0; y < b2/v2; ++y) {
                register float c[v1][v2] = 0;
                for (int k = 0; k < n; ++k) {
                    register float ar[v1] = a[x][k][:];
                    register float br[v2] = b[y][k][:];
                    C += dot(ar, br.T)
                }
            }
    }
}
```

ä¸Šè¿°ä»£ç çš„æ•°æ®åŠ è½½å¼€é”€æ˜¯ï¼š
{{< math_block >}}
speed_{l1}\cdot(\frac{n^3}{v_2}+\frac{n^3}{v1})+speed_{dram}\cdot(n^2+\frac{n^3}{b_1})
{{< /math_block >}}

# Lecture 12: GPU acceleration
## GPU programming
å¦‚ä¸‹å›¾æ‰€ç¤ºï¼ŒCPUæ˜¯ä¸€ç§é€šç”¨å¤„ç†å™¨ï¼Œå…¶å¯ä»¥çµæ´»åœ°å¤„ç†ä¸åŒçš„ä»»åŠ¡ï¼Œæ¯ä¸ªæ ¸éƒ½æœ‰ç‹¬ç«‹çš„æ§åˆ¶å™¨ã€‚ä½†åœ¨æŸäº›ä»»åŠ¡ï¼Œä¾‹å¦‚å›¾å½¢æ¸²æŸ“ä¸­ï¼Œå¯èƒ½å­˜åœ¨å¤§é‡çš„é‡å¤å·¥ä½œï¼Œä¾‹å¦‚ç»™æ¯ä¸ªåƒç´ éƒ½è¿›è¡Œç›¸åŒçš„å¤„ç†ã€‚GPUæ­£æ˜¯æ“…é•¿å¤„ç†æ­¤ç±»ä»»åŠ¡ï¼Œå…¶æœ‰å¤§é‡çš„æ‰§è¡Œå•å…ƒï¼Œå¯ä»¥æ‰¹é‡æ‰§è¡ŒåŒä¸€æŒ‡ä»¤ã€‚å°†GPUåº”ç”¨äºæ·±åº¦å­¦ä¹ ï¼Œå¯ä»¥å¸¦æ¥10X ~ 100Xçš„åŠ é€Ÿå€ç‡ã€‚
![image.png](https://pics.zhouxin.space/202407260953795.png?x-oss-process=image/quality,q_90/format,webp)
- GPU programming model: SIMT
åœ¨æœ¬ç« èŠ‚ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨CUDAä¸­çš„æœ¯è¯­ï¼Œä½†æ˜¯åœ¨åˆ«çš„æ¨¡å‹ä¸­ï¼Œé€šå¸¸ä¹Ÿæœ‰å¯¹åº”çš„æ¦‚å¿µã€‚

SIMTä¸­æ‰€æœ‰çš„çº¿ç¨‹éƒ½æ‰§è¡Œç›¸åŒçš„æŒ‡ä»¤ï¼Œä½†æ˜¯å…·æœ‰ä¸åŒçš„æ•°æ®é€šè·¯ã€‚çº¿ç¨‹è¢«åˆ†ç»„ä¸ºblockï¼Œæ¯ä¸ªblockå…±äº«å†…å­˜ã€‚blockè¢«åˆ†ç»„ä¸ºlaunch gridï¼Œå½“å¯åŠ¨ä¸€ä¸ªkernelæ—¶ï¼Œå®é™…ä¸Šå°±æ˜¯åœ¨ä¸€ä¸ªgridä¸Šæ‰§è¡Œã€‚
- Example: vector add
ä»¥ä¸‹ä»£ç æ¼”ç¤ºäº†åœ¨CPUå’ŒGPUä¸Šæ‰§è¡Œå‘é‡åŠ æ³•çš„è¿‡ç¨‹ï¼š
```c
void VecAddCPU(float* A, float *B, float* C, int n) {
    for (int i = 0; i < n; ++i) {
        C[i] = A[i] + B[i];
    }
}

__global__ void VecAddKernel(float* A, float *B, float* C, int n) {
    int i = blockDim.x * blockIdx.x + threadIdx.x;
    if (i < n) {
        C[i] = A[i] + B[i];
    }
}
```

ä»GPUç‰ˆæœ¬æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæ¯ä¸ªçº¿ç¨‹æ‰§è¡Œçš„æŒ‡ä»¤éƒ½æ˜¯ç›¸åŒï¼Œä¸åŒçš„æ˜¯æ¯ä¸ªçº¿ç¨‹å…·æœ‰ä¸åŒçš„ç¯å¢ƒå˜é‡ã€‚

ä¸ºäº†æ‰§è¡Œä¸Šè¿°GPUä»£ç ï¼Œåœ¨ä¸»æœºç«¯è¦æ‰§è¡Œä»¥ä¸‹å†…å®¹ï¼š
```c
void VecAddCUDA(float *Acpu, float *Bcpu, float *Ccpu, int n) {
    float *dA, *dB, *dC;
    cudaMalloc(&dA, n * sizeof(float));
    cudaMalloc(&dB, n * sizeof(float));
    cudaMalloc(&dC, n * sizeof(float));

    cudaMemcpy(dA, Acpu, n * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(dB, Bcpu, n * sizeof(float), cudaMemcpyHostToDevice);

    int threads_per_block = 512;
    int nblocks = (n + threads_per_block - 1) / threads_per_block;
    VecAddKernel<<<nblocks, threads_per_block>>>(dA, dB, dC, n);

    cudaMemcpy(Ccpu, dC, n * sizeof(float), cudaMemcpyDeviceToHost);

    cudaFree(dA);
    cudaFree(dB);
    cudaFree(dC);
}
```
å‡½æ•°çš„è¾“å…¥æ˜¯æ¥è‡ªcpuå†…å­˜ä¸Šçš„ä¸‰ä¸ªæ•°ç»„ï¼Œåœ¨GPUä¸Šåˆ†é…å‡ºå¯¹åº”å¤§å°çš„æ˜¾å­˜ï¼Œç„¶åå°†ä¸¤ä¸ªåŠ æ•°æ‹·è´åˆ°è®¾å¤‡ä¸­ã€‚æ ¹æ®æ•°æ®çš„è§„æ¨¡ç¡®å®šè¦å¯ç”¨çš„blockæ•°é‡ï¼Œç„¶åæ‰§è¡ŒGPUä»£ç ï¼Œæœ€åå°†ç»“æœæ‹·è´ä¼šCPUå†…å­˜å¹¶é‡Šæ”¾ç›¸åº”æ˜¾å­˜ã€‚

åœ¨å®é™…ä¸­ï¼Œå†…å­˜æ‹·è´æ˜¯ä¸€ä¸ªéå¸¸è€—æ—¶çš„è¿‡ç¨‹ï¼Œå› æ­¤æˆ‘ä»¬å¸Œæœ›å°†æ•°æ®ä¸€ç›´ä¿ç•™åœ¨æ˜¾å­˜ä¸­è¿›è¡Œè®¡ç®—ï¼Œè€Œéé¢‘ç¹åœ°æ¥å›æ‹·è´ã€‚

- Example: window sum
window sumæ˜¯ä¸€ç§æƒé‡å…¨ä¸º1çš„å·ç§¯ï¼Œä¸€ç§æœ´ç´ çš„æƒ³æ³•æ˜¯è¿™ä¹ˆäº›çš„ï¼š
```c
#define RADIUS 2

__global__ void WindowSumSimpleKernel(float* A, float *B, int n) {
    int out_idx = blockDim.x * blockIdx.x + threadIdx.x;
    if (out_idx < n) {
        float sum = 0;
        for (int dx = -RADIUS; dx <= RADIUS; ++dx) {
            sum += A[dx + out_idx + RADIUS];
        }
        B[out_idx] = sum;
    }
}

```
ä½†æ˜¾ç„¶ï¼Œè¿™ä¸ªç®—æ³•å¹¶ä¸é«˜æ•ˆï¼Œå°†é‡å¤è®¿é—®æ•°æ®ï¼Œè¦åŠ è½½$5n$æ¬¡æ•°æ®ã€‚

è¿™æ—¶å€™å¯ä»¥å¼•å…¥å…±äº«å†…å­˜è¿›è¡Œä¼˜åŒ–ï¼Œå°†ä¸€ä¸ªblockå†…è¦è¦ç”¨åˆ°çš„æ•°æ®å…¨éƒ¨è¯»å–åˆ°å…±äº«å†…å­˜ä¸­ã€‚æ•°æ®åŠ è½½çš„ä»»åŠ¡å¯ä»¥åˆ†ç»™æ¯ä¸ªçº¿ç¨‹å¹¶è¡Œå®Œæˆï¼Œæ˜¾è‘—é™ä½äº†å†…å­˜åŠ è½½æ—¶é—´å¼€é”€ã€‚

```C
__global__ void WindowSumSharedKernel(float* A, float* B, int n) {
    __shared__ float temp[THREADS_PER_BLOCK + 2 * RADIUS];
    int base = blockDim.x * blockIdx.x;
    int out_idx = base + threadIdx.x;
    if (base + threadIdx.x < n) {
        temp[threadIdx.x] = A[base + threadIdx.x];
    }
    if (threadIdx.x < 2 * RADIUS && base + THREADS_PER_BLOCK + threadIdx.x < n) {
        temp[threadIdx.x + THREADS_PER_BLOCK] = A[base + THREADS_PER_BLOCK + threadIdx.x];
    }
    __syncthreads();
    if (out_idx < n) {
        float sum = 0;
        for (int dx = -RADIUS; dx <= RADIUS; ++dx) {
            sum += temp[threadIdx.x + dx + RADIUS];
        }
        B[out_idx] = sum;
    }
}

```
é€šè¿‡`__syncthreads`åŒæ­¥ï¼Œç¡®ä¿æ‰€æœ‰çº¿ç¨‹éƒ½å°†æ•°æ®åŠ è½½å®Œæ¯•ï¼Œç„¶åå†è®¡ç®—window sumã€‚


## Case study: matrix multiplication on GPU
ä»çº¿ç¨‹çš„ç»†ç²’åº¦æ¥è¯´ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨GPUä¸Šå®ç°ä¸€ä¸ªå¯„å­˜å™¨åˆ†å—ç‰ˆæœ¬çš„çŸ©é˜µä¹˜æ³•ï¼š
```c
__global__ void mm(float A[N][N], float B[N][N], float C[N][N]) {
    int ybase = blockIdx.y * blockDim.y + threadIdx.y;
    int xbase = blockIdx.x * blockDim.x + threadIdx.x;

    float c[V][V] = {0};
    float a[V], b[V];
    for (int k = 0; k < N; ++k) {
        a[:] = A[k, ybase*V : ybase*V + V];
        b[:] = B[k, xbase*V : xbase*V + V];
        for (int y = 0; y < V; ++y) {
            for (int x = 0; x < V; ++x) {
                c[y][x] += a[y] * b[x];
            }
        }
    }
    C[ybase * V : ybase * V + V, xbase * V : xbase * V + V] = c[:,:];
}

```
æ¯ä¸ªçº¿ç¨‹è´Ÿè´£è®¡ç®—ä¸€ä¸ªåˆ†å—çš„ç»“æœï¼Œå³æ¯æ¬¡è®¡ç®—ä¸‹å›¾ä¸­çš„ä¸€å—ã€‚
![image.png](https://pics.zhouxin.space/202407261324561.png?x-oss-process=image/quality,q_90/format,webp)
è¿˜å¯ä»¥å°†è®¡ç®—ä¸€å—çš„ä»»åŠ¡äº¤ç»™ä¸€ä¸ªblockï¼Œè¿™æ ·å°±å¯ä»¥ä½¿ç”¨å…±äº«å†…å­˜æŠ€æœ¯æœ‰blockå†…çš„çº¿ç¨‹å…±åŒåŠ è½½è¦ç”¨åˆ°çš„æ•°æ®ã€‚
```c
__global__ void mm(float A[N][N], float B[N][N], float C[N][N]) {
    __shared__ float sA[S][L], sB[S][L];
    float c[V][V] = {0};
    float a[V], b[V];
    int yblock = blockIdx.y;
    int xblock = blockIdx.x;

    for (int ko = 0; ko < N; ko += S) {
        __syncthreads();
        // needs to be implemented by thread cooperative fetching
        sA[:, :] = A[ko + S, yblock * L : yblock * L + L];
        sB[:, :] = B[ko + S, xblock * L : xblock * L + L];
        __syncthreads();

        for (int ki = 0; ki < S; ++ki) {
            a[:] = sA[ki, threadIdx.x * V + V];
            b[:] = sB[ki, threadIdx.x * V + V];
            for (int y = 0; y < V; ++y) {
                for (int x = 0; x < V; ++x) {
                    c[y][x] += a[y] * b[x];
                }
            }
        }
    }

    int ybase = blockIdx.y * blockDim.y + threadIdx.y;
    int xbase = blockIdx.x * blockDim.x + threadIdx.x;
    C[ybase * V : ybase * V + V, xbase * V : xbase * V + V] = c[:, :];
}

```
ä¸Šè¿°ä»£ç ä»å…¨éƒ¨å†…å­˜åˆ°å…±äº«å†…å­˜çš„åŠ è½½è¿‡ç¨‹è¢«å¤ç”¨Læ¬¡ï¼ˆè®¡ç®—æ¯ä¸ªåˆ†å—çŸ©é˜µéƒ½è¦è¯»å–Læ¬¡ABçš„è¡Œåˆ—å‘é‡ï¼‰ï¼Œä»å…±äº«å†…å­˜åˆ°å¯„å­˜å™¨è¢«å¤ç”¨Væ¬¡ï¼ˆåœ¨åˆ†å—çŸ©é˜µä¸­æŒ‰ç…§é•¿åº¦Vè¿›è¡Œäº†äºŒæ¬¡åˆ†å—è®¡ç®—ï¼‰
![image.png](https://pics.zhouxin.space/202407261448550.png?x-oss-process=image/quality,q_90/format,webp)
å„çº¿ç¨‹è¯»å–æ•°æ®åˆ°å…±äº«å†…å­˜çš„è¿‡ç¨‹ä¸ºï¼š
```c
sA[:, :] = A[k : k + S, yblock * L : yblock * L + L];


int nthreads = blockDim.y * blockDim.x;
int tid = threadIdx.y * blockDim.x + threadIdx.x;
for(int j = 0; j < L * S / nthreads; ++j) {
    int y = (j * nthreads + tid) / L;
    int x = (j * nthreads + tid) % L;
    s[y, x] = A[k + y, yblock * L + x];
}

```

# Lecture 13: Hardware Acceleration Implemetation
è¿™èŠ‚æ˜¯å®éªŒè¯¾ï¼Œåœ¨è¿™èŠ‚è¯¾ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ needleåº“ä¸­CPUå’ŒGPUåº•ç«¯å…·ä½“å®ç°çš„ä»£ç éª¨æ¶ã€‚

è¿™èŠ‚è¯¾ä¸åšç¬”è®°ï¼Œæœ¬èŠ‚è¯¾å†…å®¹å¯é€šè¿‡å®Œæˆhw3å­¦ä¹ ã€‚

# Lecture 14: Implementing Convolutions
æœ¬èŠ‚è¯¾å°†å­¦ä¹ å·ç§¯ç®—å­çš„å…·ä½“å®ç°ã€‚

## å­˜å‚¨æ ¼å¼ Storage Order
å¯¹äºå›¾ç‰‡æ•°æ®æˆ–è€…éšè—å±‚ï¼Œæˆ‘ä»¬éœ€è¦å­˜å‚¨`batch_size*channel*height*width`å³`B*C*H*W`ä¸ªå…ƒç´ ï¼Œæœ¬è¯¾ç¨‹ä¸­ï¼Œæˆ‘ä»¬é€‰å–çš„å­˜å‚¨æ ¼å¼ä¸ºï¼š
```cpp
float Z[BATCHES][HEIGHT][WIDTH][CHANNELS];
```
ä¸Šè¿°æ ¼å¼è¢«ç§°ä¸ºNHWCæ ¼å¼ï¼ˆNä»£è¡¨numberï¼‰ã€‚PyTorché»˜è®¤æ ¼å¼ä¸ºNCHWï¼Œå…¶åœ¨åæœŸç‰ˆæœ¬ä¹Ÿæ”¯æŒNHWCã€‚ä¸åŒçš„æ ¼å¼ä¼šå½±å“æ“ä½œçš„æ€§èƒ½ï¼šå·ç§¯åœ¨NHWCä¸Šæ›´å¿«ï¼ŒBatch Normåœ¨NCHWä¸Šæ›´å¿«ã€‚

å¯¹äºå·ç§¯æ ¸ï¼Œå…¶éœ€è¦å­˜å‚¨`k*k*C_in*C_out`ä¸ªå…ƒç´ ï¼Œæœ¬è¯¾ç¨‹æˆ‘ä»¬é€‰å–çš„å­˜å‚¨æ ¼å¼ä¸ºï¼š
```cpp
float weights[KERNEL_SIZE][KERNEL_SIZE][IN_CHANNELS][OUT_CHANNELS];
```
PyTorché€‰æ‹©çš„æ ¼å¼ä¸º`(C_out, C_in, k, k)`ã€‚

## forå¾ªç¯å®ç°å·ç§¯ Convolutions with simple loops
é€šè¿‡å¾ªç¯æ¥å®ç°å·ç§¯æ“ä½œçš„è¿‡ç¨‹ï¼Œä»å¤–åˆ°å†…ï¼Œå¾ªç¯è¿­ä»£çš„å‚æ•°ä¾æ¬¡ä¸ºï¼šbatchã€channel_inã€channel_outã€out_rowã€out_columnï¼Œè¿˜æœ‰ä¸¤ä¸ªå¾ªç¯ç”¨äºå®ç°å·ç§¯ï¼Œå…±ä¸ƒä¸ªå¾ªç¯ï¼š
```python
def conv_naive(Z, weight):
    N,H,W,C_in = Z.shape
    K,_,_,C_out = weight.shape
    
    out = np.zeros((N,H-K+1,W-K+1,C_out));
    for n in range(N):
        for c_in in range(C_in):
            for c_out in range(C_out):
                for y in range(H-K+1):
                    for x in range(W-K+1):
                        for i in range(K):
                            for j in range(K):
                                out[n,y,x,c_out] += Z[n,y+i,x+j,c_in] * weight[i,j,c_in,c_out]
```

è¯¥ä¸ƒé‡å¾ªç¯å®ç°çš„å·ç§¯è€—æ—¶3ç§’ï¼Œè€ŒPyTorchä»…éœ€1.2æ¯«ç§’ï¼Œçº¦2500å€çš„æ€§èƒ½å·®è·ã€‚

## çŸ©é˜µä¹˜æ³•å®ç°å·ç§¯ Convolutions as matrix multiplications
å·ç§¯æ ¸ä¸­ä»»æ„ä¸€ä¸ªå…ƒç´ \[ i, j, :, : \]éƒ½æ˜¯ä¸€ä¸ªshapeä¸º(c_in, c_out)çš„çŸ©é˜µï¼Œå½“å…¶ä½œç”¨åœ¨è¾“å…¥å›¾ç‰‡çš„æŸä¸ªå…ƒç´ (p,q,m,:)å³ä½œç”¨åœ¨ä¸€ä¸ªé•¿åº¦ä¸ºc_inçš„å‘é‡ä¸Šæ—¶ï¼Œè¿™ä¸ªè¿‡ç¨‹å°±æ˜¯ä¸€ä¸ªçŸ©é˜µä¹˜æ³•è¿ç®—ã€‚

ç‰¹åˆ«çš„ï¼Œå¯¹äºå·ç§¯æ ¸å¤§å°ä¸º1Ã—1çš„æƒ…å†µï¼Œæ•´ä¸ªå·ç§¯è¿‡ç¨‹å¯ä»¥ç›´æ¥ç”¨ä¸€ä¸ªçŸ©é˜µä¹˜æ³•æ¥è¡¨ç¤ºï¼š
```python
W1 = np.random.randn(1,1,8,16)
out = conv_reference(Z,W1)
```

æ€ä¹ˆå°†1Ã—1çš„å·ç§¯æ ¸æ¨å¹¿åˆ°ä¸€èˆ¬æƒ…å†µå‘¢ï¼Ÿå¯ä»¥æŠŠå·ç§¯æ ¸çœ‹æˆç”±ä¸€ä¸ªä¸ª1Ã—1çš„å°å·ç§¯æ ¸ç»„æˆçš„ï¼Œä¸æ–­è¿­ä»£è¿™äº›å·ç§¯æ ¸å³å¯ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ¯ä¸ªå°å·ç§¯æ ¸åœ¨å›¾ç‰‡ä¸Šä½œç”¨çš„èŒƒå›´éƒ½ä¸ä¸€æ ·ï¼Œè¦åšå¥½åˆ‡ç‰‡ï¼š
```python
def conv_matrix_mult(Z, weight):
    N,H,W,C_in = Z.shape
    K,_,_,C_out = weight.shape
    out = np.zeros((N,H-K+1,W-K+1,C_out))
    
    for i in range(K):
        for j in range(K):
            out += Z[:,i:i+H-K+1,j:j+W-K+1,:] @ weight[i,j]
    return out
```

è¯¥ç‰ˆæœ¬å·ç§¯è€—æ—¶17æ¯«ç§’ï¼Œç›¸æ¯”PyTorch1.2æ¯«ç§’ï¼Œçº¦14å€æ€§èƒ½å·®è·ã€‚

## é€šè¿‡stridesæ¥æ“ä½œçŸ©é˜µ Manipulating matrices via strides
åœ¨å†…å­˜ä¸­ï¼Œé€šå¸¸å°†çŸ©é˜µæŒ‰ç…§äºŒç»´æ•°ç»„çš„å½¢å¼åœ¨å†…å­˜ä¸­å­˜å‚¨ï¼š
```cpp
float A[M][N];
```

ä½†æ˜¯ï¼Œæˆ‘ä»¬åœ¨å®ç°ä¸€äº›é«˜æ•ˆç®—å­æ—¶ï¼Œç»å¸¸ä¼šæŠŠçŸ©é˜µåˆ†å—ï¼Œå¦‚æœå°†å…¶åˆ†å—å­˜å‚¨ï¼Œé‚£ä¹ˆè¿™äº›ç®—å­å°†ä¼šå…·æœ‰æ›´å¥½çš„ç©ºé—´å±€éƒ¨æ€§ï¼š
```cpp
float A[M/TILE][N/TILE][TILE][TILE]
```

NumPyæä¾›äº†ä¸€ä¸ªå‡½æ•°ç”¨äºå®ç°ä»äºŒç»´æ•°ç»„è½¬å˜ä¸ºåˆ†å—çŸ©é˜µçš„æ ¼å¼ï¼š`np.lib.stride_tricks.as_strided`[^4]ã€‚

å…·ä½“æ¥è¯´ï¼Œ`as_strided`è¿™ä¸ªå‡½æ•°ç”¨äºåˆ›å»ºä¸€ä¸ªå…·æœ‰ä¸åŒshapeå’Œstridesï¼Œä½†ä¸åŸarrayå…·æœ‰ç›¸åŒåº•å±‚æ•°æ®çš„è§†å›¾ï¼ˆviewï¼‰ã€‚

ä¸¾ä¸ªä¾‹å­ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œä¸€ä¸ª6Ã—6çš„çŸ©é˜µï¼Œå¯¹äºæŒ‰ç…§2Ã—2è¿›è¡Œåˆ†å—ï¼Œæˆ‘ä»¬ä»strides\[3\]å€’ç€å†™å‡ºå…¶å€¼ã€‚
![image.png](https://pics.zhouxin.space/202408252356974.webp?x-oss-process=image/quality,q_90/format,webp)
- strides \[3\]è¡¨ç¤ºåœ¨å­çŸ©é˜µå†…éƒ¨ç§»åŠ¨åˆ°ä¸‹ä¸€åˆ—çš„å…ƒç´ çš„æ­¥é•¿ï¼Œå³ä»0ç§»åŠ¨åˆ°1çš„æ­¥é•¿ï¼Œæ•°æ®åœ¨å†…å­˜ä¸­æ˜¯æŒ‰è¡Œè¿ç»­æ’åˆ—çš„ï¼Œå› æ­¤å…¶å€¼ä¸º1ï¼›
- strides \[2\]è¡¨ç¤ºåœ¨å­çŸ©é˜µä¸­ç§»åŠ¨åˆ°ä¸‹ä¸€è¡Œå…ƒç´ çš„æ­¥é•¿ï¼Œå³ä»0ç§»åŠ¨åˆ°6çš„æ‰€éœ€æ­¥é•¿ï¼Œè§‚å¯Ÿå›¾ç‰‡å¯ä»¥çœ‹åˆ°è¯¥æ­¥æ­¥é•¿ç­‰äºçŸ©é˜µçš„åˆ—æ•°Nï¼Œå³6ï¼›
- strides \[1\]è¡¨ç¤ºä»ä¸€ä¸ªå­çŸ©é˜µç§»åŠ¨åˆ°åŒè¡Œä¸‹ä¸€ä¸ªå­çŸ©é˜µçš„å¯¹åº”ä½ç½®çš„æ­¥é•¿ï¼Œå³ä»0ç§»åŠ¨åˆ°2çš„æ­¥é•¿ï¼Œå¯ä»¥çœ‹åˆ°ç§»åŠ¨çš„æ­¥é•¿ç­‰äºåˆ†å—çš„åˆ—é•¿åº¦TILEï¼Œå³2ï¼›
- strides \[0\]è¡¨ç¤ºä»ä¸€ä¸ªå­çŸ©é˜µç§»åŠ¨åˆ°åŒåˆ—ä¸‹ä¸€ä¸ªå­çŸ©é˜µå¯¹åº”ä½ç½®çš„æ­¥é•¿ï¼Œå³ä»0ç§»åŠ¨åˆ°12çš„æ­¥é•¿ï¼Œå¯ä»¥çœ‹åˆ°ç§»åŠ¨çš„æ­¥é•¿ç­‰äºTILE\*Nï¼Œå³12ã€‚

ç¡®å®šäº†stridesä¹‹åï¼Œå°±å¯ä»¥ä½¿ç”¨`as_strided`ä¸ºåŸçŸ©é˜µåˆ›å»ºä¸€ä¸ªåˆ†å—çŸ©é˜µçš„è§†å›¾ï¼Œå¹¶ä½¿ç”¨`np.ascontiguousarray`åˆ›å»ºä¸€ä¸ªå†…å­˜è¿ç»­ç‰ˆæœ¬çš„å‰¯æœ¬ï¼š
```python
import numpy as np
n = 6
A = np.arange(n**2, dtype=np.float32).reshape(n,n)

B = np.lib.stride_tricks.as_strided(A, shape=(3,3,2,2), strides=np.array((12,2,6,1))*4)  #numpyä¸­stridesä»¥å­—èŠ‚ä¸ºå•ä½
C = np.ascontiguousarray(B)
```

----------------ä»¥ä¸‹éè¯¾ç¨‹å†…å®¹----------------
è¿™é‡Œæ’ä¸€å˜´ï¼Œè¿™é‡Œå®ç°åˆ†å—çš„æ–¹å¼éå¸¸ä¸ä¼˜é›…ï¼Œæ¯•ç«Ÿnumpyå¹¶ä¸å»ºè®®ä½¿ç”¨è¿™ä¹ˆåº•å±‚çš„APIæ¥ç›´æ¥ä¿®æ”¹æ•°æ®ï¼Œæˆ‘é—®äº†ä¸‹GPTï¼Œä»–æä¾›äº†ä¸€ç§æ›´ä¼˜é›…çš„æ–¹æ¡ˆã€‚

æˆ‘ä»¬é¦–å…ˆå¯ä»¥å°†åŸçŸ©é˜µ(M, N)reshapeä¸º(M//TILE, TILE, N//TILE, TILE)ï¼Œè¿™ä¸€æ­¥ç›¸å½“äºå°†åŸçŸ©é˜µåœ¨è¡Œå’Œåˆ—ä¸Šè¿›è¡Œåˆ†å—ï¼Œå¹¶ä¸”(p,m,q,n)è¡¨ç¤ºç¬¬pè¡Œç¬¬qåˆ—çš„å­çŸ©é˜µä¸­ç¬¬mè¡Œç¬¬nåˆ—ä¸ªå…ƒç´ ã€‚ç„¶åä½¿ç”¨`transpose(0, 2, 1, 3)`é‡æ–°æ’åˆ—ç»´åº¦å³å¯ã€‚

è‡³äºä¸ºä»€ä¹ˆreshapeé‚£ä¸€æ­¥åç´¢å¼•ä»æ˜¯æ­£ç¡®çš„ï¼Œæˆ‘ç•¥å¾®ç†è§£çš„ï¼Œä½†éš¾ä»¥è¡¨è¾¾å‡ºæ¥ï¼Œæœ‰ç‚¹åªå¯æ„ä¼šçš„æ„æ€ï¼šreshapeé‚£ä¸ªæ“ä½œå¯ä»¥åˆ†æˆä¸¤æ­¥å®Œæˆï¼Œåˆ†åˆ«æ˜¯åœ¨è¡Œå’Œåˆ—ä¸Šè¿›è¡Œåˆ‡ç‰‡ï¼Œè¿™ä¸¤ä¸ªæ­¥éª¤åˆä¸å†²çªï¼Œåˆå¹¶åçš„ç»“æœå°±æ˜¯å¦‚ä¸‹çš„ä»£ç ï¼š
```python
def block_matrix(A, TILE):
    M, N = A.shape
    assert M % TILE == 0 and N % TILE == 0, "çŸ©é˜µç»´åº¦å¿…é¡»èƒ½è¢«TILEæ•´é™¤"
    A_reshaped = A.reshape(M//TILE, TILE, N//TILE, TILE)
    A_blocked = A_reshaped.transpose(0, 2, 1, 3)
    return np.ascontiguousarray(A_blocked)
```
----------------ä»¥ä¸Šéè¯¾ç¨‹å†…å®¹----------------
## é€šè¿‡ im2col æ¥å®ç°å·ç§¯ Convolutions via im2col
åœ¨Lecture 10ä¸­æåˆ°ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨im2colæŠ€æœ¯ï¼Œå°†ä¸€ç»´å·ç§¯è¿ç®—è½¬æ¢ä¸ºçŸ©é˜µè¿ç®—ï¼š
{{< math_block >}}
\begin{bmatrix}z_1\\z_2\\z_3\\z_4\\z_5\end{bmatrix}=x*w=\begin{bmatrix}0&x_1&x_2\\x_1&x_2&x_3\\x_2&x_3&x_4\\x_3&x_4&x_5\\x_4&x_5&0\end{bmatrix}\begin{bmatrix}w_1\\w_2\\w_3\end{bmatrix}
{{< /math_block >}}
å¯¹äºäºŒç»´å·ç§¯æ¥è¯´ï¼ŒåŒæ ·ä¹Ÿæ˜¯å¯ä»¥çš„ã€‚ä»¥å·ç§¯æ ¸å¤§å°ä¸º3Ã—3ä¸ºä¾‹ï¼Œå¯¹6Ã—6çš„çŸ©é˜µè¿›è¡Œå·ç§¯ï¼Œå…¶ç»“æœçŸ©é˜µä¸º4Ã—4ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æ‰¾å‡ºæ¯æ¬¡è¿ç®—çš„æ„Ÿå—é‡ï¼Œå°†å…¶å•ç‹¬æ‹¿å‡ºæ¥ï¼Œé‚£ä¹ˆæ‰€æœ‰è¿™äº›æ„Ÿå—é‡å°±ç»„æˆäº†ä¸€ä¸ª4Ã—4Ã—3Ã—3çš„Tensorã€‚

å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œç¬¬\[0,0\]ä¸ªæ„Ÿå—é‡å°±æ˜¯\[0,1,2;6,7,8;12,13,14\]ã€‚æ€ä¹ˆå°†åŸå§‹çŸ©é˜µè½¬å˜ä¸ºTensorå‘¢ï¼Ÿè¿™é‡Œå°±å¯ä»¥ç”¨åˆ°ä¸ŠèŠ‚æåˆ°çš„`as_strided`æ–¹æ³•ã€‚strides\[0\]è¡¨ç¤ºåˆ°åŒåˆ—ä¸‹ä¸€ä¸ªæ„Ÿå—é‡çš„ç›¸åŒä½ç½®çš„å…ƒç´ çš„æ­¥é•¿ï¼Œä¸ºåˆ—é•¿6ï¼›strides\[1\]è¡¨ç¤ºåˆ°åŒè¡Œä¸‹ä¸€ä¸ªæ„Ÿå—é‡çš„æ­¥é•¿ï¼Œä¸º1ï¼›strides\[2\]è¡¨ç¤ºåŒä¸€ä¸ªæ„Ÿå—é‡å†…éƒ¨åŒåˆ—ä¸‹ä¸€ä¸ªå…ƒç´ çš„æ­¥é•¿ï¼Œä¸ºåŸå§‹åˆ—é•¿6ï¼›strides\[3\]è¡¨ç¤ºåŒä¸€ä¸ªæ„Ÿå—é‡å†…éƒ¨åŒè¡Œä¸‹ä¸€ä¸ªå…ƒç´ çš„æ­¥é•¿ï¼Œä¸º1ã€‚å³ï¼Œä½¿ç”¨`B = np.lib.stride_tricks.as_strided(A, shape=(4,4,3,3), strides=4*(np.array((6,1,6,1))))`å¯ä»¥å°†åŸå§‹å¾…å·ç§¯çŸ©é˜µAè½¬å˜ä¸ºæ„Ÿå—é‡å¼ é‡Bã€‚
![image.png](https://pics.zhouxin.space/202408300948305.png?x-oss-process=image/quality,q_90/format,webp)

ä¸‹ä¸€æ­¥ï¼Œé€šè¿‡reshapeæ“ä½œå°†å•ä¸ªæ„Ÿå—é‡å’Œå·ç§¯æ ¸éƒ½è½¬å˜ä¸ºå‘é‡ï¼Œé€šè¿‡å†…ç§¯è¿ç®—è®¡ç®—å·ç§¯å€¼ï¼š
```python
(B.reshape(16,9) @ W.reshape(9)).reshape(4,4)
```

éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒBçš„reshapeçš„æ“ä½œå¹¶ä¸æ˜¯freeçš„ï¼Œæ— æ³•é€šè¿‡åŸå§‹çš„Açš„æ•°æ®æ¥è¡¨ç¤ºreshapeåçš„Bï¼Œè¯¥reshapeæ“ä½œä¼šåˆ†é…å‡ºä¸€å—$O(K^2)$çš„å†…å­˜ç©ºé—´ï¼Œå½“Kæ¯”è¾ƒå¤§æ—¶ï¼Œè¿™ä¸ªæ“ä½œå°†ç›¸å½“è€—è´¹å†…å­˜ã€‚å› æ­¤ï¼Œåœ¨ç°ä»£ç‰ˆæœ¬ä¸­ï¼Œå¸¸å¸¸ä¼šä½¿ç”¨lazyæŠ€æœ¯æˆ–è€…å…¶å®ƒæŠ€æœ¯ï¼Œä½†è¿™ä¸åœ¨æœ¬è¯¾ç¨‹è®¨è®ºèŒƒå›´ä¹‹å†…ã€‚

## é€šè¿‡ im2col æ¥å®ç°å¤šé€šé“å·ç§¯
å¯¹äºå¤šé€šé“å¹¶ä¸”è€ƒè™‘batchçš„å·ç§¯ï¼Œå…¶è¾“å…¥shapeä¸ºNÃ—HÃ—WÃ—C_inï¼Œæ„Ÿå—é‡Tensorä¸ºNÃ—(W-K+1)Ã—(H-K+1)Ã—KÃ—KÃ—C_inï¼Œéœ€è¦å°†KÃ—KÃ—C_inå±•å¼€ä¸ºä¸€ç»´ï¼Œå·ç§¯æ ¸ä¹Ÿè¦å°†å¯¹åº”ä½ç½®å±•å¼€ï¼Œå³reshapeåshapeä¸º(KÃ—KÃ—C_in)Ã—C_outã€‚

ä»£ç å®ç°ä¸ºï¼š
```python
def conv_im2col(Z, weight):
    N,H,W,C_in = Z.shape
    K,_,_,C_out = weight.shape
    Ns, Hs, Ws, Cs = Z.strides
    
    inner_dim = K * K * C_in
    A = np.lib.stride_tricks.as_strided(Z, shape = (N, H-K+1, W-K+1, K, K, C_in),
                                        strides = (Ns, Hs, Ws, Hs, Ws, Cs)).reshape(-1,inner_dim)
    out = A @ weight.reshape(-1, C_out)
    return out.reshape(N,H-K+1,W-K+1,C_out)
```

# Lecture 15: Training Large Models
## å†…å­˜èŠ‚çœæŠ€æœ¯ Techniques for memory saving
ä¸€ç›´ä»¥æ¥ï¼ŒGPUçš„å…¨å±€å†…å­˜å¤§å°éƒ½æ˜¯æ¨¡å‹å¤§å°çš„åˆ¶çº¦ç“¶é¢ˆï¼Œé€šè¿‡ä¸€äº›å†…å­˜èŠ‚çœæŠ€æœ¯å¯ä»¥è®­ç»ƒæ›´å¤§çš„ä¸€äº›æ¨¡å‹ã€‚

æ¨¡å‹å†…å­˜æ¶ˆè€—ä¸»è¦æœ‰å¦‚ä¸‹å‡ ä¸ªæ–¹é¢ï¼šæ¨¡å‹æƒé‡ã€ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆåŠ¨é‡å€¼ç­‰ç­‰ï¼‰ã€ä¸­é—´æ¿€æ´»å±‚çš„å€¼ã€‚

å¯¹äºæ¨ç†æ¥è¯´ï¼Œä¿å­˜æ¿€æ´»å±‚çš„å†…å­˜åªéœ€è¦ä¸¤å—ï¼Œåˆ†åˆ«ç”¨æ¥ä¿å­˜ä¸€å±‚çš„è¾“å…¥å’Œè¾“å‡ºï¼Œä¸‹ä¸€å±‚çš„è¾“å…¥ä¸ºä¸Šä¸€å±‚çš„è¾“å‡ºï¼Œä¸‹ä¸€å±‚çš„è¾“å‡ºè¦†ç›–ä¸Šä¸€å±‚çš„è¾“å…¥ã€‚å…¶ä¸éœ€è¦ä¿å­˜ä¸­é—´æ¿€æ´»å±‚çš„å€¼ã€‚

è€Œåœ¨è®­ç»ƒä¸­ï¼Œç”±äºåœ¨è®¡ç®—æ¯ä¸€å±‚çš„æ¢¯åº¦æ—¶éƒ½ç”¨åˆ°äº†è¯¥å±‚çš„è¾“å…¥ï¼Œæ‰€æ¯ä¸€ä¸ªæ¿€æ´»å±‚éƒ½è¦ä¸€ç‰‡å†…å­˜ä¿å­˜ä¸‹æ¥ï¼Œå³æ¿€æ´»å±‚çš„å†…å­˜æ•°é‡ä¸º$O(N)$ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
![image.png](https://pics.zhouxin.space/202408302230612.png?x-oss-process=image/quality,q_90/format,webp)


ä¸€ç§å‡å°‘æ¿€æ´»å±‚å†…å­˜ä½¿ç”¨çš„æŠ€æœ¯å«åšcheckpointï¼Œå°±æ˜¯æ¯éš”ä¸€ä¸ªæ¿€æ´»å±‚æ‰ä¿å­˜è¯¥å±‚çš„å€¼ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
![image.png](https://pics.zhouxin.space/202408302240185.png?x-oss-process=image/quality,q_90/format,webp)
åœ¨åå‘ä¼ æ’­æ—¶ï¼Œå¦‚æœéœ€è¦ç”¨åˆ°æœªä¿å­˜çš„éšè—å±‚ï¼Œåˆ™é€šè¿‡ä¸Šä¸€ä¸ªéšè—å±‚è®¡ç®—å‡ºè¯¥å±‚çš„å€¼å³å¯ã€‚è¿™æ˜¯ä¸€ç§æ—¶é—´æ¢ç©ºé—´çš„æ€è·¯ã€‚å¯¹äºä¸€ä¸ªNå±‚çš„ç½‘ç»œï¼Œæ¯éš”Kä¸ªéšè—å±‚ä¿å­˜ä¸€æ¬¡ç»“æœï¼Œåˆ™éšè—å±‚å ç”¨çš„å†…å­˜ç©ºé—´å¤§å°ä¸º$O(N/K)+O(K)$ï¼Œå½“$K=\sqrt{N}$æ—¶å¯å–åˆ°æœ€å°å€¼ã€‚
## å¹¶è¡Œå’Œåˆ†å¸ƒå¼è®­ç»ƒ Parallel and distributed training
- è®¡ç®—å›¾åˆ’åˆ†

å½“æœ‰å¤šä¸ªGPUæ—¶ï¼Œå¯ä»¥è¿›è¡Œå¹¶è¡Œåˆ†å¸ƒå¼è®­ç»ƒã€‚ä¸€ç§æ€è·¯æ˜¯å°†è®¡ç®—å›¾è¿›è¡Œåˆ’åˆ†ï¼Œå¹¶åˆ†é…ç»™ä¸åŒçš„workerè¿›è¡Œæ‰§è¡Œï¼Œé€šè¿‡é€šè®¯åè®®åœ¨workerä¸­é—´ä¼ é€’æ•°æ®ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæ•´ä¸ªè®¡ç®—å›¾è¢«åˆ’åˆ†ä¸ºä¸¤éƒ¨åˆ†ã€‚
![image.png](https://pics.zhouxin.space/202408310842410.png?x-oss-process=image/quality,q_90/format,webp)
ä»…ä»…å°†è®¡ç®—å›¾åˆ’åˆ†å¹¶ä¸ä¼šå¸¦æ¥å¤šå°‘çš„å¹¶è¡Œæ€§ï¼Œä½†æ˜¯å½“worker1è®¡ç®—æ¥è‡ªworker0çš„æ•°æ®æ—¶ï¼Œworker0å¯ä»¥å¹¶è¡Œè®¡ç®—ä¸‹ä¸€ä¸ªminibatchçš„æ•°æ®ï¼Œä»è€Œå®ç°é«˜å¹¶è¡Œã€‚

- æ•°æ®å¹¶è¡Œè®­ç»ƒ

æ•°æ®å¹¶è¡Œè®­ç»ƒæ˜¯çš„æ˜¯å°†ä¸€ä¸ªminibatchåˆ†å‰²æˆæ›´å°çš„smaller batchï¼Œæ¯ä¸ªGPUè´Ÿè´£ä¸€ä¸ªsmaller batchçš„è®¡ç®—ï¼Œè¿™æ ·åšæ¯éš”GPUä¸Šéƒ½åœ¨è·‘ç›¸åŒçš„æ¨¡å‹ã€‚

åœ¨åˆ†å¸ƒå¼å’Œå¹¶è¡Œè®¡ç®—ä¸­ï¼Œæœ‰ä¸€ä¸ªallreduceåŸè¯­ï¼Œå…¶ä½œç”¨æ˜¯å°†åˆ†å¸ƒåœ¨å¤šä¸ªè¿›ç¨‹æˆ–èŠ‚ç‚¹ä¸Šçš„æ•°æ®è¿›è¡Œè§„çº¦ï¼ˆreductionï¼‰æ“ä½œï¼Œç„¶åå°†ç»“æœå¹¿æ’­å›æ‰€æœ‰å‚ä¸çš„è¿›ç¨‹æˆ–èŠ‚ç‚¹ã€‚è¿ç”¨è¿™ä¸ªåŸè¯­ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å¤šGPUä¸Šè®¡ç®—å‡ºsmaller batchçš„æ¢¯åº¦ï¼Œç„¶ååˆ©ç”¨è¯¥åŸè¯­å°†è®¡ç®—å‡ºæ•´ä¸ªminibatchçš„æ¢¯åº¦å¹¶è¿›è¡Œæ¢¯åº¦ä¸‹é™ã€‚

æˆ‘ä»¬è¿˜å¯ä»¥å°†å‚æ•°ä½¿ç”¨ä¸“é—¨çš„å‚æ•°æœåŠ¡å™¨ä¿å­˜ï¼Œå…¶å®ƒè®¾å¤‡éœ€è¦è®¿é—®æˆ–è€…æ›´æ–°å‚æ•°æ—¶ï¼Œåªéœ€è¦è°ƒç”¨ç›¸åº”APIå³å¯ã€‚å‚æ•°æœåŠ¡å™¨çš„å¥½å¤„æ˜¯å…¶ä¸éœ€è¦ç­‰å¾…æ‰€æœ‰çš„workeréƒ½è®¡ç®—ç»“æŸå†æ›´æ–°ï¼Œæ”¯æŒåŠ¨æ€å¢å‡workeræ•°é‡ï¼Œæé«˜äº†ç³»ç»Ÿçš„é²æ£’æ€§ã€‚

- é€šä¿¡è®¡ç®—é‡å  communication computation overlap

é€šä¿¡è®¡ç®—é‡å ï¼Œå°±æ˜¯æŒ‡åœ¨é€šä¿¡åŒæ­¥æ—¶ä½¿ç”¨éé˜»å¡çš„æ–¹å¼ï¼Œåœ¨ç­‰å¾…IOæ—¶ç»§ç»­è®¡ç®—ã€‚

# Lecture 16 Generative Adversarial Network

## ç”Ÿæˆå¯¹æŠ—è®­ç»ƒ Generative adversarial training
å¯¹äºæ— ç›‘ç£å­¦ä¹ ï¼Œæˆ–è€…ç§°ç”Ÿæˆå¼æ¨¡å‹ï¼Œå…¶ä»»åŠ¡æ˜¯é€šè¿‡éšæœºå‘é‡ç”Ÿæˆç¬¦åˆæ•°æ®é›†åˆ†å¸ƒçš„æ ·æœ¬ã€‚è¿™å°±å¼•å…¥äº†ä¸€ä¸ªé—®é¢˜ï¼šå¦‚ä½•è¯„ä¼°æ ·æœ¬å’Œç›®æ ‡åˆ†å¸ƒä¹‹é—´çš„è·ç¦»ã€‚è¿™ä¸€è¯„ä»·æŒ‡æ ‡ä½œä¸ºæˆ‘ä»¬çš„ç›®æ ‡å‡½æ•°ï¼Œå…¶å¿…é¡»æ˜¯å¯å¾®çš„ï¼Œä»¥ä¾¿åç»­å¯¹æ¨¡å‹è¿›è¡Œä¼˜åŒ–ã€‚

å¯¹æŠ—è®­ç»ƒçš„æ€è·¯æ˜¯æ„é€ ä¸€ä¸ªoracle classfier Dï¼Œå…¶ä½œç”¨æ˜¯è¾¨åˆ«ç”Ÿæˆæ•°æ®å’ŒåŸå§‹æ•°æ®ï¼ŒDçš„è¾“å‡ºæ˜¯è¾“å…¥ä¸ºç”Ÿæˆæ•°æ®ä¸ºç”Ÿæˆæ•°æ®çš„æ¦‚ç‡ã€‚é‚£å¯¹äºä»»æ„ä¸€ä¸ªè¾“å…¥zï¼Œç”Ÿæˆç½‘ç»œGçš„è¾“å‡ºä¸ºG(z)ï¼ŒDå¯¹å…¶çš„åˆ¤åˆ«ç»“æœä¸ºD(G(z))ã€‚é‚£ç”Ÿæˆå™¨çš„ç›®æ ‡å°±æ˜¯å°½å¯èƒ½è®©åˆ¤åˆ«å™¨åˆ¤åˆ«é”™è¯¯ï¼Œå³å…¶æŸå¤±å‡½æ•°ä¸ºï¼š
{{< math_block >}}
\max_G\{-E_{z\sim Noise}\log{(1-D(G(z)))}\}
{{< /math_block >}}

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™é‡Œå¹¶æ²¡æœ‰ç°æˆçš„è¾¨åˆ«å™¨Dã€‚æˆ‘ä»¬åŒæ ·å¯ä»¥ç”¨ä¸€ä¸ªç¥ç»ç½‘ç»œæ¥æ„é€ è¿™ä¸ªè¾¨åˆ«å™¨ï¼Œé‚£è¿™ä¸ªè¾¨åˆ«å™¨çš„ç›®æ ‡å°±æ˜¯å°½å¯èƒ½åˆ¤æ–­æ­£ç¡®ï¼Œå³å…¶æŸå¤±å‡½æ•°ä¸ºï¼š
{{< math_block >}}
\min_D\{-E_{z\sim Noise}\log{(1-D(G(z)))}-E_{x\sim Data}\log{D(x)}\}
{{< /math_block >}}


## å°†å¯¹æŠ—è®­ç»ƒä½œä¸ºæ·±åº¦å­¦ä¹ ä¸­çš„ä¸€ä¸ªæ¨¡å— Adversarial training as a module in deep learning models
æ¥ä¸‹æ¥æˆ‘ä»¬è€ƒè™‘å¦‚ä½•å°†å¯¹æŠ—æ¨¡å‹æ¨¡å—åŒ–ã€‚æˆ‘ä»¬å¯ä»¥å°†æ•´ä¸ªåˆ¤åˆ«å™¨ä½œä¸ºä¸€ä¸ªæŸå¤±å‡½æ•°æ¥å®ç°ï¼Œå½“ç„¶ï¼Œå…¶å’Œæˆ‘ä»¬ä¹‹å‰å®ç°çš„æŸå¤±å‡½æ•°æ˜¯ä¸ä¸€æ ·çš„ï¼Œåˆ¤åˆ«å™¨çš„å‚æ•°åœ¨æ¯è½®åå‘ä¼ æ’­æ—¶éƒ½è¦æ›´æ–°ã€‚

ã€è¿™ä¸€èŠ‚è¯¾ä¼¼ä¹æ²¡æœ‰å…·ä½“è¯´æ˜å¦‚ä½•æ¨¡å—åŒ–ï¼Œåè¾¹ä¼¼ä¹åœ¨ä»‹ç»GANç½‘ç»œçš„å„ä¸ªå˜ç§ã€‘

åœ¨DCGANä¸­ï¼Œä½¿ç”¨äº†ä¸€ç§è¢«ç§°ä¸ºåå·ç§¯ï¼ˆè½¬ç½®å·ç§¯ã€Conv2dTransposeï¼‰çš„æ¨¡å—ï¼Œå…¶ä½œç”¨æ˜¯è¿›è¡Œä¸Šé‡‡æ ·ã€‚

CycleGANæ˜¯ä¸€ä¸ªç”¨äºé£æ ¼è¿ç§»çš„æ¨¡å‹ã€‚å¯¹äºé£æ ¼è¿ç§»æ¨¡å‹æ¥è¯´ï¼Œä¸€ç§æœ‰ç›‘ç£çš„è®­ç»ƒæ€è·¯æ˜¯æ”¶é›†é£æ ¼è¿ç§»å‰åçš„å›¾ç‰‡é…å¯¹æ•°æ®é›†ï¼Œè¿›è¡Œæœ‰ç›‘ç£è®­ç»ƒã€‚ç„¶è€Œï¼Œæ­¤ç±»é…å¯¹æ•°æ®é›†æ˜¯å¾ˆéš¾è·å–çš„ã€‚å¦‚ä½•é€šè¿‡æœªé…å¯¹çš„æ•°æ®é›†è¿›è¡Œæ— ç›‘ç£è®­ç»ƒå‘¢ï¼Ÿå¯ä»¥ä½¿ç”¨GANç½‘ç»œï¼Œä¸€ä¸ªç”Ÿæˆå™¨Gç”¨äºå‡æˆé£æ ¼è¿ç§»åçš„å›¾ç‰‡ï¼Œä½¿ç”¨ä¸€ä¸ªåˆ¤åˆ«å™¨è¿›è¡Œå¯¹æŠ—è®­ç»ƒã€‚å¦å¤–æœ‰ä¸€ä¸ªç”Ÿæˆå™¨Fï¼Œç”¨äºè¿˜åŸå›¾ç‰‡ï¼Œå…¶ä¹Ÿä½¿ç”¨ä¸€ä¸ªåˆ¤åˆ«å™¨è¿›è¡Œå¯¹æŠ—è®­ç»ƒã€‚è€Œæ•´ä¸ªCycleGANæ¨¡å‹è¿˜éœ€è¦ä¿è¯å¾ªç¯ä¸€è‡´æ€§ï¼Œå³å°†æ•°æ®é›†ä¸­çš„ä¸€ä¸ªå›¾ç‰‡ç»è¿‡Gä¹‹åï¼Œå†ç»è¿‡Fï¼Œåº”å½“è¿˜åŸæˆåŸå§‹å›¾ç‰‡ï¼Œæ•…å¾ªç¯ä¸€è‡´æ€§çš„æŸå¤±å‡½æ•°å°±æ˜¯ä¸¤ä¸ªå›¾ç‰‡ä¹‹é—´çš„L2 Normã€‚

åœ¨ä¸‹èŠ‚è¯¾ä¸­ï¼Œå°†è®¨è®ºGANç³»åˆ—ç½‘ç»œçš„å…·ä½“å®ç°ã€‚

# Lecture 17: Generative Adversarial Networks implementations
æœ¬èŠ‚è¯¾ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ GANç½‘ç»œçš„å…·ä½“å®ç°ã€‚

åœ¨è¯¾ç¨‹ä¸­ï¼Œä½¿ç”¨äºŒç»´é«˜æ–¯åˆ†å¸ƒä½œä¸ºçœŸå®æ•°æ®é›†ï¼Œè®­ç»ƒä¸€ä¸ªç”Ÿæˆå™¨ç”¨äºå‡æˆè¯¥åˆ†å¸ƒçš„æ•°æ®ã€‚è®­ç»ƒé›†æ•°æ®å‡†å¤‡å¦‚ä¸‹ï¼š
```python
A = np.array([[1, 2], [-0.2, 0.5]])
mu = np.array([2, 1])
# total number of sample data to generated
num_sample = 3200
data = np.random.normal(0, 1, (num_sample, 2)) @ A + mu
```

ç”Ÿæˆå™¨ä½¿ç”¨ä¸€ä¸ªç®€å•çš„å…¨è¿æ¥å±‚å³å¯ï¼š
```python
model_G = nn.Sequential(nn.Linear(2, 2))

def sample_G(model_G, num_samples):
    Z = ndl.Tensor(np.random.normal(0, 1, (num_samples, 2)))
    fake_X = model_G(Z)
    return fake_X.numpy()
```

åˆ¤åˆ«å™¨æ˜¯ä¸€ä¸ªä¸‰å±‚çš„æ„ŸçŸ¥æœºï¼ŒæŸå¤±å‡½æ•°ä¸ºsoftmax lossï¼š
```python
model_D = nn.Sequential(
    nn.Linear(2, 20),
    nn.ReLU(),
    nn.Linear(20, 10),
    nn.ReLU(),
    nn.Linear(10, 2)
)
loss_D = nn.SoftmaxLoss()
```

ä¼˜åŒ–ç”Ÿæˆå™¨Gçš„è¿‡ç¨‹å°±æ˜¯ä½¿ç”¨Géšæœºç”Ÿæˆä¸€äº›æ•°æ®G(z)ï¼Œè®¡ç®—D(G(z))çš„è¾“å‡ºå’Œlabel 1ä¹‹é—´çš„æŸå¤±ï¼š
```python
opt_G = ndl.optim.Adam(model_G.parameters(), lr=0.01)ã€

def update_G(Z, model_G, model_D, loss_D, opt_G):
    fake_X = model_G(Z)
    fake_Y = model_D(fake_X)
    batch_size = Z.shape[0]
    ones = ndl.ones(batch_size, dtype="int32")
    loss = loss_D(fake_Y, ones)
    loss.backward()
    opt_G.step()
```

åŒæ ·ï¼Œåˆ¤åˆ«å™¨çš„æ›´æ–°è¿‡ç¨‹å°±æ˜¯è®¡ç®—D(x)å’Œlabel 1ä¹‹é—´çš„æŸå¤±ï¼ŒD(G(z))å’Œlabel 0ä¹‹é—´çš„æŸå¤±ï¼Œxæ˜¯çœŸå®æ•°æ®ï¼š
```python
opt_D = ndl.optim.Adam(model_D.parameters(), lr=0.01)

def update_D(X, Z, model_G, model_D, loss_D, opt_D):
    fake_X = model_G(Z).detach()
    fake_Y = model_D(fake_X)
    real_Y = model_D(X)
    assert X.shape[0] == Z.shape[0]
    batch_size = X.shape[0]
    ones = ndl.ones(batch_size, dtype="int32")
    zeros = ndl.zeros(batch_size, dtype="int32")
    loss = loss_D(real_Y, ones) + loss_D(fake_Y, zeros)
    loss.backward()
    opt_D.step()
```

è®­ç»ƒè¿‡ç¨‹åˆ™æ˜¯æ¯æ¬¡è¿­ä»£ä¸­ï¼Œå°†éšæœºå‘é‡é€å…¥ç”Ÿæˆå™¨ï¼Œå†å°†ç”Ÿæˆå™¨çš„è¾“å‡ºå–‚ç»™åˆ¤åˆ«å™¨ï¼Œç„¶ååˆ†åˆ«æ›´æ–°äºŒè€…çš„å‚æ•°å³å¯ï¼Œæ³¨æ„ä»¥ä¸‹ä»£ç ä¸­epochæŒ‡çš„æ˜¯è®­ç»ƒäº†å‡ ä¸ªbatchï¼Œè€Œä¸æ˜¯æŒ‡åœ¨è®­ç»ƒé›†ä¸Šå®Œæ•´è®­ç»ƒäº†å‡ è½®ï¼š
```python
def train_gan(data, batch_size, num_epochs):
    assert data.shape[0] % batch_size == 0
    data.astype(np.float32)
    for epoch in range(num_epochs):
        begin = (batch_size * epoch) % data.shape[0]
        X = data[begin: begin+batch_size, :]
        Z = np.random.normal(0, 1, (batch_size, 2))
        X = ndl.Tensor(X)
        Z = ndl.Tensor(Z)
        update_D(X, Z, model_G, model_D, loss_D, opt_D) 
        update_G(Z, model_G, model_D, loss_D, opt_G)

train_gan(data, 32, 2000)
```

ä»¥ä¸Šå°±æ˜¯è®­ç»ƒä¸€ä¸ªGANç½‘ç»œçš„å…¨è¿‡ç¨‹ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬è€ƒè™‘å¦‚ä½•GAN Lossæ¨¡å—åŒ–ã€‚GAN Lossçš„ä½œç”¨æ˜¯ç»™å®šä¸€ä¸ªç”Ÿæˆå™¨çš„è¾“å‡ºï¼Œè¿”å›ä¸€ä¸ªæŸå¤±å€¼ã€‚æ­¤å¤–ï¼Œå½“ç”Ÿæˆå™¨æ‹¿åˆ°æŸå¤±å€¼åå°±ä¼šç›´æ¥è¿›è¡Œç”Ÿæˆå™¨çš„å‚æ•°æ›´æ–°ï¼Œå› æ­¤GAN Losså†…éƒ¨å¿…é¡»éšå¼æ›´æ–°è‡ªèº«çš„å‚æ•°ï¼Œå³ï¼š
```python
class GANLoss:
    def __init__(self, model_D, opt_D):
        self.model_D = model_D
        self.opt_D = opt_D
        self.loss_D = nn.SoftmaxLoss()

    def _update_D(self, real_X, fake_X):
        real_Y = self.model_D(real_X)
        fake_Y = self.model_D(fake_X.detach())
        batch_size = real_X.shape[0]
        ones = ndl.ones(batch_size, dtype="float32")
        zeros = ndl.zeros(batch_size, dtype="float32")
        loss = self.loss_D(real_Y, ones) + self.loss_D(fake_Y, zeros)
        loss.backward()
        self.opt_D.step()

    def forward(self, fake_X, real_X):
        self._update_D(real_X, fake_X)
        fake_Y = self.model_D(fake_X)
        batch_size = real_X.shape[0]
        ones = ndl.ones(batch_size, dtype="float32")
        loss = self.loss_D(fake_Y, ones)
        return loss

```

# Lecture 18: Sequence Modeling and Recurrent Networks
## åºåˆ—å»ºæ¨¡ Sequence modeling
åœ¨å‰é¢çš„æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬éƒ½åšäº†ä¸€ä¸ªéšå¼å‡è®¾ï¼šxå’Œyä¹‹é—´æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒçš„ï¼Œä½†æ˜¯åœ¨å®è·µä¸­ï¼Œå¾ˆå¤šä»»åŠ¡çš„yéƒ½æ˜¯ä¸xç›¸å…³çš„ï¼Œå°¤å…¶æ˜¯å½“yæ˜¯ä¸€ä¸ªæ—¶é—´åºåˆ—æ•°æ®ã€‚

å¯¹äºåºåˆ—æ•°æ®æ¥è¯´ï¼Œæœ‰ä¸€ç±»é¢„æµ‹æ¨¡å‹æ˜¯è‡ªå›å½’æ¨¡å‹ï¼Œå…¶åŸºæœ¬æ€æƒ³æ˜¯åˆ©ç”¨åºåˆ—è‡ªèº«çš„å†å²å€¼æ¥é¢„æµ‹æœªæ¥å€¼ã€‚

## å¾ªç¯ç¥ç»ç½‘ç»œ Recurrent neural networks
å¾ªç¯ç½‘ç»œä¹Ÿèƒ½ç”¨äºè§£å†³åºåˆ—æ•°æ®çš„å»ºæ¨¡é—®é¢˜ã€‚RNNç½‘ç»œçš„æ€æƒ³æ˜¯æ„å»ºä¸€ä¸ªç½‘ç»œæ¨¡å‹ç”¨äºæ¨¡æ‹Ÿè¾“å…¥åºåˆ—ä¸­çš„æ—¶åºä¿¡æ¯ã€‚

å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œhè¡¨ç¤ºæ¨¡å‹ä¸­çš„éšè—å±‚ï¼Œéšè—å±‚çš„è¾“å…¥ä¸ºå‰ä¸€ä¸ªéšè—å±‚çš„çŠ¶æ€å’Œå½“å‰è¾“å…¥xï¼Œç»è¿‡éçº¿æ€§å˜æ¢åå¾—åˆ°è¯¥éšè—å±‚ï¼Œå½“å‰è¾“å…¥çš„å¯¹åº”è¾“å‡ºåˆ™ç”±å¯¹åº”éšè—å±‚ç»è¿‡éçº¿æ€§å˜æ¢åå¾—åˆ°ã€‚
å³ï¼š
{{< math_block >}}
\begin{align*}  
h_t &= f(h_{t-1},x_t)\\  
y_t &=g(h)  
\end{align*}
{{< /math_block >}}

![image.png](https://pics.zhouxin.space/202409041829344.png?x-oss-process=image/quality,q_90/format,webp)

ç†è®ºä¸Šæ¥è¯´ï¼Œå¦‚æœå»ºæ¨¡å¾—å½“ï¼Œè¿™ç§æ¨¡å¼åœ¨é¢„æµ‹$y_t$æ—¶å¯ä»¥è·å–å‰é¢æ‰€æœ‰æ—¶åˆ»çš„æ—¶åºä¿¡æ¯ã€‚

RNNçš„è®­ç»ƒæ—¶éœ€è¦é…å¯¹çš„xå’Œyä½œä¸ºæ•°æ®é›†ï¼ŒæŸå¤±å‡½æ•°ç”±æ¯ä¸€ä¸ªé¢„æµ‹å€¼å’ŒçœŸå®å€¼ä¹‹é—´çš„æŸå¤±ç´¯åŠ å¾—åˆ°ã€‚æ˜¾ç„¶ï¼Œè¿™ä¸ªæŸå¤±å‡½æ•°å¾ˆéš¾é€šè¿‡ç¬”çº¸è¿›è¡Œæ¨å¯¼ï¼Œä½†å¾—ç›Šäºæˆ‘ä»¬ä¹‹å‰æ„å»ºçš„è‡ªåŠ¨å¾®åˆ†ç³»ç»Ÿï¼Œæˆ‘ä»¬ä¸éœ€è¦æ‰‹åŠ¨è®¡ç®—ä»»ä½•æ¢¯åº¦ã€‚

å¯ä»¥å°†å¤šä¸ªRNNå †å åœ¨ä¸€èµ·ï¼Œå¾—åˆ°stacking RNNï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
![image.png](https://pics.zhouxin.space/202409041908220.png?x-oss-process=image/quality,q_90/format,webp)

RNNåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¾ˆå®¹æ˜“å‡ºç°æ¢¯åº¦/æ¿€æ´»å±‚çˆ†ç‚¸å’Œæ¢¯åº¦/æ¿€æ´»å±‚æ¶ˆå¤±é—®é¢˜ã€‚ä¹‹å‰çš„lectureæåˆ°ï¼Œå½“è®­ç»ƒå¾ˆæ·±çš„ç½‘ç»œæ—¶ï¼Œåˆå§‹åŒ–å‚æ•°æ˜¯å¾ˆé‡è¦çš„ã€‚åœ¨RNNä¸Šï¼Œè¿™ä¸ªé—®é¢˜æ›´åŠ ä¸¥é‡ï¼Œå› ä¸ºRNNçš„æ¨¡å‹é€šå¸¸å¾ˆæ·±å¾ˆæ·±ã€‚

ä¸€ä¸ªè§£å†³æ¢¯åº¦é—®é¢˜çš„æ–¹æ³•æ˜¯ç€çœ¼äºæ¿€æ´»å‡½æ•°ã€‚ReLUä½œä¸ºæ¿€æ´»å‡½æ•°å…¶ä¸€ä¸ªé—®é¢˜æ˜¯å…¶è¾“å‡ºå¯ä»¥æ— é™å¤§ã€‚ç„¶è€Œï¼Œå°†æ¿€æ´»å‡½æ•°ä¿®æ”¹ä¸ºæœ‰ç•Œå‡½æ•°ï¼Œä¾‹å¦‚sigmoidæˆ–è€…tanhå¹¶ä¸èƒ½è§£å†³è¿™ä¸€é—®é¢˜ï¼Œå°¤å…¶æ˜¯ï¼Œå…¶ä¸èƒ½è§£å†³æ¿€æ´»å±‚/æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚å¦‚ä¸‹æ‰€ç¤ºï¼Œå¯¹äºtanhï¼Œå½“xåœ¨0é™„è¿‘æ—¶ï¼Œå…¶è¾“å‡ºä»åœ¨0é™„è¿‘ï¼Œè¿™ä¼šå¯¼è‡´éšè—å±‚æ¶ˆå¤±ï¼›ä½†äºä¸¤ä¸ªå‡½æ•°ï¼Œå½“è¾“å…¥åœ¨-5å’Œ5é™„è¿‘æ—¶ï¼Œå…¶æ¢¯åº¦å¾ˆå°ï¼Œè¿™ä¼šå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ã€‚

![image.png](https://pics.zhouxin.space/202409041923270.png?x-oss-process=image/quality,q_90/format,webp)

## LSTMs
LSTMä¸€å®šç¨‹åº¦ä¸Šå‡è½»äº†RNNä¸­å­˜åœ¨çš„æ¢¯åº¦æ¶ˆå¤±å’Œçˆ†ç‚¸é—®é¢˜ã€‚LSTMåœ¨åŸç‰ˆRNNçš„åŸºç¡€ä¸Šå¯¹éšè—å±‚è¿›è¡Œäº†ä¸€å®šæ”¹è¿›ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼ŒLSTMå°†åŸå§‹hidden stateåˆ†è£‚ä¸ºä¸¤ä¸ªç»„ä»¶hidden stateå’Œcell stateã€‚
![image.png](https://pics.zhouxin.space/202409041935116.png?x-oss-process=image/quality,q_90/format,webp)

å…¶æ¬¡ï¼ŒLSTMä¸­å…·ä½“å®šä¹‰äº†hidden stateå’Œcell stateçš„å…·ä½“æ›´æ–°å…¬å¼ã€‚LSTMä¸­å®šä¹‰äº†ä¸€äº›ä¸­é—´å˜é‡ç”¨äºæ›´ç®€æ´åœ°æè¿°è¿™ä¸€å…¬å¼ï¼Œä¸­é—´å˜é‡æœ‰forget gateã€input gateã€output gateï¼Œè¿˜æœ‰ä¸€ä¸ªå€™é€‰çŠ¶æ€g_tã€‚è¿™äº›ä¸­é—´å˜é‡å’ŒçŠ¶æ€çš„æ›´æ–°å…¬å¼å¦‚ä¸‹æ‰€ç¤ºï¼š

{{< math_block >}}
\begin{align*}  
&\begin{bmatrix}i_t\\f_t\\g_t\\o_t\end{bmatrix}=\begin{pmatrix}\text{sigmoid}\\\text{sigmoid}\\\text{tanh}\\\text{sigmoid}\end{pmatrix}(W_{hh}h_{t-1}+\text{W}_{hx}x_t+b_h) \\  
&c_t=c_{t-1}\circ f_t+i_t\circ g_t \\  
&h_t=\tanh(c_t)\circ o_t\\  
&i_t,f_t,g_t,o_t,c_t,h_t \in \mathbb{R}^d\\  
&W_{hh},W_{hx}\in \mathbb{R}^{4d\times d}  
\end{align*}
{{< /math_block >}}

$W_{hh},W_{hx}\in \mathbb{R}^{4d\times d}$æ„å‘³ç€ï¼Œè®¡ç®—ä¸­é—´å˜é‡çš„æƒé‡å½¼æ­¤éƒ½æ˜¯ç‹¬ç«‹çš„ã€‚

	ï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼ï¼ï¼è¿™å…¬å¼æ€ä¹ˆæ¥çš„ï¼Œä¸ºå•¥å­è¿™ä¸ªå…¬å¼ç®¡ç”¨ï¼Ÿæœ‰å¾ˆå¤šå·¥ä½œè¯•å›¾å¯¹æ­¤è¿›è¡Œè§£é‡Šï¼Œä½†å¤§å¤šæ˜¯ä¸€å®¶ä¹‹è¨€ã€‚Zico Kolteræ•™æˆå¯¹æ­¤çš„è§£é‡Šæ˜¯ï¼š$g_t$åœ¨ç»è¿‡sigmoidä»¥åæ˜¯ä¸€ä¸ª0-1å˜é‡ï¼Œç”¨äºå†³å®šæ˜¯å¦è¦ä¿ç•™å‰ä¸€çŠ¶æ€å¯¹åº”ä½ç½®çš„cell stateä¿¡æ¯ï¼Œ$i_t$åŒæ ·æ˜¯ä¸ª0-1å˜é‡ï¼Œè€Œ$g_t$æ˜¯ä¸ªæœ‰ç•Œé¡¹ï¼Œè¿™ä¸€ç»„åˆå†³å®šäº†æ˜¯å¦è¦åœ¨cell stateçš„ä½ç½®ä¸Šæ·»åŠ ä¸€äº›é¢å¤–çš„ä¿¡æ¯ï¼›$h_t$çš„æ›´æ–°å…¬å¼åˆ™æ˜¯ä¸€ä¸ªæœ‰ç•Œå˜é‡ï¼Œå…¶ä½œç”¨æ˜¯é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸æˆ–è€…æ¶ˆå¤±ã€‚
## Beyond "simple" sequential models
é™¤äº†å¯¹åºåˆ—æ•°æ®è¿›è¡Œå»ºæ¨¡ï¼ŒRNNèƒ½åšçš„è¿˜æœ‰å¾ˆå¤šã€‚ä¾‹å¦‚ï¼Œç¿»è¯‘å¥å­ï¼Œæœ‰ä¸€ç§sequence to sequenceæ¶æ„é‡‡ç”¨äº†ä¸¤ä¸ªRNNæ¨¡å‹ï¼Œä¸€ä¸ªç”¨äºè¾“å…¥åŸå§‹å¥å­ï¼Œæå–ä¸­é—´çŠ¶æ€ï¼Œå¦ä¸€ä¸ªç”¨äºæ ¹æ®æœ€åä¸€ä¸ªä¸­é—´çŠ¶æ€ï¼Œè¾“å‡ºç¿»è¯‘åçš„å¥å­ã€‚
![image.png](https://pics.zhouxin.space/202409042233558.png?x-oss-process=image/quality,q_90/format,webp)


è¿™æ„å‘³ç€ï¼ŒRNNå¯ä»¥ä½œä¸ºä¸€ä¸ªencoderå¯¹è¯­ä¹‰ä¿¡æ¯è¿›è¡Œæå–å’Œç¼–ç ï¼Œä¹Ÿå¯ä»¥ä½œä¸ºdecoderå¯¹è¯­ä¹‰ä¿¡æ¯è¿›è¡Œè§£ç ã€‚

RNNæœ‰ä¸€ç§å˜ä½“æ˜¯åŒå‘RNNï¼Œå…¶ä½œç”¨æ˜¯$x_i$æ—¶åˆ»çš„è¾“å‡ºä¸å‰åéƒ½ç›¸å…³ï¼Œåœ¨ä¸€äº›ä»»åŠ¡ï¼Œä¾‹å¦‚å®Œå½¢å¡«ç©ºä¸­å¯ä»¥æœ‰è¾ƒå¥½çš„è¡¨ç°ã€‚

# Lecture 19: LSTM Implementation
## LSTM cell
æœ¬èŠ‚è¯¾ï¼Œæˆ‘ä»¬å°†åœ¨NumPyå®ç°LSTMã€‚é¦–å…ˆæ¥å®ç°LSTM cellï¼Œä¸€ä¸ªcellæ˜¯hidden stateå’Œcell stateçš„é›†åˆï¼Œå…¶çŠ¶æ€æ›´æ–°å…¬å¼ä¸ºï¼š
{{< math_block >}}
\begin{align*} \\  
        i_t &= \sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\  
        f_t &= \sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\  
        g_t &= \tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\  
        o_t &= \sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\  
        c_t &= f_t \odot c_{t-1} + i_t \odot g_t \\  
        h_t &= o_t \odot \tanh(c_t) \\  
    \end{align*}
{{< /math_block >}}

ä¸Šè¿°å…¬å¼åœ¨ä¸ŠèŠ‚è¯¾ä¸­ï¼Œå¯ä»¥è®°ä¸ºçŸ©é˜µçš„å½¢å¼ï¼Œå³ï¼š
{{< math_block >}}
\begin{align*}  
&\begin{bmatrix}i_t\\f_t\\g_t\\o_t\end{bmatrix}=\begin{pmatrix}\text{sigmoid}\\\text{sigmoid}\\\text{tanh}\\\text{sigmoid}\end{pmatrix}(W_{hh}h_{t-1}+\text{W}_{hx}x_t+b_h) \\  
&c_t=c_{t-1}\circ f_t+i_t\circ g_t \\  
&h_t=\tanh(c_t)\circ o_t\\  
&i_t,f_t,g_t,o_t,c_t,h_t \in \mathbb{R}^d\\  
&W_{hh},W_{hx}\in \mathbb{R}^{4d\times d}  
\end{align*}
{{< /math_block >}}


åœ¨PyTorchä¸­ï¼Œå·²ç»æœ‰LSTMçš„å…·ä½“å®ç°ï¼Œå½“æˆ‘ä»¬å®ä¾‹åŒ–ä¸€ä¸ª$20\times100$çš„cellï¼Œå³è¾“å…¥å‘é‡é•¿åº¦ä¸º20ï¼Œä¸­é—´çŠ¶æ€ç‰¹å¾é•¿åº¦ä¸º100ï¼Œé‚£$W_{hh}$å’Œ$W_{hx}$çš„å½¢çŠ¶å°±æ˜¯$400\times 100$å’Œ$400\times 20$ã€‚

æ ¹æ®ä¸Šè¿°æ›´æ–°å…¬å¼ï¼Œå¯ä»¥å¾—åˆ°è®¡ç®—ä¸€ä¸ªLSTM cell çš„æ–¹æ³•ï¼š
```python
def sigmoid(x):
    return 1/(1+np.exp(-x))

def lstm_cell(x, h, c, W_hh, W_ih, b):
    i,f,g,o = np.split(W_ih@x + W_hh@h + b, 4)
    i,f,g,o = sigmoid(i), sigmoid(f), np.tanh(g), sigmoid(o)
    c_out = f*c + i*g, 
    h_out = o * np.tanh(c_out)
    return h_out, c_out
```

## Full sequence LSTM
åŸºäºPyTorchçš„ä¼ ç»Ÿï¼Œåœ¨å®ç°LSTMæ—¶ï¼Œè¿”å›æ‰€æœ‰çš„hidden stateä»¥åŠæœ€åä¸€ä¸ªcell stateã€‚å‰é¢æ²¡æœ‰æåˆ°ï¼ŒLSTMä¸­å„ä¸ªcellçš„å‚æ•°çš„æƒé‡æ˜¯å…±äº«çš„ã€‚é‚£LSTMå®é™…ä¸Šå°±æ˜¯æ ¹æ®åºåˆ—çš„é•¿åº¦é‡å¤æ‰§è¡Œ`lstm_cell`å³å¯ï¼š
```python
def lstm(X, h, c, W_hh, W_ih, b):
    H = np.zeros((X.shape[0], h.shape[0]))
    for t in range(X.shape[0]):
        h, c = lstm_cell(X[t], h, c, W_hh, W_ih, b)
        H[t,:] = h
    return H, c
```

## Batching efficiently
æ¥ä¸‹æ¥æˆ‘ä»¬è€ƒè™‘å¦‚ä½•å®ç°batch LSTMï¼Œä¸€ç§ç¬¦åˆä¹ æƒ¯çš„åšæ³•æ˜¯å°†batchä½œä¸ºç¬¬ä¸€ä¸ªç»´åº¦å°†è¾“å…¥Xå †å èµ·æ¥ï¼Œå³`X[NUM_BATCHES][NUM_TIMESTEPS][INPUT_SIZE]`ï¼Œè¿™ç§æ ¼å¼è¢«ç§°ä¸ºNTCæ ¼å¼ã€‚å¦‚æœé‡‡ç”¨æ”¹æ ¼å¼ï¼Œé‚£ä¹ˆåœ¨ç»è¿‡lstmæ—¶ï¼Œç¬¬iä¸ªcellè®¿é—®çš„å…ƒç´ ä¸º`X[:,i,:]`ï¼Œæ³¨æ„ï¼Œè¿™äº›å…ƒç´ åœ¨å†…å­˜ä¸­ä¸æ˜¯ç´§å¯†æ’åˆ—çš„ï¼Œcacheå‘½ä¸­ç‡è¾ƒä½ã€‚

å¦‚æœå°†æ—¶é—´ç»´åº¦æ”¾åœ¨ç¬¬ä¸€ä¸ªï¼Œå³é‡‡ç”¨TNCæ ¼å¼ï¼Œåˆ™èƒ½å¤Ÿè§£å†³è¯¥é—®é¢˜ã€‚

å…¶ä½™ä»£ç å‡ ä¹ä¸éœ€è¦æ”¹åŠ¨ï¼ŒçŸ©é˜µä¹˜æ³•æ—¶è¦æ³¨æ„å°†ä¸‰ç»´çš„Xæ”¾åˆ°@è¿ç®—ç¬¦å‰é¢ï¼š
```python
def lstm_cell(x, h, c, W_hh, W_ih, b):
    i,f,g,o = np.split(x@W_ih + h@W_hh + b[None,:], 4, axis=1)
    i,f,g,o = sigmoid(i), sigmoid(f), np.tanh(g), sigmoid(o)
    c_out = f*c + i*g
    h_out = o * np.tanh(c_out)
    return h_out, c_out

def lstm(X, h, c, W_hh, W_ih, b):
    H = np.zeros((X.shape[0], X.shape[1], h.shape[1]))
    for t in range(X.shape[0]):
        h, c = lstm_cell(X[t], h, c, W_hh, W_ih, b)
        H[t,:,:] = h
    return H, c
```

## Training LSTMs
è®­ç»ƒä¸€ä¸ªå•å±‚LSTMå¾ˆç®€å•ï¼Œä¸èµ˜è¿°ï¼Œç›´æ¥çœ‹ä»£ç ï¼š
```python
def train_lstm(X, Y, h0, c0, parameters)
    H, cn = lstm(X, h0, c0, parameters)
    l = loss(H, Y)
    l.backward()
    opt.step()
```

è®­ç»ƒä¸€ä¸ªå¤šå±‚LSTMä¹Ÿä¸éš¾ï¼Œå¯ä»¥é€‰æ‹©å…ˆåœ¨æ·±åº¦æˆ–è€…æ—¶é—´ç»´åº¦ä¸Šæ­£å‘ä¼ æ’­ï¼Œå†åœ¨å¦ä¸€ä¸ªç»´åº¦ä¸Šæ­£å‘ä¼ æ’­ã€‚ç¤ºä¾‹ä»£ç é‡‡ç”¨å…ˆæ—¶é—´å†æ·±åº¦çš„å½¢å¼ï¼š
```python
def train_lstm(X, Y, h0, c0, parameters)
    H = X
    for i in range(depth):
        H, cn = lstm(H, h0[i], c0[i], parameters[i])
    l = loss(H, Y)
    l.backward()
    opt.step()
```

æ¥ä¸‹æ¥é‡å¤´æˆæ¥äº†ã€‚å¦‚æœæˆ‘ä»¬çš„åºåˆ—é•¿åº¦å¾ˆé•¿ï¼Œé‚£ä¹ˆè¿›è¡Œä¸€æ¬¡æ­£å‘ä¼ æ’­éœ€è¦ä¿å­˜çš„ä¸­é—´å˜é‡å°±å¾ˆå¤šå¾ˆå¤šï¼Œæ˜¾å­˜å¯èƒ½ä¸å¤Ÿï¼Œæ€ä¹ˆè§£å†³è¿™ä¸ªé—®é¢˜ï¼Ÿ

æˆ‘ä»¬å¯ä»¥æŠŠè¿™ä¸ªåºåˆ—æŒ‰ç…§æŸä¸ªå›ºå®šé•¿åº¦è¿›è¡Œæˆªæ–­ï¼Œé¦–å…ˆè®¡ç®—ç¬¬ä¸€æ®µä¸­çš„lossï¼Œå¹¶è¿›è¡Œåå‘ä¼ æ’­ï¼Œç„¶åå¯¹åä¸€æ®µç»§ç»­è¿›è¡Œæ­£å‘ä¼ æ’­ï¼ŒåŒæ—¶å°†ç¬¬ä¸€æ®µçš„æœ€åä¸€ä¸ªcell stateä½œä¸ºç¬¬äºŒæ®µçš„åˆå§‹stateä¼ å…¥ï¼Œç„¶ååå‘ä¼ æ’­...

ä¸€ç›´ç­‰åˆ°æ•´ä¸ªåºåˆ—å¤„ç†å®Œæ¯•ï¼Œå†æ›´æ–°å‚æ•°ã€‚ç†è§£è¿™ä¸ªè¿‡ç¨‹åï¼Œä¸éš¾å‘ç°ï¼Œé˜¶æ®µç‰ˆæœ¬å’Œå®Œæ•´ç‰ˆæœ¬æ˜¯å®Œå…¨ç­‰ä»·çš„ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆlstméœ€è¦è¿”å›æœ€åä¸€ä¸ªcell stateã€‚ä¸Šè¿°è¿‡ç¨‹å¯æè¿°ä¸ºï¼š
```python
def train_lstm(X, Y, h0, c0, parameters)
    H, cn = lstm(X, h0, c0, parameters)
    l = loss(H, Y)
    l.backward()
    opt.step()
    return H[-1].data, cn.data

h0, c0 = zeros()
for i in range(0,X.shape[0],BLOCK_SIZE):
    h0, c0 = train_lstm(X[i:i+BLOCK_SIZE], Y[i:i+BLOCK_SIZE], h0, c0, parameters)
```

# Lecture 20: Transformers and Attention

## ä¸¤ç§ä¸ºæ—¶é—´åºåˆ—å»ºæ¨¡çš„æ–¹æ³• The two approaches to time series modeling
RNNåœ¨å¯¹æ—¶é—´åºåˆ—å»ºæ¨¡æ—¶é‡‡ç”¨äº†ä¸€ç§è¢«ç§°ä¸ºæ½œåœ¨çŠ¶æ€latent stateçš„æ–¹å¼ï¼Œå…·ä½“æ¥è¯´ï¼Œå…¶ä½¿ç”¨tæ—¶åˆ»çš„hidden stateæ¥æè¿°tåŠtæ—¶åˆ»å¾€å‰çš„æ‰€æœ‰ä¿¡æ¯ã€‚è¿™ç§æ–¹æ³•çš„ä¼˜ç‚¹æ˜¯å…¶ç†è®ºä¸Šå¯ä»¥èšåˆæ— é™é•¿æ—¶åˆ»çš„ä¿¡æ¯ï¼Œç¼ºç‚¹æ˜¯å…¶éš¾ä»¥æœ‰æ•ˆè®°ä½è¾ƒè¿œæ—¶åˆ»çš„ä¿¡æ¯ï¼Œå¹¶ä¸”å­˜åœ¨æ¢¯åº¦çˆ†ç‚¸å’Œæ¶ˆå¤±é—®é¢˜ã€‚

è€Œå¦ä¸€ç§å»ºæ¨¡æ–¹å¼è¢«ç§°ä¸ºç›´æ¥é¢„æµ‹ direct predictionï¼Œå…·ä½“æ¥è¯´ï¼Œç›´æ¥ä½¿ç”¨tå’Œtæ—¶åˆ»ä¹‹å‰çš„sequenceæ¥é¢„æµ‹tæ—¶åˆ»çš„è¾“å‡ºã€‚è¿™ç§æ–¹å¼çš„ä¼˜ç‚¹æ—¶ï¼Œå¯¹äºå¤§éƒ¨åˆ†è¾“å‡ºï¼Œå…¶è®¡ç®—è·¯å¾„è¦çŸ­çš„ï¼Œç¼ºç‚¹æ˜¯æ²¡æœ‰æ˜ç¡®çš„çŠ¶æ€è¡¨ç¤ºï¼Œåœ¨å®è·µä¸­å¾€å¾€åºåˆ—é•¿åº¦æœ‰é™ã€‚Transformerå°±å±äºè¿™ç§ç›´æ¥é¢„æµ‹æ–¹å¼å¯¹æ—¶é—´åºåˆ—è¿›è¡Œå»ºæ¨¡ã€‚

ã€æ­¤å¤„è·³è¿‡å¯¹CNNç”¨äºæ—¶é—´åºåˆ—å»ºæ¨¡åŠå…¶ä¼˜ç¼ºç‚¹çš„ä»‹ç»ã€‘

## è‡ªæ³¨æ„åŠ›æœºåˆ¶å’ŒTransformer Self-attention and transformers
Attentionæœºåˆ¶æœ¬è´¨ä¸ŠæŒ‡çš„æ˜¯ä»»ä½•å¯¹çŠ¶æ€è¿›è¡ŒåŠ æƒæ±‚å’Œçš„æœºåˆ¶ï¼Œè¿™ä¸ªæƒé‡æ˜¾ç„¶ä¸åº”è¯¥ç”±æˆ‘ä»¬è‡ªå·±å†³å®šï¼Œè€Œæ˜¯å¯å­¦ä¹ çš„å‚æ•°ï¼Œå†ç»è¿‡ä¸€å±‚softmaxåå¾—åˆ°çš„æƒé‡ã€‚

è€Œè‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œé¡¾åæ€ä¹‰ï¼Œå°±æ˜¯ç”±çŠ¶æ€è‡ªå·±æ¥å†³å®šæƒé‡ï¼Œç„¶åå¯¹çŠ¶æ€æŒ‰æƒé‡æ±‚å’Œçš„æœºåˆ¶ã€‚

åœ¨è‡ªæ³¨æ„åŠ›ä¸­ï¼ŒKQVæ˜¯ä¸‰ä¸ªshapeç›¸åŒçš„çŸ©é˜µï¼Œå³$K,Q,V\in \mathbb{R}^{T\times d}$ ï¼ŒKQVéƒ½æ˜¯ç”±è¾“å…¥$X$ä¹˜ä¸Šä¸åŒçš„æƒé‡å¾—åˆ°çš„$W_K W_Q W_V$å¾—åˆ°ï¼Œself-attentionç®—å­çš„å®šä¹‰ä¸ºï¼š
{{< math_block >}}
\text{SelfAttention}(K,Q,V) = \text{softmax}(\frac{KQ^T}{\sqrt{d}})V
{{< /math_block >}}
å…¶ä¸­ï¼Œsoftmaxæ“ä½œæ˜¯å¯¹æ¯ä¸€è¡Œè¿›è¡Œçš„ã€‚

æ¥ä¸‹æ¥æˆ‘ä»¬å°è¯•ç†è§£è¿™ä¸ªå¼å­åœ¨åšä»€ä¹ˆã€‚é¦–å…ˆæˆ‘ä»¬è¦æ˜ç¡®ï¼ŒKQVçš„æ¯ä¸€è¡Œéƒ½æ˜¯ç”±Xå¯¹åº”è¡ŒåŠ æƒæ±‚å’Œå¾—åˆ°çš„ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼ŒKQVæ¯ä¸€è¡Œå¹¶æ²¡æœ‰å…¶å®ƒè¡Œçš„æ—¶åºä¿¡æ¯ï¼ˆXçš„æ¯ä¸€è¡Œè¡¨ç¤ºä¸€ä¸ªæ—¶é—´çš„è¾“å…¥ï¼‰ã€‚$KQ^T$æ˜¯ä¸€ä¸ªTÃ—Tçš„çŸ©é˜µï¼Œå…¶ç¬¬iè¡Œç¬¬jä¸ªå…ƒç´ æ˜¯ç”±Kçš„ç¬¬iè¡Œå’Œjçš„ç¬¬iåˆ—ä½œå†…ç§¯å¾—åˆ°ï¼Œåœ¨è¿™é‡Œï¼Œæ—¶åºä¿¡æ¯è¿›è¡Œäº†äº¤æ¢ã€‚å¯¹äº$KQ^T$çš„ç¬¬iè¡Œï¼Œå…¶æ¯ä¸ªå…ƒç´ çš„å€¼å¤§å°åœ¨ä¸€å®šç¨‹åº¦ä¸Šååº”äº†Qä¸­æ¯ä¸€åˆ—ä¸ä¹‹çš„ç›¸ä¼¼åº¦ï¼Œç„¶åå¯¹è¿™ä¸€è¡Œè¿›è¡Œäº†softmaxæ“ä½œï¼Œå¾—åˆ°æƒé‡ã€‚æ¥ä¸‹æ¥å°†è¿™ä¸ªæƒé‡çŸ©é˜µä¹˜ä¸ŠVï¼Œå¾—åˆ°è‡ªæ³¨æ„åŠ›çš„å€¼ã€‚æœ€åå¾—åˆ°çš„ç»“æœçŸ©é˜µä¸­ï¼Œæ¯ä¸€è¡Œç»“æœéƒ½æ˜¯æ ¹æ®æƒé‡çŸ©é˜µå¯¹Vè¿›è¡ŒåŠ æƒæ±‚å’Œå¾—åˆ°çš„ï¼Œä¹Ÿæ­£æ˜¯åœ¨è¿™é‡Œï¼Œå‘ç”Ÿäº†æ—¶åºä¿¡æ¯çš„æ··åˆã€‚

è‡ªæ³¨æ„åŠ›æœ‰å¦‚ä¸‹å‡ ä¸ªç‰¹ç‚¹
- å¯¹KQVçš„æ’åˆ—å…·æœ‰ä¸å˜æ€§ï¼ˆå®é™…ä¸Šæ˜¯ç­‰ä»·æ€§ï¼‰ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå¦‚æœæŒ‰è¡Œé‡æ’KQVï¼Œè‡ªæ³¨æ„åŠ›çš„ç»“æœä¸ä¼šå› æ­¤æ”¹å˜ï¼Œåªä¼šå› æ­¤å‘ç”Ÿå¯¹åº”çš„é‡æ’ã€‚
- è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¼šåœ¨æ‰€æœ‰çš„æ—¶é—´æ­¥ä¸Šèµ·ä½œç”¨ï¼Œä¹Ÿå°±æ˜¯è‡ªæ³¨æ„åŠ›å¯ä»¥æ··åˆæ—¶åºä¿¡æ¯ã€‚
- è®¡ç®—å¼€é”€ä¸º$O(T^2d)$ã€‚

ä¸€ä¸ªTransformer Blockç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
![image.png](https://pics.zhouxin.space/202409080919833.png?x-oss-process=image/quality,q_90/format,webp)

è¿™ä¸ªæµç¨‹ç”¨å…¬å¼è¡¨ç¤ºä¸ºï¼š
{{< math_block >}}
\begin{align*}  
\tilde{Z} &:= \text{SelfAttention}\big(Z^{(i)}W_K,Z^{(i)}W_Q,Z^{(i)}W_V\big) \\  
&= \mathrm{softmax}\left(\frac{Z^{(i)}W_KW_V^T(Z^{(i)})^T}{d^{1/2}}\right)Z^{(i)}W_V \\  
\tilde{Z} &:= \text{LayerNorm}\bigg(Z^{(i)} \boldsymbol{+}\tilde{Z}\bigg) \\  
Z^{(i+1)} &:= \text{LayerNorm}(\mathrm{ReLU}(\tilde{Z}W)+\tilde{Z})  
\end{align*}
{{< /math_block >}}

Transformerçš„ä¼˜ç‚¹æ˜¯ï¼š
- å¯ä»¥åœ¨ä¸€ä¸ªblockä¸­æ··åˆæ‰€æœ‰æ—¶é—´æ­¥çš„æ—¶åºä¿¡æ¯ï¼›
- éšç€æ—¶é—´æ­¥çš„å¢åŠ ï¼ŒTransformerä¸éœ€è¦é¢å¤–å¼•å…¥æ–°çš„å‚æ•°ã€‚

å…¶ç¼ºç‚¹æ˜¯ï¼š
- æ¯ä¸ªè¾“å‡ºéƒ½ä¾èµ–äºæ‰€æœ‰æ—¶é—´æ­¥çš„è¾“å…¥ï¼›
- è¾“å…¥æ²¡æœ‰æ—¶åºï¼Œä¹Ÿå°±æ˜¯è¯´å¯ä»¥å°†æ—¶åºæ‰“ä¹±å†è¾“å…¥ç»™Transformerï¼Œç»“æœè¿˜æ˜¯ä¸€æ ·çš„ã€‚

æ¥ä¸‹æ¥ä»‹ç»ä¸¤ç§æŠ€æœ¯é’ˆå¯¹ç¼ºç‚¹è¿›è¡Œæ”¹è¿›ã€‚

é¦–å…ˆæ˜¯æ©ç è‡ªæ³¨æ„åŠ›ï¼Œå³masked self-attentionã€‚ä¹‹å‰æåˆ°ï¼Œåœ¨è‡ªæ³¨æ„åŠ›çš„è®¡ç®—å…¬å¼ä¸­ï¼Œsoftmaxåçš„$KQ^T$æ˜¯ä¸€ä¸ªå¯†é›†çŸ©é˜µï¼Œæ¯ä¸€è¡Œéƒ½æ˜¯è¡¨ç¤ºä¸€ä¸ªæƒé‡ï¼Œä¼šå°†æ‰€æœ‰æ—¶åˆ»çš„çŠ¶æ€åŠ æƒæ±‚å’Œã€‚è€Œæ©ç è‡ªæ³¨æ„åŠ›çš„åšæ³•æ˜¯ï¼Œå°†è®©$KQ^T$çš„ä¸Šä¸‰è§’éƒ¨åˆ†å‡å»æ— ç©·å¤§ï¼Œè¿™æ ·æƒé‡çŸ©é˜µçš„ä¸Šä¸‰è§’éƒ¨åˆ†ä¸º0ï¼Œå³åªå¯¹tä¹‹å‰çš„æ—¶åˆ»åŠ æƒæ±‚å’Œï¼Œä»¥é˜²æ­¢è·å–æœªæ¥ä¿¡æ¯ã€‚

ä¸ºäº†è§£å†³è¾“å…¥æ—¶åºçš„é—®é¢˜ï¼Œå¼•å…¥äº†ä½ç½®ç¼–ç position encodingæŠ€æœ¯ï¼Œç»™è¾“å…¥åŠ ä¸Šä¸€ä¸ªç”¨äºè¡¨ç¤ºæ—¶é—´ä¿¡æ¯çš„çŸ©é˜µï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
{{< math_block >}}
X\in\mathbb{R}^n= \begin{bmatrix}-&x_1^\top&-\\-&x_2^\top&-\\&\vdots&\\-&x_T^\top&-\end{bmatrix}+\begin{bmatrix}\sin(\omega_1\cdot1)&\cdots&\sin(\omega_n\cdot1)\\\sin(\omega_1\cdot2)&\cdots&\sin(\omega_n\cdot2)\\\vdots&\ddots&\vdots\\\sin(\omega_1\cdot T)&\cdots&\sin(\omega_n\cdot T)\end{bmatrix}
{{< /math_block >}}
é€šå¸¸ï¼Œå…¶ä¸­çš„$w_i$æ ¹æ®å¯¹æ•°å‡½æ•°çš„å˜åŒ–è¶‹åŠ¿æ¥é€‰æ‹©ã€‚

# Lecture 21: Transformer Implementation
æœ¬èŠ‚è¯¾ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨NumPyæ¥å®ç°Transformerã€‚

## è‡ªæ³¨æ„åŠ›æœºåˆ¶ Self-attention
è‡ªæ³¨æ„åŠ›çš„å…¬å¼ä¸ºï¼š
{{< math_block >}}
Y = \left(\mathrm{softmax}\left(\frac{X W_K W_Q^T X^T}{\sqrt{d}}\right)X W_V \right) W_o
{{< /math_block >}}
ä¸ä¸Šä¸€è®²æœ‰äº›è®¸ä¸åŒä¹‹å¤„åœ¨äºåœ¨è¾“å‡ºå‰è¿›è¡Œäº†ä¸€æ¬¡é¢å¤–çš„çº¿æ€§å˜æ¢ã€‚

æ³¨æ„åˆ°å…¬å¼ä¸­æˆ‘ä»¬éœ€è¦å°†Xä¸ä¸‰ä¸ªWåˆ†åˆ«ç›¸ä¹˜ä»¥å¾—åˆ°KQVï¼Œå¯ä»¥å°†è¿™ä¸‰æ¬¡çŸ©é˜µè¿ç®—å˜ä¸ºä¸€ä¸ªè¿ç®—ï¼Œå³å°†ä¸‰ä¸ªçŸ©é˜µconcatåœ¨ä¸€èµ·ï¼Œç„¶åä¸Xç›¸ä¹˜ï¼Œä¸€ä¸‹å­å¾—åˆ°concatåœ¨ä¸€èµ·çš„KQVã€‚ä¸€ä¸ªè‡ªæ³¨æ„åŠ›æ¨¡å—ä¸ºï¼š
```python
def self_attention(X, mask, W_KQV, W_out):
    K,Q,V = np.split(X@W_KQV, 3, axis=-1)
    attn = softmax(K@Q.swapaxes(-1,-2) / np.sqrt(X.shape[-1]) + mask)
    return attn@V@W_out, attn
```


## Minibatching with batch matrix multiply 
è‡ªæ³¨æ„åŠ›ä¸æ˜¯æŒ‰ç…§æ—¶é—´å¾ªåºè¿›è¡Œå‰å‘ä¼ æ’­çš„ï¼Œå› æ­¤Xä»æŒ‰ç…§æ­£å¸¸BTDçš„é¡ºåºåœ¨å†…å­˜ä¸­ç»„ç»‡ã€‚å½“æˆ‘ä»¬å®ç°æ‰¹é‡self-attentionæ—¶ï¼Œå°±æ¶‰åŠåˆ°äº†æ‰¹é‡çŸ©é˜µä¹˜æ³•çš„æ¦‚å¿µã€‚

å…·ä½“æ¥è¯´ï¼Œå…¬å¼ä¸­$K@Q^T$è¿™ä¸€æ­¥çš„çŸ©é˜µä¹˜æ³•ï¼ŒKå’ŒQçš„shapeéƒ½æ˜¯BÃ—TÃ—Tï¼Œè¿™å°±æ¶‰åŠåˆ°äº†æ‰¹é‡çŸ©é˜µä¹˜æ³•ã€‚å¯¹æˆ‘ä»¬éƒ½è‡ªæ³¨æ„åŠ›æ¥è¯´ï¼Œæƒ³è¦çš„åº”è¯¥æ˜¯K\[i,:,:\]ä¸Q.T\[i,:,:\]ç›¸ä¹˜ï¼Œç¢°å·§ï¼Œæ‰¹é‡çŸ©é˜µä¹˜æ³•æ­£æ˜¯è¿™ä¹ˆå®šä¹‰çš„ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæ‰¹é‡çŸ©é˜µä¹˜æ³•è¦æ±‚ä¸¤ä¸ªçŸ©é˜µä¹‹é—´é™¤äº†å€’æ•°ä¸¤ä¸ªç»´åº¦ç¬¦åˆçŸ©é˜µä¹˜æ³•è¦æ±‚ï¼Œå‰©ä½™å…¶å®ƒç»´åº¦è¦ä¹ˆä¸å­˜åœ¨æˆ–ä¸º1è¿›è¡Œå¹¿æ’­ï¼Œè¦ä¹ˆå°±æ˜¯è¦ç›¸ç­‰çš„ã€‚

## Multihead attention å¤šå¤´æ³¨æ„åŠ›
å¤šå¤´è‡ªæ³¨æ„åŠ›çš„åŠ¨æœºæ¥è‡ªäº$K@Q^T$è¿™ä¸€æ­¥ï¼Œç»“æœæ¯ä¸ªå…ƒç´ éƒ½æ˜¯é•¿åº¦ä¸ºdçš„ä¸¤ä¸ªå‘é‡å†…ç§¯å¾—åˆ°çš„ã€‚ä¸ºäº†é™ä½è®¡ç®—æˆæœ¬ï¼Œæå‡ºäº†ä¸€ç§å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ã€‚å³ï¼Œå°†KQVçš„æ¯ä¸€è¡Œåˆ†ä¸ºhä¸ªéƒ¨åˆ†ï¼Œè¿›è¡Œæ³¨æ„åŠ›æ“ä½œï¼Œç„¶åå†æ‹¼æ¥èµ·æ¥ã€‚è¿™æ ·ï¼Œ$K@Q^T$æ¯ä¸ªå€¼éƒ½æ˜¯é•¿åº¦ä¸ºd/hå‘é‡è¿›è¡Œå†…ç§¯å¾—åˆ°çš„ã€‚

```python
def multihead_attention(X, mask, heads, W_KQV, W_out):
    N,T,d = X.shape
    K,Q,V = np.split(X@W_KQV, 3, axis=-1)
    K,Q,V = [a.reshape(N,T,heads,d//heads).swapaxes(1,2) for a in (K,Q,V)]
    
    attn = softmax(K@Q.swapaxes(-1,-2) / np.sqrt(d//heads) + mask)
    return (attn@V).swapaxes(1,2).reshape(N,T,d) @ W_out, attn
```

## Transformer block
ä¸€ä¸ªTransformer Blockç»“æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
![image.png](https://pics.zhouxin.space/202409080919833.png?x-oss-process=image/quality,q_90/format,webp)

åº”ç”¨å·²ç»å®ç°çš„å„ä¸ªç»„ä»¶ï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾åœ°å†™å‡ºä¸€ä¸ªæ”¯æŒå¤šå¤´è‡ªæ³¨æ„åŠ›çš„Transformerå—ï¼š
```python
def layer_norm(Z, eps):
    return (Z - Z.mean(axis=-1, keepdims=True)) / np.sqrt(Z.var(axis=-1, keepdims=True) + eps)
    
def relu(Z):
    return np.maximum(Z,0)

def transformer(X, mask, heads, W_KQV, W_out, W_ff1, W_ff2, eps):
    Z = layer_norm(multihead_attention(X, mask, heads, W_KQV, W_out)[0] + X, eps)
    return layer_norm(Z + relu(Z@W_ff1)@W_ff2, eps)
```

# Lecture 23 Moel Deployment
## æ¨¡å‹éƒ¨ç½²æ¦‚è§ˆ Model deployment overview
åœ¨ç‰¹å®šçš„è®¾å¤‡ä¸Šéƒ¨ç½²è®­ç»ƒå¥½çš„æ¨¡å‹æ˜¯ä¸€ä»¶æ¯”è¾ƒéº»çƒ¦çš„äº‹æƒ…ï¼Œå…¶å—è®¾å¤‡çš„å½±å“å¾ˆå¤§ã€‚ç°åœ¨æœ‰ä¸€äº›ç”¨äºéƒ¨ç½²æ¨ç†çš„æ¡†æ¶ï¼Œä¾‹å¦‚NVIDIAè®¾å¤‡ä¸Šçš„TensorRTï¼Œåœ¨åµŒå…¥å¼è®¾å¤‡ä¸Šæœ‰ARMComputeLibå’ŒTFLiteï¼Œè‹¹æœæœ‰CoreMLã€‚

ä¸Šè¿°æ¡†æ¶éƒ½éœ€è¦ä¸€ç§æ¨ç†æ¨¡å‹æ ¼å¼çš„è¾“å…¥ï¼Œè¿™ä¸ªè¾“å…¥èƒ½å¤Ÿæè¿°æ¨¡å‹çš„è®¡ç®—æµç¨‹ï¼Œè¿™ç§æ ¼å¼ç›®å‰æœ‰ONNXã€CoreMLå’ŒTFLiteã€‚æ¨¡å‹é€šè¿‡Pythonç¼–å†™ï¼Œå…¶å¥½å¤„æ˜¯æé«˜äº†ç¼–ç æ•ˆç‡ï¼Œå¸¦æ¥çš„ç¼ºç‚¹å°±æ˜¯å¯èƒ½æŸäº›æ¨¡å‹æ²¡åŠæ³•å®Œç¾è½¬æ¢ä¸ºä¸Šè¿°é€šç”¨æ ¼å¼ã€‚

è®¸å¤šæ¨ç†æ¡†æ¶éƒ½æ˜¯ä»¥è®¡ç®—å›¾è§£é‡Šå™¨çš„å½¢å¼ç»„ç»‡çš„ï¼Œå…¶é€šè¿‡é¢„åˆ†é…å’Œé‡ç”¨å†…å­˜ã€ç®—å­èåˆã€ç²¾åº¦é‡åŒ–ç­‰ä¼˜åŒ–æ‰‹æ®µï¼Œå®ç°æ›´é«˜æ•ˆçš„æ¨ç†ã€‚ä½†åŒæ ·ï¼Œå…¶ä¹Ÿæœ‰å¾ˆå¤šé™åˆ¶ï¼Œä¾‹å¦‚ä»–ä»¬æ”¯æŒçš„ç®—å­ç±»åˆ«æ˜¯æœ‰é™çš„ã€‚
## æœºå™¨å­¦ä¹ ç¼–è¯‘ Machine learning compilation
æœºå™¨å­¦ä¹ ç¼–è¯‘è¯•å›¾æ‰“ç ´éœ€è¦ä¸ºæ¯ç§è®¾å¤‡å®šåˆ¶æ¨ç†åº“çš„ç°çŠ¶ï¼Œå…¶ç›®æ ‡æ˜¯å°†è¾“å…¥çš„æ·±åº¦å­¦ä¹ æ¨¡å‹è½¬æ¢ä¸ºå¯ä»¥ç›´æ¥åœ¨ç»ˆç«¯ä¸Šè¿è¡Œçš„ä»£ç ã€‚

ä¸€ä¸ªMLç¨‹åºå¯ä»¥è¢«ç§°ä¸ºä¸€ä¸ªæ¨¡å—ï¼Œè¿™ä¸ªæ¨¡å—ç”±å¤šä¸ªå‡½æ•°æ„æˆï¼Œå‡½æ•°ä¹‹é—´äº’ç›¸è°ƒç”¨ã€‚ä¸‹å›¾è¿™ç§æ ¼å¼è¢«ç§°ä¸ºä¸­é—´çŠ¶æ€è¡¨ç¤ºIRï¼Œä¸‹å›¾è¿™ä¸€æ¨¡å—è¢«ç§°ä¸ºIRæ¨¡å—ã€‚

![image.png](https://pics.zhouxin.space/202409081958517.png?x-oss-process=image/quality,q_90/format,webp)

MLç¼–è¯‘çš„æµç¨‹å¤§è‡´æœ‰ï¼š
- ä»æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­å¯¼å…¥æ¨¡å‹ï¼›
![](https://pics.zhouxin.space/202409082011257.png?x-oss-process=image/quality,q_90/format,webp)
- å¯¹IRæ¨¡å—è¿›è¡Œå˜æ¢ï¼Œç®—å­èåˆ
![image.png](https://pics.zhouxin.space/202409082011947.png?x-oss-process=image/quality,q_90/format,webp)

- å°†ä¸­é—´çŠ¶æ€ç¿»è¯‘ä¸ºæ›´ä½çº§çš„å¾ªç¯ä»£ç 
![image.png](https://pics.zhouxin.space/202409082011782.png?x-oss-process=image/quality,q_90/format,webp)
- è¿›è¡Œæ›´ä½çº§çš„å˜æ¢ï¼Œè¿›è¡Œç®—å­èåˆ
![image.png](https://pics.zhouxin.space/202409082013131.png?x-oss-process=image/quality,q_90/format,webp)
- è¿›è¡Œä»£ç ç”Ÿæˆ
![image.png](https://pics.zhouxin.space/202409082014722.png?x-oss-process=image/quality,q_90/format,webp)
æœ¬è®²åç»­å†…å®¹å’Œä¸‹ä¸€è®²å‡ä¸ºä»‹ç»MLCï¼Œè®¡åˆ’åé¢ç»§ç»­å­¦ä¹ MLCï¼Œè¿™é‡Œå°±ä¸æµ…å°è¾„æ­¢äº†ã€‚

å…¨æ–‡å®Œã€‚
# å‚è€ƒæ–‡æ¡£

[^1]: [æŒ‡æ•°ç§»åŠ¨å¹³å‡EMA\_emaç§»åŠ¨å¹³å‡æ•°æ€ä¹ˆç®—-CSDNåšå®¢](https://blog.csdn.net/qq_36892712/article/details/133774755)
[^2]: [zhuanlan.zhihu.com/p/22810533](https://zhuanlan.zhihu.com/p/22810533)
[^3]: [An overview of gradient descent optimization algorithms](https://www.ruder.io/optimizing-gradient-descent/#Nesterov%20accelerated%20gradient)
[^4]: [numpy.lib.stride\_tricks.as\_strided â€” NumPy v2.1 Manual](https://numpy.org/doc/stable/reference/generated/numpy.lib.stride_tricks.as_strided.html)